{
  "hash": "aa16536e3ea323ccd9d0024a8873fb62",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 7: Model Assumptions and Diagnostics\"\nformat: \n   html:\n     df-print: kable\ntoc: true\ntoc-depth: 3\n---\n\n\n\n## Introduction\n\nIn the previous chapters, we studied regression models with various types of predictors.\n\nIn this chapter, we focus on the key assumptions behind the linear regression model and practical ways to diagnose them in R.\n\nYou will learn the assumptions that most matter in practice:\n\n-   Linearity / correct specification\n-   Independent errors\n-   Constant error variance (homoscedasticity)\n-   Normality of errors (usually less critical for large sample sizes)\n\nWe use the `performance` package to quickly diagnose issues and show simple remedies. See the [`check_model` vignette](https://easystats.github.io/performance/articles/check_model.html) for details on the plots.\n\n## Packages and Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(parameters)\nlibrary(splines)\nlibrary(performance)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Load Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_clean <- read_csv2(here(\"data\", \"steps_clean.csv\"))\n```\n:::\n\n\n\n## STePS Model\n\nLet's perform a quick diagnostic of the ANCOVAâ€‘style STePS model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lm <- lm(\n  lsas_post ~ trt + lsas_screen + phq9_screen + gad_screen,\n  data = df_clean\n)\ncheck_model(mod_lm)\n```\n\n::: {.cell-output-display}\n![](regression-diagnostics_files/figure-html/fit-main-model-1.png){width=960}\n:::\n:::\n\n\n\nThere are some possible problematic residual values in the left tail: a few participants report much lower observed values than predicted by the model. This is likely not a substantive issue.\n\n## Using Fake-Data Simulation to Understand Assumptions\n\nSimulating data is a great way to understand model assumptions.\n\n### A Well-Behaved Base Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 180\nx <- rnorm(n, 0, 1)\nepsilon <- rnorm(n, 0, 1)\ny <- 1 + 2 * x + epsilon\n\nm_base <- lm(y ~ x)\n\ncheck_model(m_base)\n```\n\n::: {.cell-output-display}\n![](regression-diagnostics_files/figure-html/sim-good-model-1.png){width=960}\n:::\n:::\n\n\n\nNote that even data simulated from a correctly specified regression model will deviate from perfection due to sampling variability.\n\n## Nonlinear Model\n\nNow let's break the linearity assumption by simulating from a quadratic function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\nx <- runif(n, -3, 3)\nepsilon <- rnorm(n, 0, 2)\n# linear slope = 0\ny_nl <- 10 + 3 * x^2 + epsilon\n\n# Wrong model spec\nm_lin <- lm(y_nl ~ x)\ncheck_model(m_lin)\n```\n\n::: {.cell-output-display}\n![](regression-diagnostics_files/figure-html/sim-nonlinear-linear-fit-1.png){width=960}\n:::\n:::\n\n\n\nWe can see that there are many issues with the model.\n\n### Fix 1: Add Polynomial Term\n\nIf we add a quadratic term then most issues should disappear.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_quad <- lm(y_nl ~ x + I(x^2))\ncheck_model(m_quad)\n```\n\n::: {.cell-output-display}\n![](regression-diagnostics_files/figure-html/sim-quad-fit-1.png){width=960}\n:::\n:::\n\n\n\n### Fix 2: Natural Cubic Spline\n\nAs an illustration, a more flexible cubic spline should also remedy the problems.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_spline <- lm(y_nl ~ ns(x, df = 4))\ncheck_model(m_spline)\n```\n\n::: {.cell-output-display}\n![](regression-diagnostics_files/figure-html/sim-spline-fit-1.png){width=960}\n:::\n:::\n\n\n\nLet's plot all the fits.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare curves\nnd <- data.frame(x = seq(min(x), max(x), length.out = 200))\nnd$y_lin <- predict(m_lin, newdata = nd)\nnd$y_quad <- predict(m_quad, newdata = nd)\nnd$y_spline <- predict(m_spline, newdata = nd)\n\n# Plot\np <- ggplot(data.frame(x = x, y = y_nl), aes(x, y)) +\n  geom_point(alpha = 0.3, shape = 16) +\n  geom_line(\n    data = nd,\n    aes(x, y = y_lin, color = \"Linear (mis-specified)\"),\n    linewidth = 1\n  ) +\n  geom_line(\n    data = nd,\n    aes(x, y = y_quad, color = \"Quadratic\"),\n    linewidth = 1\n  ) +\n  geom_line(\n    data = nd,\n    aes(x, y = y_spline, color = \"Spline\"),\n    linewidth = 1\n  ) +\n  labs(\n    title = \"Nonlinearity: Linear vs Quadratic vs Spline\",\n    x = \"x\", y = \"y\", color = \"Fit\"\n  ) +\n  theme_minimal()\n\np\n```\n\n::: {.cell-output-display}\n![](regression-diagnostics_files/figure-html/sim-nonlinearity-plot-1.png){width=672}\n:::\n:::\n\n\n\nClearly, the linear coefficient is not a good summary of the relationship.\n\n## Heteroskedasticity (Non-Constant Variance)\n\nLet's break the OLS assumptions by simulating data from a lognormal model, which will lead to heteroskedastic errors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate data from a lognormal distribution\ngen_lnorm <- function(n) {\n  x <- rnorm(n, 0, 1)\n  mu <- 1 + 2 * x\n  sdlog <- 0.5\n  u <- rlnorm(n, meanlog = -sdlog^2 / 2, sdlog = sdlog) # so that E[u]=1\n  y_lnorm <- mu * u\n\n  data.frame(y = y_lnorm, x = x)\n}\nd_lnorm <- gen_lnorm(n = 300)\nm_het <- lm(y ~ x, d_lnorm)\ncheck_model(m_het)\n```\n\n::: {.cell-output-display}\n![](regression-diagnostics_files/figure-html/sim-heterosk-run-1.png){width=960}\n:::\n\n```{.r .cell-code}\ncheck_heteroscedasticity(m_het)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWarning: Heteroscedasticity (non-constant error variance) detected (p < .001).\n```\n\n\n:::\n:::\n\n\n\nWe can see a clear deviation from constant variance. One remedy is to use robust standard errors.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompare_parameters(\n  \"standard\" = model_parameters(m_het),\n  \"Robust SEs\" = model_parameters(m_het, vcov = \"HC3\")\n) |>\n  display()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter    |         standard |       Robust SEs |\n|:------------|:-----------------|:-----------------|\n|(Intercept)  |1.04 (0.87, 1.21) |1.04 (0.86, 1.22) |\n|x            |2.31 (2.13, 2.49) |2.31 (1.97, 2.65) |\n|             |                  |                  |\n|Observations |              300 |              300 |\n\n\n:::\n:::\n\n\n\nIf we repeat this process, we can demonstrate that while the regression coefficient is correct, the standard error is wrong and therefore so are p-values and confidence intervals.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Helper functions for the simulation\nlibrary(mirai)\ndaemons(8)\nset.seed(44334234)\neverywhere({\n  library(tidyverse)\n  library(parameters)\n})\nrun_sim <- function(\n    data_gen_func,\n    data_gen_params = list(NULL),\n    formula,\n    vcov = NULL,\n    n_simulations = 1000) {\n  map(\n    seq_len(n_simulations),\n    in_parallel( # nolint: object_usage_linter\n      \\(i) {\n        d <- do.call(data_gen_func, data_gen_params)\n        mod <- lm(formula, data = d)\n        model_parameters(mod, vcov = vcov) |> as_tibble()\n      },\n      formula = formula,\n      vcov = vcov,\n      data_gen_func = data_gen_func,\n      data_gen_params = data_gen_params\n    )\n  ) |>\n    list_rbind()\n}\nsummarize_sim <- function(x) {\n  x |>\n    summarize(\n      est_mean = mean(Coefficient),\n      est_sd = sd(Coefficient),\n      se_mean = mean(SE),\n      CI_coverage = mean(CI_low <= 2 & CI_high >= 2)\n    )\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate\nres <- run_sim(\n  gen_lnorm,\n  list(n = 300),\n  formula = y ~ x,\n  n_simulations = 5000\n)\nsim_sum <- res |>\n  filter(Parameter == \"x\") |>\n  summarize_sim()\nsim_sum\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| est_mean| est_sd| se_mean| CI_coverage|\n|--------:|------:|-------:|-----------:|\n|        2|   0.11|    0.07|        0.78|\n\n</div>\n:::\n:::\n\n\n\nWe see that the estimated standard error is too small, and the confidence intervals include the true parameter value in only 78% of simulations. Let's try with robust SEs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres2 <- run_sim(\n  gen_lnorm,\n  list(n = 300),\n  formula = y ~ x,\n  n_simulations = 5000,\n  vcov = \"HC3\"\n)\nsim_sum2 <- res2 |>\n  filter(Parameter == \"x\") |>\n  summarize_sim()\nsim_sum2\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| est_mean| est_sd| se_mean| CI_coverage|\n|--------:|------:|-------:|-----------:|\n|        2|   0.11|    0.11|        0.94|\n\n</div>\n:::\n:::\n\n\n\nNow 94% of the CIs include the true value.\n\n## Outliers and Influential Points\n\nOutliers can be problematic under certain circumstances, for example due to data errors. Let's add a clearly faulty observation to the STePS data to see how it appears in the model diagnostics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add extreme outlier\ndf_outlier <- bind_rows(\n  df_clean,\n  tibble(lsas_post = 50, phq9_screen = 50)\n)\n\nm_clean <- lm(lsas_post ~ phq9_screen, data = df_clean)\nm_outlier <- lm(lsas_post ~ phq9_screen, data = df_outlier)\ncompare_parameters(m_clean, m_outlier) |>\n  display()\n```\n\n::: {.cell-output-display}\n\n\n|Parameter    |             m_clean |           m_outlier |\n|:------------|:--------------------|:--------------------|\n|(Intercept)  |52.83 (44.14, 61.51) |58.60 (50.92, 66.27) |\n|phq9 screen  | 1.50 ( 0.67,  2.32) | 0.86 ( 0.17,  1.55) |\n|             |                     |                     |\n|Observations |                 169 |                 170 |\n\n\n:::\n:::\n\n\n\nThe relation between LSAS at post and PHQ-9 at baseline is severely impacted by our extreme observation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Diagnostics with outlier\ncheck_model(m_outlier)\n```\n\n::: {.cell-output-display}\n![](regression-diagnostics_files/figure-html/outlier-diagnostics-1.png){width=960}\n:::\n\n```{.r .cell-code}\ncheck_outliers(m_outlier)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1 outlier detected: case 170.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model).\n```\n\n\n:::\n:::\n\n\n\nThe extreme outlier is clearly visible on some of the plots, but note that it may not be apparent on the commonly used \"Normality of Residuals\" plot.\n\n## Non-Independent Errors (Therapist Effects)\n\nErrors are assumed to be independent and have constant variance. A common violation of the independence assumption occurs when we take repeated measures on participants. However, longitudinal or timeseries data is not the focus of this course, so instead we will assume that participants are clustered (e.g., treated by the same psychotherapist). Patients who share the same therapist tend to have outcomes more similar than patients treated by another therapist. For example, if therapists have varying skills. This can also lead to correlated errors. Let's simulate this: assume 10 therapists per treatment group, and each treats 50 patients (a nested design).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngen_therapist_data <- function(...) {\n  n <- 50 # n per therapist\n  n_therapists <- 20\n  lvl2 <- rnorm(n_therapists, 0, 0.5)\n  therapist_id <- rep(1:n_therapists, each = n)\n  trt <- rbinom(n_therapists, 1, 0.5)\n  epsilon <- rnorm(n * n_therapists, 0, 1)\n\n  y <- 5 + 0.5 * trt[therapist_id] + lvl2[therapist_id] + epsilon\n  data.frame(\n    y,\n    trt = factor(trt[therapist_id]),\n    therapist_id = therapist_id\n  )\n}\n\nd_therapist <- gen_therapist_data()\nm_therapist <- lm(y ~ trt, data = d_therapist)\ncheck_autocorrelation(m_therapist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWarning: Autocorrelated residuals detected (p = 0.012).\n```\n\n\n:::\n\n```{.r .cell-code}\n# Simulate\nres3 <- run_sim(\n  gen_therapist_data,\n  formula = y ~ trt,\n  n_simulations = 500\n)\n\nsim_sum3 <- res3 |>\n  filter(Parameter == \"trt1\") |>\n  summarize(\n    est_mean = mean(Coefficient),\n    est_sd = sd(Coefficient),\n    se_mean = mean(SE),\n    CI_coverage = mean(CI_low <= 0.5 & CI_high >= 0.5)\n  )\nsim_sum3\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| est_mean| est_sd| se_mean| CI_coverage|\n|--------:|------:|-------:|-----------:|\n|     0.51|   0.25|    0.07|        0.45|\n\n</div>\n:::\n:::\n\n\n\nBecause we assumed a large therapist effect, the SEs are much smaller in the linear model that ignores clustering, and the confidence intervals include the true treatment effect in only 45% of simulations. This happens because the data generation treats therapists as a random factor: each therapist has a random effect drawn from a distribution, which induces correlation among patients of the same therapist and adds a therapist-level variance component. Ignoring this structure violates independence and typically underestimates SEs. To remedy this, use cluster-robust standard errors or fit a multilevel (random-effects) model with, for example, random intercepts for therapists.\n\n::: callout-note\n## Validity, Representativeness and Fixed versus Random Effects\n\nThe clustering example illustrates issues of *validity* and *representativeness*, not just statistical assumptions. In our simulation, therapists are treated as a random factor: for each dataset, 20 therapist effects are drawn from a normal distribution. This induces within-therapist correlation and a therapist-level variance component.\n\nHowever, fitting a random-effects model does not, by itself, make your study therapists representative of a wider population. Representativeness is a property of your sampling and study design, not the analysis model. Moreover, we might not want to generalize beyond the therapists in our study. From that perspective, the random-effects assumption may not match our research question. In this case, we could treat therapists as a fixed factor and condition on the therapists we include in our study. This reduces uncertainty about the treatment effect because we assume the therapists and their skill levels would be identical across hypothetical repetitions of our study. Neither perspective is right or wrong from a statistical perspective, it depends in what your inferential goals are.\n:::\n\n## Summary\n\nIn this chapter you learned:\n\n-   Run a quick diagnostic check with `check_model()`\n-   Diagnose and fix nonlinearity using polynomials and natural splines\n-   Detect heteroskedasticity\n-   Identify outliers and influence\n-   Non-independent errors\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}