{
  "hash": "323ba6643babd1078af3231230373106",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Chapter 4: Regression with a Numeric Predictor\"\nformat: html\neditor: visual\ntoc: true\ntoc-depth: 3\n---\n\n\n\n\n# Introduction\n\nIn the previous chapters, we studied regression models with:\n\n-   no predictors (intercept-only models)\n-   binary predictors\n-   categorical predictors with multiple levels\n\nIn this chapter, we move to a **numeric predictor**, called `gad_screen`.\n\nNumeric predictors allow us to model **continuous relationships**, where the expected outcome changes smoothly as the predictor changes.\n\nYou will learn:\n\n-   how numeric predictors enter regression models\n-   how to interpret intercepts and slopes\n-   how linear regression models **mean trends**\n-   how quantile regression models **quantile trends**\n-   how to obtain predicted values across a range of the predictor\n-   how to visualize regression lines and uncertainty\n-   how to account for non-linear relationships between the predictor and the outcome\n\n# 1. Packages and Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(broom) \nlibrary(quantreg) \nlibrary(marginaleffects) \nlibrary(here)\n```\n:::\n\n\n\n\n## Load data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.csv(here(\"data\", \"steps_clean.csv\"))\ndf <- df %>% rename(grp = group)\n```\n:::\n\n\n\n\nWe will use `gad_screen`, a numeric measure of generalized anxiety symptoms at screening, as our predictor.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(df$gad_screen)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    7.00   10.00   10.86   14.00   21.00 \n```\n\n\n:::\n:::\n\n\n\n\n# 2. Regression with a Numeric Predictor\n\nWith a numeric predictor, the regression model estimates:\n\n1.  the expected outcome when the predictor equals zero\n\n2.  the **change in the outcome** associated with a one-unit increase in the predictor\n\n## 2.1 Model formula\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nlsas_post ~ gad_screen\n```\n\n\n:::\n:::\n\n\n\n\nThis corresponds to the linear regression model:\n\n$$\nE(LSAS_{post}) = \\beta_0 + \\beta_1 \\cdot gadscreen\n$$\n\nWhere:\n\n-   $\\beta_0$ is the expected change in LSAS score when `gad_screen = 0`\n\n-   $\\beta_1$ is the expected **change in LSAS** for a one-unit increase in `gad_screen`\n\nFor individual observations:\n\n$$\nLSAS_{post_i} = \\beta_0 + \\beta_1 \\cdot gadscreen + \\epsilon_i\n$$\n\n# 3. Linear Regression (Mean Trend)\n\n## 3.1 Fit the model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lm <- lm(lsas_post ~ gad_screen, data = df)\nsummary(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lsas_post ~ gad_screen, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-65.031 -18.149   1.527  16.674  64.527 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  56.6502     4.8858  11.595   <2e-16 ***\ngad_screen    0.9705     0.4171   2.327   0.0212 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.46 on 167 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.0314,\tAdjusted R-squared:  0.0256 \nF-statistic: 5.414 on 1 and 167 DF,  p-value: 0.02118\n```\n\n\n:::\n:::\n\n\n\n\n### Interpretation\n\n-   The **intercept** represents the estimated mean LSAS score when `gad_screen = 0`\n\n-   The **slope** (coefficient) represents how much the mean LSAS score changes per unit increase in baseline anxiety\n\nA positive slope indicates higher post-treatment LSAS scores among participants with higher baseline anxiety. So for each one-unit increase in GAD-7 at screen, the expected mean LSAS at post increase with 0.97 points.\n\n## 3.2 Relationship to correlation\n\nWith a single numeric predictor, linear regression is closely related to Pearson correlation:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(df$lsas_post, df$gad_screen, use = \"complete.obs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1772046\n```\n\n\n:::\n:::\n\n\n\n\nIn fact, if we standardize both the predictor and the outcome to z-scores with mean=0, and SD=1, the correlation and the coefficient, will be almost identical with only one numeric predictor. This can be done using the `scale()` function with-in the regression:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm(scale(lsas_post) ~ scale(gad_screen), data = df))[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nscale(gad_screen) \n        0.1765964 \n```\n\n\n:::\n\n```{.r .cell-code}\ncor(df$lsas_post, df$gad_screen, use = \"complete.obs\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1772046\n```\n\n\n:::\n:::\n\n\n\n\n# 4. Quantile Regression (Quantile Trends)\n\n## 4.1 Model formulation\n\nFor quantile regression at quantile $\\tau$:\n\n$$\nQ_{LSAS_{post}}(\\tau) =  \\beta_0,\\tau + \\beta_1, \\tau \\cdot gadscreen\n$$\n\n-   $\\beta_0,\\tau$ = $\\tau$-th quantile when `gad_screen = 0`\n-   $\\beta_1,\\tau$ = change in the $\\tau$-th quantile per one-unit increase in `gad_screen`\n\n## 4.2 Fit a median regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_rq <- rq(lsas_post ~ gad_screen, tau = 0.5, data = df)\nsummary(mod_rq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall: rq(formula = lsas_post ~ gad_screen, tau = 0.5, data = df)\n\ntau: [1] 0.5\n\nCoefficients:\n            coefficients lower bd upper bd\n(Intercept) 61.72727     50.14774 74.72628\ngad_screen   0.45455     -0.87730  2.38616\n```\n\n\n:::\n:::\n\n\n\n\nThe slope now describes how the **median** LSAS score changes as baseline anxiety increases.\n\n# 5. Model Objects and Coefficients\n\n## 5.1 Linear regression object\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)  gad_screen \n 56.6502034   0.9705066 \n```\n\n\n:::\n:::\n\n\n\n\n-   `(Intercept)` → expected mean at `gad_screen = 0`\n\n<!-- -->\n\n-   `gad_screen` → mean change per unit increase\n\n## 5.2 Quantile regression object\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(mod_rq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)  gad_screen \n 61.7272727   0.4545455 \n```\n\n\n:::\n:::\n\n\n\n\n# 6. Predictions Using `marginaleffects`\n\nWith numeric predictors, predictions depend on the value of the predictor. We therefore compute predictions across a **range of values**.\n\n## 6.1 Predicted means across `gad_screen`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a grid of gad_screen values (50 points from min to max)\nnewdat <- marginaleffects::datagrid(\n  newdata = df,\n  gad_screen = seq(\n    min(df$gad_screen, na.rm = TRUE),\n    max(df$gad_screen, na.rm = TRUE),\n    length.out = 50\n  )\n)\n\n# Get predictions from the linear model at those values\npred_lm <- marginaleffects::predictions(mod_lm, newdata = newdat)\npred_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n gad_screen Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n       1.00     57.6       4.50 12.8   <0.001 122.1  48.8   66.4\n       1.41     58.0       4.35 13.3   <0.001 132.4  49.5   66.5\n       1.82     58.4       4.20 13.9   <0.001 143.9  50.2   66.6\n       2.22     58.8       4.05 14.5   <0.001 156.6  50.9   66.7\n       2.63     59.2       3.90 15.2   <0.001 170.9  51.6   66.8\n--- 40 rows omitted. See ?print.marginaleffects ---\n      19.37     75.4       4.03 18.7   <0.001 256.8  67.5   83.4\n      19.78     75.8       4.19 18.1   <0.001 241.3  67.6   84.0\n      20.18     76.2       4.34 17.6   <0.001 227.2  67.7   84.7\n      20.59     76.6       4.49 17.1   <0.001 214.3  67.8   85.4\n      21.00     77.0       4.65 16.6   <0.001 202.5  67.9   86.1\nType: response\n```\n\n\n:::\n:::\n\n\n\n\nThe `Estimate` column show the predicted **mean** value of `lsas_post` across a ragne of `gad_screen` values.\n\n## 6.2 Predicted medians across `gad_screen`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_rq <- predictions(mod_rq, newdata = newdat)\npred_rq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n gad_screen Estimate Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n       1.00     62.2       6.97  8.92   <0.001  60.9  48.5   75.8\n       1.41     62.4       6.74  9.25   <0.001  65.3  49.2   75.6\n       1.82     62.6       6.51  9.60   <0.001  70.1  49.8   75.3\n       2.22     62.7       6.29  9.98   <0.001  75.5  50.4   75.1\n       2.63     62.9       6.06 10.38   <0.001  81.5  51.0   74.8\n--- 40 rows omitted. See ?print.marginaleffects ---\n      19.37     70.5       5.74 12.28   <0.001 112.7  59.3   81.8\n      19.78     70.7       5.97 11.85   <0.001 105.3  59.0   82.4\n      20.18     70.9       6.19 11.45   <0.001  98.5  58.8   83.0\n      20.59     71.1       6.42 11.08   <0.001  92.3  58.5   83.7\n      21.00     71.3       6.64 10.73   <0.001  86.8  58.2   84.3\nType: response\n```\n\n\n:::\n:::\n\n\n\n\nThe `Estimate` column show the predicted **median** value of `lsas_post` across a ragne of `gad_screen` values.\n\n## 6.3 Marginal effect (slope)\n\nFor numeric predictors, contrasts correspond to **slopes**. These will be the same as the coefficient you got using the `summary()` function above.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_slopes(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %\n    0.971      0.417 2.33     0.02 5.6 0.153   1.79\n\nTerm: gad_screen\nType: response\nComparison: dY/dX\n```\n\n\n:::\n:::\n\n\n\n\nFor quantile regression:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\navg_slopes(mod_rq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n    0.455       0.62 0.733    0.463 1.1 -0.76   1.67\n\nTerm: gad_screen\nType: response\nComparison: dY/dX\n```\n\n\n:::\n:::\n\n\n\n\n# 7. Visualization\n\nWe can now visualize the relationship using the predicted values we got using the `predictions()` function earlier. Remember that these were saved using in the objects `pred_lm` and `pred_rq`\n\n## 7.1 Linear regression line with confidence interval\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df, aes(x = gad_screen, y = lsas_post)) +\ngeom_point(alpha = 0.4) + # plot the individual values\ngeom_line(data = pred_lm, aes(y = estimate), linewidth = 1) + # add the regression line\ngeom_ribbon( # add a confidence interval around the line\ndata = pred_lm,\naes(ymin = conf.low, ymax = conf.high),\nalpha = 0.2\n) +\nlabs(\nx = \"GAD score at screening\",\ny = \"LSAS post-treatment\",\ntitle = \"Linear Regression: Mean LSAS vs Baseline Anxiety\"\n) +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](regression-one-numeric-predictor_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n## 7.2 Median regression line with confidence interval\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(df, aes(x = gad_screen, y = lsas_post)) + \ngeom_point(alpha = 0.4) + # plot the individual values\ngeom_line(data = pred_rq, aes(y = estimate), linewidth = 1) + # plot the regression line\ngeom_ribbon( # plot confidence interval\ndata = pred_rq,\naes(ymin = conf.low, ymax = conf.high),\nalpha = 0.2\n) +\nlabs(\nx = \"GAD score at screening\",\ny = \"LSAS post-treatment\",\ntitle = \"Quantile Regression: Median LSAS vs Baseline Anxiety\"\n) +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](regression-one-numeric-predictor_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n# 8. Modeling Non-Linear Relationships\n\nSo far, we have assumed that the relationship between baseline severity and post-treatment severity is **linear**.\n\nHowever, clinical change processes are sometimes **non-linear**. For example:\n\n-   participants with very low baseline severity have limited room for improvement\n-   participants with very high baseline severity may show diminishing returns\n-   treatment effects may level off at the extremes\n\nTo explore this, we now examine the relationship between **baseline LSAS score** (`lsas_screen`) and **post-treatment LSAS score** (`lsas_post`).\n\nIn this section, we illustrate two common approaches to modeling non-linearity:\n\n1.  **Polynomial regression**\n2.  **Spline regression**\n\n------------------------------------------------------------------------\n\n### 8.1 Polynomial regression\n\nPolynomial regression allows the relationship between predictor and outcome to be **curved rather than straight**.\n\n#### Model formulation\n\nA quadratic polynomial model can be written as: $$\nE(LSAS_{post}) = \\beta_0 + \\beta_1 \\cdot lsasscreen + \\beta_2 \\cdot lsasscreen^2\n$$\n\nThis allows the relationship to be **curved** rather than straight.\n\n#### Fit a quadratic model\n\nIn R, we use the `poly()` function.\\\nSetting `raw = TRUE` makes the coefficients correspond directly to powers of the predictor.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lm_poly <- lm(lsas_post ~ poly(lsas_screen, 2, raw = TRUE), data = df)\nsummary(mod_lm_poly)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lsas_post ~ poly(lsas_screen, 2, raw = TRUE), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-85.060 -12.942   3.565  13.443  42.811 \n\nCoefficients:\n                                    Estimate Std. Error t value Pr(>|t|)  \n(Intercept)                       -43.518108  37.945495  -1.147   0.2531  \npoly(lsas_screen, 2, raw = TRUE)1   1.753930   0.841860   2.083   0.0387 *\npoly(lsas_screen, 2, raw = TRUE)2  -0.005122   0.004540  -1.128   0.2609  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.78 on 166 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.3048,\tAdjusted R-squared:  0.2964 \nF-statistic: 36.39 on 2 and 166 DF,  p-value: 7.875e-14\n```\n\n\n:::\n:::\n\n\n\n\n#### Interpretation\n\n-   The **intercept** is the expected LSAS score when `lsas_screen = 0`\n\n-   The **linear term** captures the overall trend\n\n-   The **quadratic term** captures curvature\n\nBecause individual coefficients are difficult to interpret, we rely on **predicted values and visualization**.\n\n#### Visualizing the polynomial fit\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdat <- marginaleffects::datagrid(\nnewdata = df,\nlsas_screen = seq(\nmin(df$lsas_screen, na.rm = TRUE),\nmax(df$lsas_screen, na.rm = TRUE),\nlength.out = 100\n)\n)\n\npred_poly <- marginaleffects::predictions(\nmod_lm_poly,\nnewdata = newdat\n)\n\nggplot(df, aes(x = lsas_screen, y = lsas_post)) +\ngeom_point(alpha = 0.4) +\ngeom_line(data = pred_poly, aes(y = estimate), linewidth = 1) +\ngeom_ribbon(\ndata = pred_poly,\naes(ymin = conf.low, ymax = conf.high),\nalpha = 0.2\n) +\nlabs(\nx = \"LSAS score at screening\",\ny = \"LSAS post-treatment\",\ntitle = \"Polynomial Regression (Quadratic Relationship)\"\n) +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](regression-one-numeric-predictor_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n### 8.2 Spline regression\n\nPolynomial models impose a **global shape** on the data.\\\nSpline regression offers a more flexible approach by fitting the relationship **locally**, while maintaining smoothness.\n\n#### Model formulation\n\nUsing splines, the model is written as: $$\nE(LSAS_{post}) = \\beta_0 + f(LSAS_{screen})\n$$ where $f(\\cdot)$ is a smooth function estimated from the data\n\n#### Fit a spline model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(splines)\n\nmod_lm_spline <- lm(\nlsas_post ~ ns(lsas_screen, df = 4),\ndata = df\n)\nsummary(mod_lm_spline)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lsas_post ~ ns(lsas_screen, df = 4), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-82.486 -12.822   2.455  13.112  45.024 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                37.322      6.629   5.630 7.66e-08 ***\nns(lsas_screen, df = 4)1   19.444      7.546   2.577   0.0109 *  \nns(lsas_screen, df = 4)2   47.252      8.099   5.834 2.81e-08 ***\nns(lsas_screen, df = 4)3   82.116     16.089   5.104 9.13e-07 ***\nns(lsas_screen, df = 4)4   43.253      9.620   4.496 1.30e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.62 on 164 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.324,\tAdjusted R-squared:  0.3075 \nF-statistic: 19.65 on 4 and 164 DF,  p-value: 3.146e-13\n```\n\n\n:::\n:::\n\n\n\n\n-   `df = 4` controls how flexible the curve is\n\n-   Higher values allow more curvature, but risk overfitting\n\nAs with polynomials, spline coefficients are **not interpreted individually**.\n\n#### Visualizing the spline fit\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_spline <- marginaleffects::predictions(\nmod_lm_spline,\nnewdata = newdat\n)\n\nggplot(df, aes(x = lsas_screen, y = lsas_post)) +\ngeom_point(alpha = 0.4) +\ngeom_line(data = pred_spline, aes(y = estimate), linewidth = 1) +\ngeom_ribbon(\ndata = pred_spline,\naes(ymin = conf.low, ymax = conf.high),\nalpha = 0.2\n) +\nlabs(\nx = \"LSAS score at screening\",\ny = \"LSAS post-treatment\",\ntitle = \"Spline Regression (Natural Cubic Spline)\"\n) +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](regression-one-numeric-predictor_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n### 8.3 Comparing linear and non-linear fits\n\nFinally, we compare:\n\n-   a linear model\n\n-   a quadratic polynomial model\n\n-   a spline model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lm_linear <- lm(lsas_post ~ lsas_screen, data = df)\n\npred_linear <- marginaleffects::predictions(\nmod_lm_linear,\nnewdata = newdat\n)\n\nggplot(df, aes(x = lsas_screen, y = lsas_post)) +\ngeom_point(alpha = 0.3) +\ngeom_line(data = pred_linear, aes(y = estimate), linewidth = 1) +\ngeom_line(data = pred_poly, aes(y = estimate),\nlinetype = \"dashed\", linewidth = 1) +\ngeom_line(data = pred_spline, aes(y = estimate),\nlinetype = \"dotted\", linewidth = 1) +\nlabs(\nx = \"LSAS score at screening\",\ny = \"LSAS post-treatment\",\ntitle = \"Linear vs Polynomial vs Spline Regression\"\n) +\ntheme_minimal()\n```\n\n::: {.cell-output-display}\n![](regression-one-numeric-predictor_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n\nWhen comparing, we see that the difference in the non-linear lines from the straight line are small, and that non-linearities could arguably be ignored in modelling.\n\n# Summary\n\nIn this chapter you learned:\n\n-   how numeric predictors enter regression models\n\n-   how intercepts and slopes are interpreted\n\n-   how linear regression models **mean trends**\n\n-   how quantile regression models **quantile trends**\n\n-   how to compute predictions across a range of predictor values\n\n-   how to estimate and interpret marginal effects (slopes)\n\n-   how to visualize regression lines with confidence intervals\n\n-   how to allow for non-linear relationships\n\nThis chapter completes the foundation for regression models with a **single predictor**.\\\nIn the next chapter, we will combine **multiple predictors** in the same model.\n",
    "supporting": [
      "regression-one-numeric-predictor_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}