[
  {
    "objectID": "working in R.html",
    "href": "working in R.html",
    "title": "working in R",
    "section": "",
    "text": "The R studio interface has four main panes.\n\nSource pane (top-left) is where you write you write and execute your code.\nConsole pane (bottom-left)This is the interactive\nEnvironment pane (top-right)\nOutput pane (bottom-left)"
  },
  {
    "objectID": "working in R.html#user-interface",
    "href": "working in R.html#user-interface",
    "title": "working in R",
    "section": "",
    "text": "The R studio interface has four main panes.\n\nSource pane (top-left) is where you write you write and execute your code.\nConsole pane (bottom-left)This is the interactive\nEnvironment pane (top-right)\nOutput pane (bottom-left)"
  },
  {
    "objectID": "exercises/solutions.html",
    "href": "exercises/solutions.html",
    "title": "Exercise solutions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(here)\n\nd_bl &lt;- read_rds(here(\"data\", \"steps_baseline.rds\"))\ndf_data &lt;- read_csv(here(\"data\", \"steps_clean.csv\"))"
  },
  {
    "objectID": "exercises/solutions.html#lab-1",
    "href": "exercises/solutions.html#lab-1",
    "title": "Exercise solutions",
    "section": "Lab 1",
    "text": "Lab 1"
  },
  {
    "objectID": "exercises/solutions.html#lab-2",
    "href": "exercises/solutions.html#lab-2",
    "title": "Exercise solutions",
    "section": "Lab 2",
    "text": "Lab 2\n2.1 Use the functions described in yesterdays lab to get a quick overview of the dataset and give a brief summary of it.\n\nglimpse(d_bl)\n\nRows: 181\nColumns: 15\n$ id             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ group          &lt;dbl&gt; 2, 1, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 2, 1, 1, 2, 0, 1…\n$ lsas_screen    &lt;dbl&gt; 63, 71, 98, 63, 74, 81, 67, 76, 88, 73, 86, 78, 97, 72,…\n$ gad_screen     &lt;dbl&gt; 7, 17, 18, 8, 14, 11, 5, 8, 14, 5, 15, 16, 17, 13, 10, …\n$ phq9_screen    &lt;dbl&gt; 6, 13, 19, 4, 18, 8, 9, 8, 14, 3, 5, 11, 12, 18, 10, 4,…\n$ bbq_screen     &lt;dbl&gt; 60, 66, 4, 50, 22, 23, 47, 52, 31, 46, 67, 24, 57, 40, …\n$ scs_screen     &lt;dbl&gt; 25, 16, 22, 35, 29, 30, 20, 34, 21, 26, 35, 21, 32, 33,…\n$ dmrsodf_screen &lt;dbl&gt; 49178, 50727, 45074, 5381, 48444, 50899, 46923, 41428, …\n$ ders_screen    &lt;dbl&gt; 44, 73, 65, 45, 46, 49, 57, 38, 67, 45, 55, 56, 71, 36,…\n$ pid_5_screen   &lt;dbl&gt; 25, 20, 48, 17, 24, 20, 24, 26, 39, 23, 24, 26, 23, 15,…\n$ gender         &lt;chr&gt; \"Woman\", \"Man\", \"Man\", \"Man\", \"Woman\", \"Woman\", \"Woman\"…\n$ education      &lt;chr&gt; \"Primary\", \"University\", \"Secondary\", \"Secondary\", \"Uni…\n$ income         &lt;chr&gt; \"Medium\", \"Medium\", \"Medium\", \"Medium\", \"Medium\", \"Low\"…\n$ gad_cat        &lt;chr&gt; \"Low anxiety\", \"High anxiety\", \"High anxiety\", \"Low anx…\n$ phq_cat        &lt;chr&gt; \"Low depression\", \"High depression\", \"High depression\",…\n\nhead(d_bl)\n\n# A tibble: 6 × 15\n     id group lsas_screen gad_screen phq9_screen bbq_screen scs_screen\n  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1     1     2          63          7           6         60         25\n2     2     1          71         17          13         66         16\n3     3     0          98         18          19          4         22\n4     4     2          63          8           4         50         35\n5     5     2          74         14          18         22         29\n6     6     2          81         11           8         23         30\n# ℹ 8 more variables: dmrsodf_screen &lt;dbl&gt;, ders_screen &lt;dbl&gt;,\n#   pid_5_screen &lt;dbl&gt;, gender &lt;chr&gt;, education &lt;chr&gt;, income &lt;chr&gt;,\n#   gad_cat &lt;chr&gt;, phq_cat &lt;chr&gt;\n\nncol(d_bl)\n\n[1] 15\n\nnrow(d_bl)\n\n[1] 181\n\n\n2.5 Calculate spread measures for the LSAS-SR scale, what do they tell you and which ones do you think are most useful to describe the spread of the values. Motivate your answer briefly!\n\nsd(d_bl$lsas_screen)\n\n[1] 16.46812\n\nvar(d_bl$lsas_screen)\n\n[1] 271.199\n\nrange(d_bl$lsas_screen)\n\n[1]  60 134\n\nIQR(d_bl$lsas_screen)\n\n[1] 21\n\nhist(d_bl$lsas_screen)\n\n\n\n\n\n\n\n\nOvercourse:\n2.6 Calculate the population variance and standard deviation. How do these differ from the ones given by the functions sd() and var().\n\n# variance\nx_bar &lt;- mean(d_bl$lsas_screen)\nn &lt;- nrow(d_bl)\nresiduals &lt;- d_bl$lsas_screen - x_bar\nsquared_residuals &lt;- residuals^2\nsigma_squared_residuals &lt;- sum(squared_residuals)\ns_2 &lt;- sigma_squared_residuals / n\ns_2 # population variance\n\n[1] 269.7006\n\nvar(d_bl$lsas_screen) # sample variance\n\n[1] 271.199\n\n# standard deviation\nsd &lt;- sqrt(s_2)\nsd\n\n[1] 16.42256\n\nsd(d_bl$lsas_screen)\n\n[1] 16.46812\n\n\n2.7 Use only the first ten participants and compare the population and the sample variance and standard deviation of LSAS-SR. What do you find, and how do the results compare to those from exercise 2.6.\n\nd_bl_10 &lt;- d_bl[1:10, ]\n# variance\nx_bar &lt;- mean(d_bl_10$lsas_screen)\nn &lt;- nrow(d_bl_10)\nresiduals &lt;- d_bl_10$lsas_screen - x_bar\nsquared_residuals &lt;- residuals^2\nsigma_squared_residuals &lt;- sum(squared_residuals)\ns_2 &lt;- sigma_squared_residuals / n\ns_2 # population variance\n\n[1] 110.64\n\nvar(d_bl_10$lsas_screen) # sample variance\n\n[1] 122.9333\n\n# standard deviation\nsd &lt;- sqrt(s_2)\nsd\n\n[1] 10.51856\n\nsd(d_bl_10$lsas_screen)\n\n[1] 11.08753\n\n\n2.9 Visualize the joint distribution of GAD-7 and PHQ-9 as numeric variables and describe what you see\n\nplot(d_bl$gad_screen, d_bl$phq9_screen)\nabline(lm(d_bl$gad_screen ~ d_bl$phq9_screen))\n\n\n\n\n\n\n\n\n2.10 Visualize the distribution of of LSAS scores by income level and describe what you see\n\nboxplot(d_bl$lsas_screen ~ d_bl$income)\n\n\n\n\n\n\n\n\n2.11 Create a variable for high vs. low DERS scores and investigate the joint distribution of this variable and phq_cat (that we created in an earlier example)\n\nd_bl$ders_screen_cat &lt;- ifelse(d_bl$ders_screen &gt; median(d_bl$ders_screen),\n  \"High DERS\",\n  \"Low DERS\"\n)\n\nplot(table(d_bl$ders_screen_cat, d_bl$phq_cat), main = \"Depression and Emotion regulation\")\n\n\n\n\n\n\n\n\n2.12 Create a table using the tableone package to show descriptives statistics stratified by high vs low depression levels. Briefly interpret what you see.\n\nlibrary(tableone)\nvars &lt;- c(\n  \"lsas_screen\",\n  \"gad_screen\",\n  \"phq9_screen\",\n  \"bbq_screen\",\n  \"scs_screen\",\n  \"dmrsodf_screen\",\n  \"ders_screen\",\n  \"pid_5_screen\",\n  \"gender\",\n  \"education\",\n  \"income\"\n)\n\nCreateContTable(vars = vars, data = d_bl, strata = \"phq_cat\")\n\nWarning in CreateContTable(vars = vars, data = d_bl, strata = \"phq_cat\"):\nNon-numeric variables dropped\n\n\n                            Stratified by phq_cat\n                             High depression                         \n  n                          82                                      \n  lsas_screen (mean (SD))                 88.71 (17.74)              \n  gad_screen (mean (SD))                  12.85 (4.25)               \n  phq9_screen (mean (SD))                 13.59 (2.82)               \n  bbq_screen (mean (SD))                  36.16 (16.89)              \n  scs_screen (mean (SD))                  25.91 (6.39)               \n  dmrsodf_screen (mean (SD)) 589841463458342.12 (5341241621657216.00)\n  ders_screen (mean (SD))                 53.40 (13.45)              \n  pid_5_screen (mean (SD))                26.96 (8.66)               \n                            Stratified by phq_cat\n                             Low depression      p      test\n  n                          99                             \n  lsas_screen (mean (SD))       81.47 (14.63)     0.003     \n  gad_screen (mean (SD))         9.20 (4.03)     &lt;0.001     \n  phq9_screen (mean (SD))        6.32 (2.10)     &lt;0.001     \n  bbq_screen (mean (SD))        42.52 (15.64)     0.009     \n  scs_screen (mean (SD))        29.11 (7.23)      0.002     \n  dmrsodf_screen (mean (SD)) 42288.83 (18185.78)  0.273     \n  ders_screen (mean (SD))       45.73 (12.45)    &lt;0.001     \n  pid_5_screen (mean (SD))      21.41 (7.54)     &lt;0.001"
  },
  {
    "objectID": "exercises/solutions.html#lab-3",
    "href": "exercises/solutions.html#lab-3",
    "title": "Exercise solutions",
    "section": "Lab 3",
    "text": "Lab 3\n3.1 Estimate the standard error of LSAS_Screen in the STePS study using the formula above, and describe in words what the number means\n\ns_2 &lt;- var(d_bl$lsas_screen)\nn &lt;- nrow(d_bl)\nse &lt;- sqrt(s_2 / n)\n\n# or using the standard deviation\n\nse &lt;- sd(d_bl$lsas_screen) / sqrt(n)\n\n3.2 Calculate the standard error for the proportion of men in the STePS study, and describe the meaning of this number in words\n\np_hat &lt;- mean(d_bl$gender == \"Man\")\nn &lt;- nrow(d_bl)\nse &lt;- sqrt(p_hat * (1 - p_hat) / n)\n\n3.3 Describe what would happen to these standard errors if the sample size had been 1000 participants and explain why?\n3.4 Calculate the two-sided p-value for the null hypothesis that the mean PHQ-9 value in the underlying population is 9, and describe in words what this number mean.\n\nx_bar &lt;- mean(d_bl$phq9_screen)\nse &lt;- sd(d_bl$phq9_screen) / sqrt(nrow(d_bl))\nt_value &lt;- (x_bar - 9) / se\n(1 - pt(t_value, df = 180)) * 2\n\n[1] 0.06076406\n\n# or\n\nt.test(d_bl$phq9_screen,\n  mu = 9,\n  alternative = \"two.sided\"\n)\n\n\n    One Sample t-test\n\ndata:  d_bl$phq9_screen\nt = 1.887, df = 180, p-value = 0.06076\nalternative hypothesis: true mean is not equal to 9\n95 percent confidence interval:\n  8.971991 10.254529\nsample estimates:\nmean of x \n  9.61326 \n\n\n3.5 Calculate the p-value for getting our observed proportion of men, \\(\\hat{p}\\), if the the true population proportion, \\(p\\), is 40% or more using a z-test.\nHINT: use the standard error of the proportion: \\[ \\mathrm{SE}(p) = \\sqrt{\\frac{p(1 - p)}{n}} \\]\nand combine with the formula for the z-scores\n\\[ z= \\frac{p - \\hat{p}}{SE} \\]\n\np_hat &lt;- mean(d_bl$gender == \"Man\")\np &lt;- 0.4\nn &lt;- nrow(d_bl)\nse &lt;- sqrt(p * (1 - p) / n)\nz_value &lt;- (p_hat - p) / se\n1 - pnorm(abs(z_value))\n\n[1] 0.001622876\n\n# or\nprop.test(\n  x = sum(d_bl$gender == \"Man\"),\n  n = length(d_bl$gender),\n  p = 0.4,\n  alternative = \"less\",\n  correct = FALSE\n)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  sum(d_bl$gender == \"Man\") out of length(d_bl$gender), null probability 0.4\nX-squared = 8.6639, df = 1, p-value = 0.001623\nalternative hypothesis: true p is less than 0.4\n95 percent confidence interval:\n 0.0000000 0.3511775\nsample estimates:\n        p \n0.2928177 \n\n\n3.6 Modify the simulation code for the sampling distribution above to determine what would happen to the p-value if the sample size was 10, 100 or 1000\nP-value with a sample size of 10\n\nn_samples &lt;- 1e4 # the number of samples\nsmp_size &lt;- 10 # the size of our samples\nmeans &lt;- rep(NA, n_samples) # an empty vector to contain our mean values\n\nfor (i in 1:n_samples) {\n  x &lt;- rnorm(smp_size, mean = 82, sd = sd(d_bl$lsas_screen))\n  means[i] &lt;- mean(x)\n}\n\nmean(means &gt;= mean(d_bl$lsas_screen)) # proportion of simulated means that are larger than our observed mean\n\n[1] 0.2938\n\n\nP-value with a sample size of 100\n\nn_samples &lt;- 1e4 # the number of samples\nsmp_size &lt;- 100 # the size of our samples\nmeans &lt;- rep(NA, n_samples) # an empty vector to contain our mean values\n\nfor (i in 1:n_samples) {\n  x &lt;- rnorm(smp_size, mean = 82, sd = sd(d_bl$lsas_screen))\n  means[i] &lt;- mean(x)\n}\n\nmean(means &gt;= mean(d_bl$lsas_screen)) # proportion of simulated means that are larger than our observed mean\n\n[1] 0.0486\n\n\nP-value with a sample size of 10000\n\nn_samples &lt;- 1e4 # the number of samples\nsmp_size &lt;- 1000 # the size of our samples\nmeans &lt;- rep(NA, n_samples) # an empty vector to contain our mean values\n\nfor (i in 1:n_samples) {\n  x &lt;- rnorm(smp_size, mean = 82, sd = sd(d_bl$lsas_screen))\n  means[i] &lt;- mean(x)\n}\n\nmean(means &gt;= mean(d_bl$lsas_screen)) # proportion of simulated means that are larger than our observed mean\n\n[1] 0\n\n\n3.7. Explain why the confidence intervals calculated using z-scores are narrower that the ones using t-scores.\n3.8 Calculate the 95% confidence interval for PHQ-9, and describe in words what these numbers mean.\n\nx_bar &lt;- mean(d_bl$phq9_screen)\nse &lt;- sd(d_bl$phq9_screen) / sqrt(nrow(d_bl))\nz &lt;- 1.96\n\n# upper confidence limit\nucl &lt;- x_bar + z * se\n# lower confidence limit\nlcl &lt;- x_bar - z * se\n\nprint(c(lcl, ucl))\n\n[1]  8.976291 10.250229\n\n# or\nt.test(d_bl$phq9_screen,\n  mu = 9,\n  alternative = \"two.sided\"\n)\n\n\n    One Sample t-test\n\ndata:  d_bl$phq9_screen\nt = 1.887, df = 180, p-value = 0.06076\nalternative hypothesis: true mean is not equal to 9\n95 percent confidence interval:\n  8.971991 10.254529\nsample estimates:\nmean of x \n  9.61326 \n\n\n3.9 Calculate the 95% Wald confidence interval for the proportion of men in the dataset using the formula above and interpret its meaning\n\np_hat &lt;- mean(d_bl$gender == \"Man\")\nn &lt;- nrow(d_bl)\nse &lt;- sqrt(p_hat * (1 - p_hat) / n)\nz &lt;- 1.96\n# upper coinfidence limit\np_hat + z * se\n\n[1] 0.3591127\n\n# lower confidence limit\np_hat - z * se\n\n[1] 0.2265226\n\n\n3.10 Compare this to what you would obtain using the function prop.test() in R.\n\nprop.test(table(d_bl$gender))\n\n\n    1-sample proportions test with continuity correction\n\ndata:  table(d_bl$gender), null probability 0.5\nX-squared = 30.254, df = 1, p-value = 3.79e-08\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2288545 0.3657467\nsample estimates:\n        p \n0.2928177 \n\n\n3.11 Reason about the the meaning and interpretation of the confidence intervals you have calculated in the context of how the actual STePs study was performed. The study can be found at: https://www.nature.com/articles/s44184-024-00063-0"
  },
  {
    "objectID": "exercises/solutions.html#testing-two-means-and-contingency-tables",
    "href": "exercises/solutions.html#testing-two-means-and-contingency-tables",
    "title": "Exercise solutions",
    "section": "testing two means and contingency tables",
    "text": "testing two means and contingency tables\nCalculate the mean difference in post-treatment LSAS scores, and the associated two-sided p-value, between the therapist-guided and the wait list group\n\ndf_data %&gt;%\n  filter(trt != \"self-guided\") %&gt;%\n  t.test(lsas_post ~ trt, data = ., var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  lsas_post by trt\nt = -4.5763, df = 110, p-value = 1.25e-05\nalternative hypothesis: true difference in means between group therapist-guided and group waitlist is not equal to 0\n95 percent confidence interval:\n -30.23220 -11.96065\nsample estimates:\nmean in group therapist-guided         mean in group waitlist \n                      57.35185                       78.44828 \n\n# or manually\nx_bar_tg &lt;- mean(df_data$lsas_post[df_data$trt == \"therapist-guided\"], na.rm = T)\nx_bar_wl &lt;- mean(df_data$lsas_post[df_data$trt == \"waitlist\"], na.rm = T)\ns_2_tg &lt;- var(df_data$lsas_post[df_data$trt == \"therapist-guided\"], na.rm = T)\ns_2_wl &lt;- var(df_data$lsas_post[df_data$trt == \"waitlist\"], na.rm = T)\nn_tg &lt;- sum(!is.na(df_data$lsas_post[df_data$trt == \"therapist-guided\"]))\nn_wl &lt;- sum(!is.na(df_data$lsas_post[df_data$trt == \"waitlist\"]))\nsp2 &lt;- ((n_tg - 1) * s_2_tg + (n_wl - 1) * s_2_wl) / (n_tg + n_wl - 2) # pooled variance\nSE_pooled &lt;- sqrt(sp2 * (1 / n_tg + 1 / n_wl)) # pooled standard error\n\n# and put the together\nt_value &lt;- (x_bar_wl - x_bar_tg) / SE_pooled\n\n# find the p-value\ndf &lt;- n_tg + n_wl - 2\np_value &lt;- 2 * (1 - pt(abs(t_value), df)) # multiplied by two to get the two-tailed p-value\np_value\n\n[1] 1.250232e-05\n\n\nComplement this with a 95% confidence interval\n\n# z-value 95% CI\nse_z &lt;- sqrt((s_2_wl / n_wl) + (s_2_tg / n_tg))\nlcl &lt;- (x_bar_wl - x_bar_tg) - 1.96 * se_z\nucl &lt;- (x_bar_wl - x_bar_tg) + 1.96 * se_z\nprint(c(lcl, ucl))\n\n[1] 12.09103 30.10182\n\n# t-value 95% CI\ndf &lt;- n_wl + n_tg - 2\nalpha &lt;- 0.05\nt_crit &lt;- qt(1 - alpha / 2, df)\nlcl &lt;- (x_bar_wl - x_bar_tg) - t_crit * SE_pooled\nucl &lt;- (x_bar_wl - x_bar_tg) + t_crit * SE_pooled\nprint(c(lcl, ucl))\n\n[1] 11.96065 30.23220\n\n\nDo you think the assumption of equal variance between the groups is justifies?\nDo you think the other assumptions of the t-test are fulfilled?\nHow do the results differ if you use the Welch t-test instead\n\n# students t-test\ndf_data %&gt;%\n  filter(trt != \"waitlist\") %&gt;%\n  t.test(lsas_post ~ trt, data = ., var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  lsas_post by trt\nt = 1.7993, df = 109, p-value = 0.07474\nalternative hypothesis: true difference in means between group self-guided and group therapist-guided is not equal to 0\n95 percent confidence interval:\n -0.7676792 15.8885369\nsample estimates:\n     mean in group self-guided mean in group therapist-guided \n                      64.91228                       57.35185 \n\n# Welch t-test\ndf_data %&gt;%\n  filter(trt != \"waitlist\") %&gt;%\n  t.test(lsas_post ~ trt, data = ., var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  lsas_post by trt\nt = 1.7946, df = 106.62, p-value = 0.07555\nalternative hypothesis: true difference in means between group self-guided and group therapist-guided is not equal to 0\n95 percent confidence interval:\n -0.7913918 15.9122495\nsample estimates:\n     mean in group self-guided mean in group therapist-guided \n                      64.91228                       57.35185 \n\n# very similar resluts\n\nBONUS: If you feel up to it, try to also calculate a z-test for the mean difference in post-treatment LSAS-scores using the formulas above\n\n# z-test of the differences\nz_value &lt;- (x_bar_wl - x_bar_tg) / se_z\np_value &lt;- 2 * (1 - pnorm(abs(z_value)))\np_value\n\n[1] 4.399067e-06\n\n\nBONUS: Modify the code above to calculate the 99% confidence interval of the mean difference in LSAS-scores at post-treatment between the self-guided and the therapist-guided groups.\n\nz_crit &lt;- qnorm(1 - 0.01 / 2) # finding the z-value for a 99% CI\n\n# define the components of the formula\nx_bar_tg &lt;- mean(df_data$lsas_post[df_data$trt == \"therapist-guided\"], na.rm = T)\nx_bar_sg &lt;- mean(df_data$lsas_post[df_data$trt == \"self-guided\"], na.rm = T)\ns_2_tg &lt;- var(df_data$lsas_post[df_data$trt == \"therapist-guided\"], na.rm = T)\ns_2_sg &lt;- var(df_data$lsas_post[df_data$trt == \"self-guided\"], na.rm = T)\nn_tg &lt;- sum(!is.na(df_data$lsas_post[df_data$trt == \"therapist-guided\"]))\nn_sg &lt;- sum(!is.na(df_data$lsas_post[df_data$trt == \"self-guided\"]))\nse_z &lt;- sqrt((s_2_tg / n_tg) + (s_2_sg / n_sg))\n\n# and put it together with the z-value formula\n\nucl &lt;- (x_bar_sg - x_bar_tg) + z_crit * se_z\nlcl &lt;- (x_bar_sg - x_bar_tg) - z_crit * se_z\nprint(c(lcl, ucl))\n\n[1] -3.291151 18.412009\n\n# or\ndf_data %&gt;%\n  filter(trt != \"waitlist\") %&gt;%\n  t.test(lsas_post ~ trt, data = ., var.equal = TRUE, conf.level = 0.99)\n\n\n    Two Sample t-test\n\ndata:  lsas_post by trt\nt = 1.7993, df = 109, p-value = 0.07474\nalternative hypothesis: true difference in means between group self-guided and group therapist-guided is not equal to 0\n99 percent confidence interval:\n -3.455748 18.576605\nsample estimates:\n     mean in group self-guided mean in group therapist-guided \n                      64.91228                       57.35185 \n\n\nCompute a t-test for the difference in LSAS-scores between post-treatment and 12-month follow-up and provide an interpretation of its meaning\n\n# using the t-test function\nt.test(df_data$lsas_fu12, df_data$lsas_post, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  df_data$lsas_fu12 and df_data$lsas_post\nt = -2.512, df = 97, p-value = 0.01366\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -8.3112199 -0.9744944\nsample estimates:\nmean difference \n      -4.642857 \n\n# or manually\ndiff_post_12FU &lt;- df_data$lsas_fu12 - df_data$lsas_post\nmean_diff &lt;- mean(diff_post_12FU, na.rm = TRUE)\nsd_diff &lt;- sd(diff_post_12FU, na.rm = TRUE)\nn &lt;- sum(!is.na(diff_post_12FU))\nse_diff &lt;- sd_diff / sqrt(n)\nt_value &lt;- mean_diff / se_diff\ndf &lt;- n - 1\np_value &lt;- 2 * (1 - pt(abs(t_value), df))\np_value\n\n[1] 0.01365707\n\n\nAlso calculate the 95% confidence interval for this difference, using the z-value formula and provide an interpretation of its meaning\n\ndiff_post_12FU &lt;- df_data$lsas_fu12 - df_data$lsas_post\nmean_diff &lt;- mean(diff_post_12FU, na.rm = TRUE)\nsd_diff &lt;- sd(diff_post_12FU, na.rm = TRUE)\nn &lt;- sum(!is.na(diff_post_12FU))\nse_diff &lt;- sd_diff / sqrt(n)\n\n# and putting it together\nlcl &lt;- mean_diff - 1.96 * se_diff\nucl &lt;- mean_diff + 1.96 * se_diff\nprint(c(lcl, ucl))\n\n[1] -8.265524 -1.020190\n\n\nCompare the means of GAD-7 from pre- to post-treatment and interpret the results\n\nt.test(df_data$gad_post, df_data$gad_screen, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  df_data$gad_post and df_data$gad_screen\nt = -6.052, df = 167, p-value = 9.133e-09\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.141877 -1.596218\nsample estimates:\nmean difference \n      -2.369048 \n\n\nChi-squared tests, and other tests of significance, are sometimes used to check that important pre-treatment characteristics, such as gender or symptom level, are balanced between the treatment groups. Non-sinificant p-values are then taken as an argument that the groups are balanced. Reason about why this is a problematic approach.\nAnswer: Because a non-significant result is not evidence of no difference. With small samples, even large differences may be non-significant. Conversely, in large samples even the smallest difference may be statistically significant.\nCreate a categorical for high or low generalized anxiety and one for high and low social anxiety, and use the Chi squared test to test the null hypothesis of no association between the variables\n\ndf_data &lt;- df_data %&gt;%\n  mutate(\n    gad_cat_screen = if_else(\n      gad_screen &gt; median(gad_screen),\n      \"High GAD\", \"Low GAD\"\n    ),\n    sad_cat_screen = if_else(\n      lsas_screen &gt; median(lsas_screen),\n      \"High SAD\", \"Low SAD\"\n    )\n  )\n\nchisq.test(df_data$gad_cat_screen, df_data$sad_cat_screen)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  df_data$gad_cat_screen and df_data$sad_cat_screen\nX-squared = 2.4198, df = 1, p-value = 0.1198\n\nchisq.test(df_data$gad_cat_screen, df_data$sad_cat_screen)$observed\n\n                      df_data$sad_cat_screen\ndf_data$gad_cat_screen High SAD Low SAD\n              High GAD       49      39\n              Low GAD        40      53\n\nchisq.test(df_data$gad_cat_screen, df_data$sad_cat_screen)$expected\n\n                      df_data$sad_cat_screen\ndf_data$gad_cat_screen High SAD  Low SAD\n              High GAD 43.27072 44.72928\n              Low GAD  45.72928 47.27072"
  },
  {
    "objectID": "exercises/lab-webr-testing-two-means.html",
    "href": "exercises/lab-webr-testing-two-means.html",
    "title": "Lab webr testing two means",
    "section": "",
    "text": "Exercise 1HintsSolution\n\n\nCalculate the mean difference in post-treatment LSAS scores, and the associated two-sided p-value for the mean difference between the therapist-guided and the wait list group\n\n\n\n\n\n\n\n\n\n\n\nYou can calculate it using the t.test() function\ndf_data %&gt;%\n  filter(trt != \"_____\") %&gt;%\n  t.test(______ ~ trt, data = ., var.equal = TRUE)\nOr if you want to do it manually, you can use the following formulas\nTo test the difference between two means, we can use the Student’s independent groups t-test:\n\\[\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{SE_{\\text{pooled}}}\n\\]\nWhere \\(\\bar{X_1}\\) and \\(\\bar{X_2}\\) are the means of the first and second group, and where the pooled standard error is:\n\\[\nSE_{\\text{pooled}} = \\sqrt{s_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\n\\]\nand the pooled variance is:\n\\[\ns_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\n\n\n\n\nThe full solution is:\n#using the t.test() function\ndf_data %&gt;%\n  filter(trt != \"self-guided\") %&gt;%\n  t.test(lsas_post ~ trt, data = ., var.equal = TRUE)\n\n# or manually\nx_bar_tg &lt;- mean(df_data$lsas_post[df_data$trt==\"therapist-guided\"], na.rm=T)\nx_bar_wl &lt;- mean(df_data$lsas_post[df_data$trt==\"waitlist\"], na.rm=T)\ns_2_tg &lt;- var(df_data$lsas_post[df_data$trt==\"therapist-guided\"], na.rm=T)\ns_2_wl &lt;- var(df_data$lsas_post[df_data$trt==\"waitlist\"], na.rm=T)\nn_tg &lt;- sum(!is.na(df_data$lsas_post[df_data$trt==\"therapist-guided\"]))\nn_wl &lt;- sum(!is.na(df_data$lsas_post[df_data$trt==\"waitlist\"]))\nsp2 &lt;- ((n_tg - 1) * s_2_tg + (n_wl - 1) * s_2_wl) / (n_tg + n_wl - 2) # pooled variance\nSE_pooled &lt;- sqrt(sp2 * (1/n_tg + 1/n_wl)) # pooled standard error\n\n# and put the together\nt_value &lt;- (x_bar_wl - x_bar_tg) / SE_pooled\n\n#find the p-value\ndf &lt;- n_tg + n_wl -2 \np_value &lt;- 2 * (1 - pt(abs(t_value), df)) # multiplied by two to get the two-tailed p-value\np_value\n\n\n\n\n\n\n\n\nExercise 2HintsSolution\n\n\nDo you think the assumption of equal variance between the groups is justified?\n\n\n\n\n\n\n\n\n\n\n\nYou can check the variance of each group\nvar(df_data$______[df_data$trt==\"_____\"], na.rm=T)\nvar(df_data$_____[df_data$trt==\"therapist-guided\"], na.rm=T)\n\n\n\n\nThe full solution is:\nvar(df_data$lsas_post[df_data$trt==\"waitlist\"], na.rm=T)\nvar(df_data$lsas_post[df_data$trt==\"therapist-guided\"], na.rm=T)\nIn these results show rather similar variances, although not equal\n\n\n\n\n\n\n\n\nExercise 3HintsSolution\n\n\nHow do the results differ if you use the Welch t-test instead, that does not assume equal variances?\n\n\n\n\n\n\n\n\n\n\n\nYou can again calculate it using the t.test() function, but need to change the setting of the var.equal argument\ndf_data %&gt;%\n  filter(trt != \"_____\") %&gt;%\n  t.test(______ ~ trt, data = ., var.equal = _____)\n\n\n\n\nThe full solution is:\ndf_data %&gt;%\n  filter(trt != \"self-guided\") %&gt;%\n  t.test(lsas_post ~ trt, data = ., var.equal = FALSE)\nResults are very similar to when the variances where assumed to equal, so the difference in variance between the groups do not 0affect our results to any large degree\n\n\n\n\n\n\n\n\nExercise 4HintsSolution\n\n\nAlso calculate the 95% confidence interval for the difference in LSAS post-treatment scores between the therapist-guided and waitlist group\n\n\n\n\n\n\n\n\n\n\n\nYou can again calculate it using the t.test() function\ndf_data %&gt;%\n  filter(trt != \"_____\") %&gt;%\n  t.test(______ ~ trt, data = ., var.equal = TRUE)\nOr if you want to do it manually, use the formula for the z-score 95% CI of mean differences\n\\[\n(\\bar{X}_1 - \\bar{X}_2) \\;\\pm\\; z_{\\alpha/2} \\cdot SE_{z}\n\\]\nwhere\n\\[\nSE_{z} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n\\]\n\n\n\n\nThe full solution is:\n#using the t.test() function\ndf_data %&gt;%\n  filter(trt != \"self-guided\") %&gt;%\n  t.test(lsas_post ~ trt, data = ., var.equal = TRUE)\n\n# or manually\n\n#get the parts of the formula\nx_bar_tg &lt;- mean(df_data$lsas_post[df_data$trt == \"therapist-guided\"], na.rm = T)\nx_bar_wl &lt;- mean(df_data$lsas_post[df_data$trt == \"waitlist\"], na.rm = T)\ns_2_tg &lt;- var(df_data$lsas_post[df_data$trt == \"therapist-guided\"], na.rm = T)\ns_2_wl &lt;- var(df_data$lsas_post[df_data$trt == \"waitlist\"], na.rm = T)\nn_tg &lt;- sum(!is.na(df_data$lsas_post[df_data$trt == \"therapist-guided\"]))\nn_wl &lt;- sum(!is.na(df_data$lsas_post[df_data$trt == \"waitlist\"]))\n\n# z-value 95% CI\nse_z &lt;- sqrt((s_2_wl / n_wl) + (s_2_tg / n_tg))\nlcl &lt;- (x_bar_wl - x_bar_tg) - 1.96 * se_z\nucl &lt;- (x_bar_wl - x_bar_tg) + 1.96 * se_z\nprint(c(lcl, ucl))\n\n\n\n\n\n\n\n\nExercise 5HintsSolution\n\n\nConduct a dependent t-test for the difference in LSAS-scores between post-treatment and 12-month follow-up and interprets its meaning\n\n\n\n\n\n\n\n\n\n\n\nYou can again calculate it using the t.test() function\nt.test(df_data$_____, df_data$lsas_post, paired = ____)\nOr if you want to do it manually:\nFor a paired samples t-test, the statistic is:\n\\[\nt = \\frac{\\bar{D}}{SE_D}\n\\]\nwhere:\n\\(\\bar{D}\\) = mean of the difference scores\n\\(SE_D\\) = the standard error of the difference scores calculated as \\(s_D/ \\sqrt{n}\\) = the standard error of the differences, with \\(s_D\\) = the standard deviation of the differences and \\(n\\) = the number of paired observations\n\n\n\n\nThe full solution is:\n# using the t-test function\nt.test(df_data$lsas_fu12, df_data$lsas_post, paired = TRUE)\n\n# or manually\ndiff_post_12FU &lt;- df_data$lsas_fu12 - df_data$lsas_post\nmean_diff &lt;- mean(diff_post_12FU, na.rm = TRUE)\nsd_diff &lt;- sd(diff_post_12FU, na.rm = TRUE)\nn &lt;- sum(!is.na(diff_post_12FU))\nse_diff &lt;- sd_diff / sqrt(n)\nt_value &lt;- mean_diff / se_diff\ndf &lt;- n - 1\np_value &lt;- 2 * (1 - pt(abs(t_value), df))\np_value\nThe interpretation is that, for the full group, the LSAS score at the 12-month follow-up show a significant difference to the post-treatment LSAS score.\n\n\n\n\n\n\n\n\nBONUS Exercise 6HintsSolution\n\n\nAlso calculate the 95% confidence interval for this difference, using the z-value formula and provide an interpretation of its meaning\n\n\n\n\n\n\n\n\n\n\n\nWe can get the z-score confidence intervals of this difference, using a familiar formula for large samples\n\\[\n\\bar{D} \\;\\pm\\; z_{\\alpha/2} \\cdot SE_D\n\\] where \\(SE_D\\) = the standard error of the difference scores calculated as \\(s_D/ \\sqrt{n}\\) = the standard error of the differences, with \\(s_D\\) = the standard deviation of the differences and \\(n\\) = the number of paired observations\n\n\n\n\nThe full solution is:\ndiff_post_12FU &lt;- df_data$lsas_fu12 - df_data$lsas_post\nmean_diff &lt;- mean(diff_post_12FU, na.rm = TRUE)\nsd_diff &lt;- sd(diff_post_12FU, na.rm = TRUE)\nn &lt;- sum(!is.na(diff_post_12FU))\nse_diff &lt;- sd_diff / sqrt(n)\n\n# and putting it together\nlcl &lt;- mean_diff - 1.96 * se_diff\nucl &lt;- mean_diff + 1.96 * se_diff\nprint(c(lcl, ucl))",
    "crumbs": [
      "Exercises",
      "Testing two means"
    ]
  },
  {
    "objectID": "exercises/lab-webr-test-exercises.html",
    "href": "exercises/lab-webr-test-exercises.html",
    "title": "Lab webr test exercises",
    "section": "",
    "text": "Exercise 1HintsSolution\n\n\nSelect all the columns that start with lsas and the id and group columns.\n\n\n\n\n\n\n\n\n\n\n\nUse select() and the starts_with() function.\ndf_data |&gt; \n  select(id, group, starts_with(______))\n\n\n\n\nThe full solution is:\ndf_data |&gt; \n1  select(id, group, starts_with(\"lsas\"))\n\n1\n\nSelect the id, group, and all columns that start with lsas.",
    "crumbs": [
      "Exercises",
      "Webr test exercises"
    ]
  },
  {
    "objectID": "exercises/lab-webr-descriptive-statistics.html",
    "href": "exercises/lab-webr-descriptive-statistics.html",
    "title": "Lab webr descriptive statistics",
    "section": "",
    "text": "Exercise 1HintsSolution\n\n\nUse the functions described in the Import and clean data lab to get a quick overview of the dataset called df_dataand give a brief summary of it.\n\n\n\n\n\n\n\n\n\n\n\nHave a look at Import and clean data\n\n\n\n\nThe full solution is:\nglimpse(d_bl)\nhead(d_bl)\nncol(d_bl)\nnrow(d_bl)\n\n\n\n\n\n\n\n\nExercise 2HintsSolution\n\n\nVisualize the distribution of the PHQ-9 scale and the PID-5 scale and provide the mean, median and mode.\n\n\n\n\n\n\n\n\n\n\n\nHave a look at Import and clean data\n\n\n\n\nThe full solution is:\n#PHQ-9\nhist(df_data$phq9_screen)\nmean(df_data$phq9_screen)\nmedian(df_data$phq9_screen)\nget_mode(df_data$phq9_screen)\n\n#PID-5\nhist(df_data$pid_5_screen)\nmean(df_data$pid_5_screen)\nmedian(df_data$pid_5_screen)\nget_mode(df_data$pid_5_screen)\n\n\n\n\n\nExercise 3HintsSolution\n\n\nHow does the centrality measures differ and why?\n\n\nThink about mean, median, and mode and how they are influenced by the shape of the distribution or outliers.\n\n\nThe mean, median, and mode differ because they describe different aspects of the data:\n\nThe mean is sensitive to extreme values and gives the arithmetic average.\nThe median is the middle value, unaffected by outliers, and reflects the central point in skewed distributions.\nThe mode identifies the most frequent value and is useful for categorical or discrete variables.\n\nThey differ because of how they respond to skewness and outliers in the data.\n\n\n\n\nExercise 4HintsSolution\n\n\nReason on the pros and cons of the different centrality measures for these scales.\n\n\nConsider which measure gives the most typical picture and when one measure might mislead.\n\n\n\nMean\n\nPros: Uses all data values, good for symmetric distributions.\n\nCons: Highly affected by skewness and outliers.\n\nMedian\n\nPros: Robust to outliers, better for skewed distributions.\n\nCons: Ignores the magnitude of extreme values, less informative for symmetric data.\n\nMode\n\nPros: Useful for categorical data and identifying the most common response.\n\nCons: Can be unstable if multiple modes exist or if the distribution is flat.\n\n\nFor these psychological scales, the median is often more informative if the distributions are skewed, while the mean is more common when data is approximately normal. The mode can be informative but is less often used for continuous scales.\n\n\n\n\n\n\n\nExercise 5HintsSolution\n\n\nCalculate some spread measures for the LSAS-SR scale, what do they tell you and which ones do you think are most useful to describe the spread of the values. Motive you answer briefly!\n\n\n\n\n\n\n\n\n\n\n\nHave a look at Descriptive statistics\n\n\n\n\nThe full solution is:\nsd(df_data$lsas_screen)\nvar(df_data$lsas_screen)\nrange(df_data$lsas_screen)\nIQR(df_data$lsas_screen)\nhist(df_data$lsas_screen)\nwhere the historgram helps you determine which of the spread measures is most useful\n\n\n\n\n\n\n\n\nExercise 6HintsSolution\n\n\nCalculate the counts, proportions and percentages for the simulated income categories and visualize the distribution\n\n\n\n\n\n\n\n\n\n\n\nTo calculate the proportion, you will need to compare the counts with the total number of rows in the dataset nrow().\n\n\n\n\nThe full solution is:\n# counts\ntable(df_data$income)\n\n# proportions\ntable(df_data$income)/nrow(df_data)\n\n# percentages\ntable(df_data$income)/nrow(df_data)*100 \nwhere the histogram helps you determine which of the spread measures is most useful\n\n\n\n\n\n\n\n\nExercise 7HintsSolution\n\n\nVisualize the joint distribution of GAD-7 and PHQ-9 as numeric variables and describe what you see. Which plot is the most useful for this purpose?\n\n\n\n\n\n\n\n\n\n\n\nOne useful way to visualize joint distributions of numeric variables is to create a scatter plot. See Descriptive statistics\n\n\n\n\nThe full solution is:\nplot(df_data$gad_screen, df_data$phq9_screen)\n\n\n\n\n\n\n\n\nExercise 8HintsSolution\n\n\nVisualize the distribution of of LSAS scores by income level and describe what you see. Which plot is the most useful for this purpose?\n\n\n\n\n\n\n\n\n\n\n\nOne useful way to visualize joint distributions of numeric variables and categoric variables is to create a grouped boxplot. See Descriptive statistics\n\n\n\n\nThe full solution is:\nboxplot(df_data$lsas_screen ~df_data$income,\n        ylab = \"LSAS-SR\",\n        xlab = \"Income\")\n\n\n\n\n\n\n\n\nExercise 9HintsSolution\n\n\nCreate a table using the tableone package to show descriptive statistics stratified by high vs low depression levels. Briefly interpret what you see.\n\n\n\n\n\n\n\n\n\n\n\nSee Descriptive statistics\n\n\n\n\nThe full solution is:\n# define the variables you want\nvars &lt;- c(\n  \"lsas_screen\",\n  \"gad_screen\",\n  \"phq9_screen\",\n  \"bbq_screen\",\n  \"scs_screen\",\n  \"dmrsodf_screen\",\n  \"ders_screen\",\n  \"pid_5_screen\"\n)\n\nCreateTableOne(vars= vars, data = df_data, strata = \"phq_cat\", test = FALSE)\n\n\n\n\n\n\n\n\n\nBONUS Exercise 10HintsSolution\n\n\nCalculate the population variance and standard deviation of LSAS. How and why do these differ from the ones given by the functions sd() and var()?\n\n\n\n\n\n\n\n\n\n\n\nSee Descriptive statistics\nYou can use the following code to create functions for the population variance and SD:\n#creating a function to calculate the population variance \npop_var &lt;- function(x){\n  1/length(x)*sum((x-mean(x))^2)\n}\n\n# and the population sd\npop_sd &lt;- function(x){\n  sqrt(1/length(x)*sum((x-mean(x))^2))\n}\n\n\n\n\nThe full solution is:\n#creating a function to calculate the population variance \npop_var &lt;- function(x){\n  1/length(x)*sum((x-mean(x))^2)\n}\n\n# and the population sd\npop_sd &lt;- function(x){\n  sqrt(1/length(x)*sum((x-mean(x))^2))\n}\n\n# variance\npop_var(df_data$lsas_screen) # population\nvar(df_data$lsas_screen) #sample\n\n# SD\npop_sd(df_data$lsas_screen) # population \nsd(df_data$lsas_screen) # sample\n\n\n\n\n\n\n\n\nBONUS Exercise 11HintsSolution\n\n\nUse only the first ten participants and compare the population and the sample variance and standard deviation of LSAS-SR. What do you find, and how do the results compare to those from the previous exercise?\n\n\n\n\n\n\n\n\n\n\n\nYou can use the following code to get the first 10 rows of your data\ndf_data_10 &lt;- df_data[1:10,]\n\n\n\n\nThe full solution is:\n#creating a function to calculate the population variance \npop_var &lt;- function(x){\n  1/length(x)*sum((x-mean(x))^2)\n}\n\n# and the population sd\npop_sd &lt;- function(x){\n  sqrt(1/length(x)*sum((x-mean(x))^2))\n}\n\n#create dataset with only the first 10 participants\ndf_data_10 &lt;- df_data[1:10,]\n\n# variance\npop_var(df_data_10$lsas_screen)\nvar(df_data_10$lsas_screen)\n\n# SD\npop_sd(df_data_10$lsas_screen)\nsd(df_data_10$lsas_screen)",
    "crumbs": [
      "Exercises",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/probability-rules.html",
    "href": "labs/probability-rules.html",
    "title": "Probability rules",
    "section": "",
    "text": "In this lab, we will look at how we can work with probability rules in R.",
    "crumbs": [
      "Labs",
      "Probability rules"
    ]
  },
  {
    "objectID": "labs/probability-rules.html#check-that-the-sum-of-all-probabilities-is-1",
    "href": "labs/probability-rules.html#check-that-the-sum-of-all-probabilities-is-1",
    "title": "Probability rules",
    "section": "Check that the sum of all probabilities is 1",
    "text": "Check that the sum of all probabilities is 1\nWe do a quick check of the education variable, which has three levels: “Primary”, “Secondary”, and “University”. When we count the proportion of each level, we get the following:\nThe proportion with the “Primary” level is 0.4917127, the proportion with the “Secondary” level is 0.3480663, and the proportion with the “University” level is 0.160221. These add up to 1. All good!\n\n\n\n\n\n\nExercise 1: Check the sum of probabilities in the income variable\n\n\n\n\n\nDo the numbers in our income variable add up to 1? You can use the income_summary object we created above.",
    "crumbs": [
      "Labs",
      "Probability rules"
    ]
  },
  {
    "objectID": "labs/probability-rules.html#complement-rule",
    "href": "labs/probability-rules.html#complement-rule",
    "title": "Probability rules",
    "section": "Complement rule",
    "text": "Complement rule\nThe probability of an event not occurring is 1 minus the probability that it will occur.\nLet’s check this for the “Secondary” level.\n\\[\nP(\\text{not Secondary}) = 1 - P(\\text{Secondary})\n\\]\n\n# probability of Secondary education\np_secondary &lt;- edu_summary |&gt;\n  filter(education == \"Secondary\") |&gt;\n  pull(proportion)\n\n# complement rule\np_not_secondary &lt;- 1 - p_secondary\n\n\nP(Secondary education) = 0.3481\nP(not Secondary education) = 0.6519\nCheck complement rule, sum = 1\n\n\n\n\n\n\n\nExercise 2: Complement rule\n\n\n\n\n\nCalculate the complement rule for Medium income.",
    "crumbs": [
      "Labs",
      "Probability rules"
    ]
  },
  {
    "objectID": "labs/probability-rules.html#addition-rule",
    "href": "labs/probability-rules.html#addition-rule",
    "title": "Probability rules",
    "section": "Addition rule",
    "text": "Addition rule\nThe probability that event A or event B occurs (or both).\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\nLet’s implement this using our dataset. We’ll look at the probability of having either “Primary” education OR being in the “Low” income group.\n\n# get probabilities\np_primary &lt;- edu_summary |&gt;\n  filter(education == \"Primary\") |&gt;\n  pull(proportion)\n\np_low_income &lt;- income_summary |&gt;\n  filter(income == \"Low\") |&gt;\n  pull(proportion)\n\n# calculate probability of both Primary education AND Low income\np_both &lt;- d_bl |&gt;\n  filter(education == \"Primary\" & income == \"Low\") |&gt;\n  nrow() / nrow(d_bl)\n\n# addition rule\np_either &lt;- p_primary + p_low_income - p_both\n\n\nP(Primary education) = 0.4917\nP(Low income) = 0.2431\nP(Both) = 0.1215\nP(Either) = 0.6133\n\n\n\n\n\n\n\nExercise 3: Addition rule\n\n\n\n\n\nCalculate the probability of having either “University” education OR being in the “Medium” income group.",
    "crumbs": [
      "Labs",
      "Probability rules"
    ]
  },
  {
    "objectID": "labs/probability-rules.html#multiplication-rule",
    "href": "labs/probability-rules.html#multiplication-rule",
    "title": "Probability rules",
    "section": "Multiplication rule",
    "text": "Multiplication rule\nFor independent events, the probability of both events occurring is the product of their individual probabilities:\n\\[P(A \\cap B) = P(A) \\times P(B)\\]\nFor dependent events, we need to account for the conditional probability:\n\\[P(A \\cap B) = P(A) \\times P(B|A)\\]\nLet’s check if education and income are independent by comparing the observed joint probability with the product of marginal probabilities. We will first use group_by() and summarise() to create Table 3.\n\nedu_income_table &lt;- d_bl |&gt;\n  group_by(education, income) |&gt;\n  summarise(\n    n = n(),\n    proportion = n / nrow(d_bl),\n    percent = round(proportion * 100, 1),\n    .groups = \"drop\"\n  )\n\nkable(edu_income_table)\n\n\n\nTable 3: Joint probabilities of education and income\n\n\n\n\n\n\neducation\nincome\nn\nproportion\npercent\n\n\n\n\nPrimary\nHigh\n15\n0.0828729\n8.3\n\n\nPrimary\nLow\n22\n0.1215470\n12.2\n\n\nPrimary\nMedium\n52\n0.2872928\n28.7\n\n\nSecondary\nHigh\n9\n0.0497238\n5.0\n\n\nSecondary\nLow\n16\n0.0883978\n8.8\n\n\nSecondary\nMedium\n38\n0.2099448\n21.0\n\n\nUniversity\nHigh\n7\n0.0386740\n3.9\n\n\nUniversity\nLow\n6\n0.0331492\n3.3\n\n\nUniversity\nMedium\n16\n0.0883978\n8.8\n\n\n\n\n\n\n\n\n\n# Check independence for Primary education and Low income\np_primary_indep &lt;- p_primary * p_low_income\np_primary_dep &lt;- p_both\n\n\nIf independent: P(Primary ∩ Low income) = 0.119532\nObserved: P(Primary ∩ Low income) = 0.121547\nDifference = 0.002015\n\n\n\n\n\n\n\nTip\n\n\n\nKeep in mind that this is simulated data, so the numbers may not represent the real world. Nonetheless, if we observed a result like this, what would we conclude?",
    "crumbs": [
      "Labs",
      "Probability rules"
    ]
  },
  {
    "objectID": "labs/probability-rules.html#conditional-probability",
    "href": "labs/probability-rules.html#conditional-probability",
    "title": "Probability rules",
    "section": "Conditional probability",
    "text": "Conditional probability\nThe probability of event B occurring given that event A has occurred:\n\\[P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\]\nLet’s calculate the probability of having “Low” income given that someone has “Primary” education:\n\np_low_given_primary &lt;- p_both / p_primary\n\n\nP(Low income | Primary education) = 0.2472\n\n\n\n\n\n\n\nExercise 4: Conditional probability\n\n\n\n\n\nCalculate the probability of having “Medium” income given that someone has “University” education.",
    "crumbs": [
      "Labs",
      "Probability rules"
    ]
  },
  {
    "objectID": "labs/import-clean.html",
    "href": "labs/import-clean.html",
    "title": "Import and clean data",
    "section": "",
    "text": "In this chapter, we will:",
    "crumbs": [
      "Labs",
      "Import and clean data"
    ]
  },
  {
    "objectID": "labs/import-clean.html#steps-study",
    "href": "labs/import-clean.html#steps-study",
    "title": "Import and clean data",
    "section": "STePS-study",
    "text": "STePS-study\nThis is a dataset from the STePS study, which is a RCT comparing guided and unguided internet-delivered psychodynamic therapy for social anxiety disorder. The study is published online.\nIn true open science fashion, the data is openly available online from the Open Science Framework.",
    "crumbs": [
      "Labs",
      "Import and clean data"
    ]
  },
  {
    "objectID": "labs/import-clean.html#check-data-structure",
    "href": "labs/import-clean.html#check-data-structure",
    "title": "Import and clean data",
    "section": "Check data structure",
    "text": "Check data structure\nAfter importing the data, we can check the structure of the dataset using the glimpse() function. This function provides a quick overview of the dataset, including the number of rows and columns, as well as the data types of each column. Does it look as expected?\nYour object df_rawdata should contain 181 rows and 37 columns. We can see that the first column is named ID, which is the unique identifier for each participant. The second column is named Group, which indicates the group assignment: unguided treatment, guided treatment, or waitlist. The rest seem to be various questionnaires and scales.\n\nglimpse(df_rawdata)\n\nRows: 181\nColumns: 37\n$ ID               &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ Group            &lt;dbl&gt; 2, 1, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 2, 1, 1, 2, 0,…\n$ `LSAS Screening` &lt;dbl&gt; 63, 71, 98, 63, 74, 81, 67, 76, 88, 73, 86, 78, 97, 7…\n$ GAD_screen       &lt;dbl&gt; 7, 17, 18, 8, 14, 11, 5, 8, 14, 5, 15, 16, 17, 13, 10…\n$ `PHQ-9 screen`   &lt;dbl&gt; 6, 13, 19, 4, 18, 8, 9, 8, 14, 3, 5, 11, 12, 18, 10, …\n$ BBQ_screen       &lt;dbl&gt; 60, 66, 4, 50, 22, 23, 47, 52, 31, 46, 67, 24, 57, 40…\n$ SCS_screen       &lt;dbl&gt; 25, 16, 22, 35, 29, 30, 20, 34, 21, 26, 35, 21, 32, 3…\n$ DMRSODF_screen   &lt;dbl&gt; 49178, 50727, 45074, 5381, 48444, 50899, 46923, 41428…\n$ `DERS-16_screen` &lt;dbl&gt; 44, 73, 65, 45, 46, 49, 57, 38, 67, 45, 55, 56, 71, 3…\n$ `PID-5_screen`   &lt;dbl&gt; 25, 20, 48, 17, 24, 20, 24, 26, 39, 23, 24, 26, 23, 1…\n$ LSAS_V1          &lt;chr&gt; \"72\", \"missing\", \"81\", \"44\", \"39\", \"65\", \"68\", \"69\", …\n$ LSAS_V2          &lt;chr&gt; \"64\", \"missing\", \"89\", \"33\", \"115\", \"64\", NA, \"70\", \"…\n$ LSAS_V3          &lt;chr&gt; \"72\", \"missing\", \"73\", \"36\", \"missing\", \"63\", NA, \"71…\n$ LSAS_V4          &lt;chr&gt; \"61\", \"missing\", \"94\", \"44\", \"missing\", \"60\", NA, \"51…\n$ LSAS_V5          &lt;chr&gt; \"61\", NA, \"93\", \"21\", \"missing\", \"55\", NA, \"55\", NA, …\n$ LSAS_V6          &lt;chr&gt; \"46\", NA, \"88\", \"20\", \"missing\", \"46\", NA, \"56\", \"93\"…\n$ LSAS_V7          &lt;dbl&gt; 55, NA, NA, 18, NA, 45, NA, 64, NA, 54, 101, NA, 89, …\n$ LSAS_V8          &lt;dbl&gt; 49, NA, NA, 17, NA, 44, NA, 52, NA, 61, 84, NA, 89, N…\n$ LSAS_POST        &lt;dbl&gt; 50, NA, 77, 22, NA, 52, 75, 45, 79, 64, 80, 89, 83, 4…\n$ GAD_POST         &lt;dbl&gt; 4, NA, 19, 6, NA, 9, 4, 3, 10, 7, 15, 7, 11, 11, 11, …\n$ `PHQ-9_POST`     &lt;dbl&gt; 3, NA, 22, 4, NA, 6, 11, 2, 14, 4, 8, 7, 8, 8, 13, 4,…\n$ BBQ_POST         &lt;chr&gt; \"76\", NA, \"68\", \"57\", \"missing\", \"14\", \"46\", \"70\", \"3…\n$ SCS_POST         &lt;dbl&gt; 34, NA, 34, 34, NA, 30, 19, 34, 23, 23, 31, 37, 34, 3…\n$ DMRSODF_POST     &lt;dbl&gt; 50776, NA, 42809, 52069, NA, 51758, 49, 50235, 47978,…\n$ `DERS-16_POST`   &lt;dbl&gt; 36, NA, 78, 38, NA, 54, 61, 26, 56, 51, 52, 25, 37, 3…\n$ LSAS_FU6         &lt;dbl&gt; 33, NA, NA, 14, 6, 60, 64, 49, NA, 45, 85, 74, NA, 58…\n$ GAD_FU6          &lt;dbl&gt; 0, NA, NA, 0, NA, 14, 6, 4, NA, 5, 16, NA, NA, 11, 8,…\n$ PHQ9_FU6         &lt;dbl&gt; 3, NA, NA, 2, NA, 6, 8, 3, NA, 6, 22, NA, NA, 6, 5, 1…\n$ BBQ_FU6          &lt;dbl&gt; 77, NA, NA, 68, NA, 9, 56, 64, NA, 48, 36, NA, NA, 39…\n$ SCS_FU6          &lt;chr&gt; \"28\", NA, NA, \"41\", \"missing\", \"24\", \"29\", \"33\", \"mis…\n$ DERS_FU6         &lt;dbl&gt; 35, NA, NA, 36, NA, 72, 61, 26, NA, 37, 67, NA, NA, 3…\n$ LSAS_FU12        &lt;dbl&gt; 27, NA, NA, 16, 39, 75, 66, 43, NA, 51, 79, NA, NA, 5…\n$ GAD_FU12         &lt;dbl&gt; 5, NA, NA, 0, 11, 7, 6, 4, NA, 4, 14, NA, NA, 8, 19, …\n$ `PHQ-9_FU12`     &lt;dbl&gt; 5, NA, NA, 2, 19, 9, 14, 3, NA, 5, 15, NA, NA, 8, 18,…\n$ BBQ_FU12         &lt;dbl&gt; 76, NA, NA, 62, 12, 22, 52, 54, NA, 47, 25, NA, NA, 3…\n$ SCS_FU12         &lt;dbl&gt; 38, NA, NA, 40, 28, 33, 26, 34, NA, 21, 30, NA, NA, 3…\n$ DERS16_FU12      &lt;dbl&gt; 35, NA, NA, 32, 46, 65, 42, 27, NA, 53, 52, NA, NA, 3…\n\n\nOther options for checking the basic structure of the data include head(), names(), ncol(), and nrow().\n\nhead(df_rawdata) shows the first few rows of the dataset.\nnames(df_rawdata) shows the names of the columns.\nncol(df_rawdata) shows the number of columns in the dataset.\nnrow(df_rawdata) shows the number of rows in the dataset.\n\n\n\n\n\n\n\nExercise 2: Check the structure of STePS\n\n\n\n\n\nuse the glimpse() function to check the structure of the STePS data. What do you notice about the data? Are there any potential issues with the data structure?\nAlso try calling view() to open the data in a spreadsheet-like view.",
    "crumbs": [
      "Labs",
      "Import and clean data"
    ]
  },
  {
    "objectID": "labs/import-clean.html#types-of-data",
    "href": "labs/import-clean.html#types-of-data",
    "title": "Import and clean data",
    "section": "Types of data",
    "text": "Types of data\nIt’s good practice to check and verify the type of data in each column after importing the data. For example, if you have a variable that should be numeric, but is instead character, you need to fix this before you can do any analyses. You will learn more about manipulating data types in the Tidy data manipulation chapter.\nThe most common types of columns you will work with are:\n\nchr: character strings\nint: integers\ndbl: numeric values\nlgl: logical (TRUE/FALSE)\nfct: factors\ndate: dates\n\nOur dataset is fairly clean, but there are problems with some variables. You can see from the glimpse() call above that some variables for LSAS and other questionnaires are coded as &lt;chr&gt; even though they should only have numbers. We will address these issues below.",
    "crumbs": [
      "Labs",
      "Import and clean data"
    ]
  },
  {
    "objectID": "labs/import-clean.html#clean-column-names",
    "href": "labs/import-clean.html#clean-column-names",
    "title": "Import and clean data",
    "section": "Clean column names",
    "text": "Clean column names\nWe will use the clean_names() function from the janitor package to clean the column names. This function replaces spaces and other problematic characters with underscores, and converts all names to lowercase. This makes it easier to work with the data later on.\nWhat’s wrong with these column names? And what is the difference after clean_names()?\n\n\n\n\n\n\nExercise 3: Clean column names, part 1\n\n\n\n\n\nUse the clean_names() function to clean the column names in df_data. Check the names before and after cleaning. What changes do you notice? Are there any column names that are still problematic?\n\n\n\n\nnames(df_data)\n\n [1] \"ID\"             \"Group\"          \"LSAS Screening\" \"GAD_screen\"    \n [5] \"PHQ-9 screen\"   \"BBQ_screen\"     \"SCS_screen\"     \"DMRSODF_screen\"\n [9] \"DERS-16_screen\" \"PID-5_screen\"   \"LSAS_V1\"        \"LSAS_V2\"       \n[13] \"LSAS_V3\"        \"LSAS_V4\"        \"LSAS_V5\"        \"LSAS_V6\"       \n[17] \"LSAS_V7\"        \"LSAS_V8\"        \"LSAS_POST\"      \"GAD_POST\"      \n[21] \"PHQ-9_POST\"     \"BBQ_POST\"       \"SCS_POST\"       \"DMRSODF_POST\"  \n[25] \"DERS-16_POST\"   \"LSAS_FU6\"       \"GAD_FU6\"        \"PHQ9_FU6\"      \n[29] \"BBQ_FU6\"        \"SCS_FU6\"        \"DERS_FU6\"       \"LSAS_FU12\"     \n[33] \"GAD_FU12\"       \"PHQ-9_FU12\"     \"BBQ_FU12\"       \"SCS_FU12\"      \n[37] \"DERS16_FU12\"   \n\n\n\ndf_data &lt;- df_data |&gt;\n  clean_names()\n\nWe have sorted some of the problems, but we still have inconsistent names for time-points (screening and screen), and some questionnaires seems to have gotten inconsistent names as well (ders_16, ders16, and ders).\n\ndf_data &lt;- df_data |&gt;\n  rename_with(~ .x |&gt;\n    str_replace_all(\"screening\", \"screen\") |&gt;\n    str_replace_all(\"ders_16|ders16\", \"ders\") |&gt;\n    str_replace_all(\"phq_9\", \"phq9\"))\n\n\n\n\n\n\n\nTipThis is rarely a linear process\n\n\n\nThis type of data cleaning is often a result of going back and forth between running code and checking the results. Sometimes you will come all the way to the analyses before finding that DERS-16 is missing a time-point! This is why it’s helpful to design your project in a way that allows you to easily go back and forth between steps.\n\n\n\n\n\n\n\n\nExercise 4: Clean column names, part 2\n\n\n\n\n\nUse the rename_with() function to fix the column names of PHQ-9 in df_data. Check the names before and after cleaning. What changes do you notice? Are there any column names that are still problematic?",
    "crumbs": [
      "Labs",
      "Import and clean data"
    ]
  },
  {
    "objectID": "labs/import-clean.html#ensure-missing-values-are-coded-as-na",
    "href": "labs/import-clean.html#ensure-missing-values-are-coded-as-na",
    "title": "Import and clean data",
    "section": "Ensure missing values are coded as NA",
    "text": "Ensure missing values are coded as NA\nIf we are lucky, all the missing values are already coded as NA and R will recognize them as missing. More often, however, is that the missing values are coded as \"\", -99, or other values. This can cause problems later, so we need to ensure that all missing values are coded as NA.\nSometimes, weird labels for missing values is also the reason why some columns are not recognized as numeric.\nLet’s check the data structure again. It looks like some columns that should be numeric are instead &lt;chr&gt;, which means they are character strings.\n\nglimpse(df_data)\n\nRows: 181\nColumns: 37\n$ id             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ group          &lt;dbl&gt; 2, 1, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 2, 1, 1, 2, 0, 1…\n$ lsas_screen    &lt;dbl&gt; 63, 71, 98, 63, 74, 81, 67, 76, 88, 73, 86, 78, 97, 72,…\n$ gad_screen     &lt;dbl&gt; 7, 17, 18, 8, 14, 11, 5, 8, 14, 5, 15, 16, 17, 13, 10, …\n$ phq9_screen    &lt;dbl&gt; 6, 13, 19, 4, 18, 8, 9, 8, 14, 3, 5, 11, 12, 18, 10, 4,…\n$ bbq_screen     &lt;dbl&gt; 60, 66, 4, 50, 22, 23, 47, 52, 31, 46, 67, 24, 57, 40, …\n$ scs_screen     &lt;dbl&gt; 25, 16, 22, 35, 29, 30, 20, 34, 21, 26, 35, 21, 32, 33,…\n$ dmrsodf_screen &lt;dbl&gt; 49178, 50727, 45074, 5381, 48444, 50899, 46923, 41428, …\n$ ders_screen    &lt;dbl&gt; 44, 73, 65, 45, 46, 49, 57, 38, 67, 45, 55, 56, 71, 36,…\n$ pid_5_screen   &lt;dbl&gt; 25, 20, 48, 17, 24, 20, 24, 26, 39, 23, 24, 26, 23, 15,…\n$ lsas_v1        &lt;chr&gt; \"72\", \"missing\", \"81\", \"44\", \"39\", \"65\", \"68\", \"69\", NA…\n$ lsas_v2        &lt;chr&gt; \"64\", \"missing\", \"89\", \"33\", \"115\", \"64\", NA, \"70\", \"97…\n$ lsas_v3        &lt;chr&gt; \"72\", \"missing\", \"73\", \"36\", \"missing\", \"63\", NA, \"71\",…\n$ lsas_v4        &lt;chr&gt; \"61\", \"missing\", \"94\", \"44\", \"missing\", \"60\", NA, \"51\",…\n$ lsas_v5        &lt;chr&gt; \"61\", NA, \"93\", \"21\", \"missing\", \"55\", NA, \"55\", NA, \"6…\n$ lsas_v6        &lt;chr&gt; \"46\", NA, \"88\", \"20\", \"missing\", \"46\", NA, \"56\", \"93\", …\n$ lsas_v7        &lt;dbl&gt; 55, NA, NA, 18, NA, 45, NA, 64, NA, 54, 101, NA, 89, 43…\n$ lsas_v8        &lt;dbl&gt; 49, NA, NA, 17, NA, 44, NA, 52, NA, 61, 84, NA, 89, NA,…\n$ lsas_post      &lt;dbl&gt; 50, NA, 77, 22, NA, 52, 75, 45, 79, 64, 80, 89, 83, 45,…\n$ gad_post       &lt;dbl&gt; 4, NA, 19, 6, NA, 9, 4, 3, 10, 7, 15, 7, 11, 11, 11, 1,…\n$ phq9_post      &lt;dbl&gt; 3, NA, 22, 4, NA, 6, 11, 2, 14, 4, 8, 7, 8, 8, 13, 4, 1…\n$ bbq_post       &lt;chr&gt; \"76\", NA, \"68\", \"57\", \"missing\", \"14\", \"46\", \"70\", \"36\"…\n$ scs_post       &lt;dbl&gt; 34, NA, 34, 34, NA, 30, 19, 34, 23, 23, 31, 37, 34, 34,…\n$ dmrsodf_post   &lt;dbl&gt; 50776, NA, 42809, 52069, NA, 51758, 49, 50235, 47978, 4…\n$ ders_post      &lt;dbl&gt; 36, NA, 78, 38, NA, 54, 61, 26, 56, 51, 52, 25, 37, 33,…\n$ lsas_fu6       &lt;dbl&gt; 33, NA, NA, 14, 6, 60, 64, 49, NA, 45, 85, 74, NA, 58, …\n$ gad_fu6        &lt;dbl&gt; 0, NA, NA, 0, NA, 14, 6, 4, NA, 5, 16, NA, NA, 11, 8, 8…\n$ phq9_fu6       &lt;dbl&gt; 3, NA, NA, 2, NA, 6, 8, 3, NA, 6, 22, NA, NA, 6, 5, 12,…\n$ bbq_fu6        &lt;dbl&gt; 77, NA, NA, 68, NA, 9, 56, 64, NA, 48, 36, NA, NA, 39, …\n$ scs_fu6        &lt;chr&gt; \"28\", NA, NA, \"41\", \"missing\", \"24\", \"29\", \"33\", \"missi…\n$ ders_fu6       &lt;dbl&gt; 35, NA, NA, 36, NA, 72, 61, 26, NA, 37, 67, NA, NA, 36,…\n$ lsas_fu12      &lt;dbl&gt; 27, NA, NA, 16, 39, 75, 66, 43, NA, 51, 79, NA, NA, 55,…\n$ gad_fu12       &lt;dbl&gt; 5, NA, NA, 0, 11, 7, 6, 4, NA, 4, 14, NA, NA, 8, 19, 12…\n$ phq9_fu12      &lt;dbl&gt; 5, NA, NA, 2, 19, 9, 14, 3, NA, 5, 15, NA, NA, 8, 18, 3…\n$ bbq_fu12       &lt;dbl&gt; 76, NA, NA, 62, 12, 22, 52, 54, NA, 47, 25, NA, NA, 33,…\n$ scs_fu12       &lt;dbl&gt; 38, NA, NA, 40, 28, 33, 26, 34, NA, 21, 30, NA, NA, 32,…\n$ ders_fu12      &lt;dbl&gt; 35, NA, NA, 32, 46, 65, 42, 27, NA, 53, 52, NA, NA, 36,…\n\n\nLet’s fix this by replacing the problematic values with NA. We will use the mutate() and across() functions from the dplyr package to apply the na_if() function to all character columns. This will replace any occurrence of “missing” with NA.\n\ndf_data &lt;- df_data |&gt;\n  mutate(across(where(is.character), ~ na_if(., \"missing\")))\n\n\n\n\n\n\n\nExercise 5: Fix missing values\n\n\n\n\n\nCheck which columns have weird values for missing values. You can use the glimpse() function or view the data in the viewer. You can also use the unique() function to check which values are present in a column.\nThen, use the mutate() and across() functions to replace these values with NA. Make sure to replace all occurrences of “missing” in the character columns.",
    "crumbs": [
      "Labs",
      "Import and clean data"
    ]
  },
  {
    "objectID": "labs/import-clean.html#fix-column-types",
    "href": "labs/import-clean.html#fix-column-types",
    "title": "Import and clean data",
    "section": "Fix column types",
    "text": "Fix column types\nThe NA values are now correctly coded, but we still have some columns that are not numeric despite only including numbers. We can use the mutate() function again to convert the columns to the correct data type. In this step, we also ensure that the id and group columns are factors.\n\n# which columns should be numeric?\nnum_cols &lt;- c(\"lsas\", \"gad\", \"phq9\", \"bbq\", \"scs\", \"dmrsodf\", \"ders\", \"pid_5\")\n\ndf_data &lt;- df_data |&gt;\n  mutate(\n    across(starts_with(num_cols), as.numeric),\n    id = factor(id),\n    group = factor(group)\n  )\n\n\n\n\n\n\n\nExercise 6: Fix column types\n\n\n\n\n\nLet’s imagine we have many columns that should be factors. How can you use the same procedure as above to convert all columns from a fct_cols vector to factors?",
    "crumbs": [
      "Labs",
      "Import and clean data"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html",
    "href": "labs/descriptive-statistics.html",
    "title": "Descriptive statistics",
    "section": "",
    "text": "In this chapter we show how to calculate and interpret descriptive statistics using the STePS dataset. We start by loading the cleaned data set saved in the Import and clean data chapter.\nlibrary(here)\nlibrary(tidyverse)\n\ndf_data &lt;- read_csv(here(\"data\", \"steps_clean.csv\"))\nWe’ll start of doing some basic descriptive statistics of the baseline variables. We start by checking the variable names to identify the baseline variables.\ncolnames(df_data)\n\n [1] \"id\"             \"group\"          \"lsas_screen\"    \"gad_screen\"    \n [5] \"phq9_screen\"    \"bbq_screen\"     \"scs_screen\"     \"dmrsodf_screen\"\n [9] \"ders_screen\"    \"pid_5_screen\"   \"lsas_v1\"        \"lsas_v2\"       \n[13] \"lsas_v3\"        \"lsas_v4\"        \"lsas_v5\"        \"lsas_v6\"       \n[17] \"lsas_v7\"        \"lsas_v8\"        \"lsas_post\"      \"gad_post\"      \n[21] \"phq9_post\"      \"bbq_post\"       \"scs_post\"       \"dmrsodf_post\"  \n[25] \"ders_post\"      \"lsas_fu6\"       \"gad_fu6\"        \"phq9_fu6\"      \n[29] \"bbq_fu6\"        \"scs_fu6\"        \"ders_fu6\"       \"lsas_fu12\"     \n[33] \"gad_fu12\"       \"phq9_fu12\"      \"bbq_fu12\"       \"scs_fu12\"      \n[37] \"ders_fu12\"      \"trt\"\nFor easy handling, we’ll create a data frame containing only the baseline variables, ID, Group and all variables ending with the suffix “_screen”.\nd_bl &lt;- df_data |&gt;\n  select(\n    id,\n    group,\n    lsas_screen,\n    gad_screen,\n    phq9_screen,\n    bbq_screen,\n    scs_screen,\n    dmrsodf_screen,\n    ders_screen,\n    pid_5_screen\n  )\nOr more efficiently (if variables are correctly named) we can use the ends_with() function from the tidyselect package:\nd_bl &lt;- df_data |&gt;\n  select(\n    id,\n    group,\n    ends_with(\"_screen\")\n  )",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html#visual-presentations",
    "href": "labs/descriptive-statistics.html#visual-presentations",
    "title": "Descriptive statistics",
    "section": "Visual presentations",
    "text": "Visual presentations\nThe simplest, and often most informative way, to get an overview of a variable is to produce a visual representation of its distribution. For uni-variable numeric variables, two common visualizations are histograms hist() and boxplots boxplot().\n\nHistogram\nA histogram is a graphical representation used to visualize the distribution of a numeric variable. It shows how data are spread across intervals (bins) and helps identify patterns such as central tendency, variability, skewness, and outliers.\n\n\n\nFeature\nWhat It Tells You\n\n\n\n\nHeight of bars\nNumber of observations in each bin\n\n\nWidth of bars\nRange of values grouped together\n\n\nShape\nSymmetry, skewness, modality\n\n\nOutliers\nBars isolated from the main group\n\n\n\n\nhist(d_bl$lsas_screen)\n\n\n\n\n\n\n\n\n\nBoxplot\nA boxplot (also called a box-and-whisker plot) is a compact, visual summary of the distribution, central tendency, and variability of a dataset.\n\n\n\nComponent\nDescription\n\n\n\n\nMinimum\nSmallest value (excluding outliers)\n\n\nQ1 (1st Quartile)\n25th percentile (lower hinge of the box)\n\n\nMedian (Q2)\n50th percentile (line inside the box)\n\n\nQ3 (3rd Quartile)\n75th percentile (upper hinge of the box)\n\n\nMaximum\nLargest value (excluding outliers)\n\n\n\n\nboxplot(d_bl$lsas_screen)",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html#centrality-measures",
    "href": "labs/descriptive-statistics.html#centrality-measures",
    "title": "Descriptive statistics",
    "section": "Centrality measures",
    "text": "Centrality measures\nSometimes we need more comprehensive summaries of our data. For this purpose, it is common to use centrality measures. Centrality measures are statistical summaries that describe the center or typical value of a dataset. They help summarize where most values lie and include:\n\nMean: The average of all values (sum of all values divided by the number of values).\nMedian: The middle value when data is ordered (or the average of the two middle numbers if the length of the vector is an even number).\nMode: The most frequently occurring value.\n\nThese measures provide insight into the distribution’s central tendency, helping you understand the “typical” case in your data.\n\n\n\n\n\n\nNoteMeans, medians and outliers\n\n\n\nThe mean is sensitive to outliers, while the median is not. The values [1, 2, 3, 100] has a mean of 26.5. and a median of 2.5. None of them are “wrong”, but the usefulness of each measure depends on what you want your centrality measures to tell. However, with highly skewed variables (e.g. income), the median is usually viewed as more informative.\n\n\nNow let’s use R to calculate some centrality measures for LSAS at baseline.\n\nGetting the mean using mean()\n\nmean(d_bl$lsas_screen)\n\n[1] 84.75138\n\n\n\n\nGetting the median using median()\n\nmedian(d_bl$lsas_screen)\n\n[1] 82\n\n\n\n\nGetting the mode\nUnlike mean() and median(), base R does not include a built-in mode() function for computing the statistical mode (i.e. the most frequent value). However, you can create one.\n\nget_mode &lt;- function(x) {\n  ux &lt;- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\nget_mode(d_bl$lsas_screen)\n\n[1] 74\n\n\nMuch of this information could also be easily found using the summary() function.\n\nsummary(d_bl$lsas_screen)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  60.00   72.00   82.00   84.75   93.00  134.00 \n\n\n\n\nVisualizing centrality measures\nYou could also plot the mean, median and mode over the histogram, to get a better sense of the data.\n\nhist(d_bl$lsas_screen,\n  main = \"Histogram with Mean, Median, and Mode\",\n  xlab = \"Values\", probability = TRUE\n)\n\n# Add lines for mean, median, and mode\nabline(v = mean(d_bl$lsas_screen), col = \"blue\", lwd = 2, lty = 2) # Mean\nabline(v = median(d_bl$lsas_screen), col = \"red\", lwd = 2, lty = 2) # Median\nabline(v = get_mode(d_bl$lsas_screen), col = \"darkgreen\", lwd = 2, lty = 2) # Mode",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html#spread-measures",
    "href": "labs/descriptive-statistics.html#spread-measures",
    "title": "Descriptive statistics",
    "section": "Spread measures",
    "text": "Spread measures\nCentrality measures give information on the most typical value in the distribution, but no info on the spread of values. For example, the two distributions below have the same mean value (0), but different spread (standard deviation 0.2 vs. 1).\n\n\n\n\n\n\n\n\n\nTo get a summary of the spread of the data, we use different spread measures. Spread measures describe how much the data varies or is dispersed around a central value like the mean or median. They help you understand whether the values are tightly clustered or widely scattered. Commonly used spread measures include:\n\nRange range(): The difference between the maximum and minimum values. Simple but sensitive to outliers.\n\\[\\text{Range} = \\max(X) - \\min(X)\\]\nInterquartile Range (IQR) IQR(): The range of the middle 50% of data (Q3 − Q1). More robust against extreme values.\n\\[\\mathrm{IQR} = Q_3 - Q_1\\]\nVariance var(): The average of the squared deviations from the mean. It gives more weight to larger deviations.\nPopulation variance\n\\[\n\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\n\\]\nSample variance\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\nStandard Deviation (SD) sd(): The square root of variance. It measures average distance from the mean and is widely used in statistics.\nPopulation SD\n\\[\n\\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2}\n\\]\nSample SD\n\\[ s = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]\n\n\n\n\n\n\nNotePopulation vs samples\n\n\n\nThe population variance (\\(\\sigma^2\\)) and standard deviation (\\(\\sigma\\)) are used when you have data for the entire population — that is, every single value of interest is included.\nThe sample variance (\\(s^2\\)) and standard deviation (\\(s\\)) are used when you’re working with a subset (sample) of a larger population. It includes a correction (called Bessel’s correction) to account for the fact that samples tend to underestimate variability. The sample SD divides by \\(n -1\\) to compensate for the fact that we use \\(\\bar{x}\\), an estimate of the true population mean (\\(\\mu\\)). This correction makes the sample variance an unbiased estimator of the population variance.\nThe functions sd() and var() gives the sample standard deviation and variance.",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html#visual-presentations-1",
    "href": "labs/descriptive-statistics.html#visual-presentations-1",
    "title": "Descriptive statistics",
    "section": "Visual presentations",
    "text": "Visual presentations\nAs for numeric variables, visualizations can help get a better sense for the distribution of the data. Two common ways are barcharts and piecharts\n\nBarcharts\nThe height of each bar gives the number of occurrences of each category. When you use the base function plot() on a factor level variable, it gives you a barchart.\n\nplot(as.factor(d_bl$education))\n\n\n\n\n\n\n\n\n\n\nPiecharts\nA piechart shows the distribution of a categorical variable as a pie, with the size of each piece representing the proportion of each level of the categorical variable.\n\npie(table(d_bl$education))",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html#numeric-by-numeric-distributions",
    "href": "labs/descriptive-statistics.html#numeric-by-numeric-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric by numeric distributions",
    "text": "Numeric by numeric distributions\nFor two numeric variables, the most common visualization is the scatter plot. It shows the distribution of each datapoint with one variable on the x-axis and the other on the y-axis. Using the plot() function with two numeric variables will give you a scatter plot.\n\nplot(d_bl$lsas_screen, d_bl$gad_screen)",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html#numeric-by-categorical-distributions",
    "href": "labs/descriptive-statistics.html#numeric-by-categorical-distributions",
    "title": "Descriptive statistics",
    "section": "Numeric by categorical distributions",
    "text": "Numeric by categorical distributions\nThe joint distribution of a numeric and a categorical variable can be visualized as a stratified boxplots. For this let’s look at the distribution of LSAS scores for people with high generalized anxiety (GAD-7 ≥ 10 or more) vs low generalized anxiety (GAD-7 &lt; 10).\n\n# create a variable indicating if GAD-7 is 10 or more\nd_bl$gad_cat &lt;- ifelse(d_bl$gad_screen &gt;= 10, \"High anxiety\", \"Low anxiety\")\n\nboxplot(lsas_screen ~ gad_cat, data = d_bl)",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html#categorical-by-categorical-distributions",
    "href": "labs/descriptive-statistics.html#categorical-by-categorical-distributions",
    "title": "Descriptive statistics",
    "section": "Categorical by categorical distributions",
    "text": "Categorical by categorical distributions\nAll the information of the joint distribution of two categorical variables can be seen using a cross-table. For this let’s look at the distribution of high vs low depression (PHQ-9 ≥ 10 vs PHQ-9 &lt;10 or less) against high vs low generalized anxiety.\n\nd_bl$phq_cat &lt;- ifelse(d_bl$phq9_screen &gt;= 10, \"High depression\", \"Low depression\")\n\ntable(d_bl$gad_cat, d_bl$phq_cat)\n\n              \n               High depression Low depression\n  High anxiety              62             42\n  Low anxiety               20             57\n\n\nAlthough all information about the distribution is available in the crosstable, you may still want to visualize this distribution. One way is to use a mosaic plot, which you can get by providing cross-table to the plot() function.\n\nplot(table(d_bl$gad_cat, d_bl$phq_cat), main = \"Depression and anxiety\")",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/descriptive-statistics.html#descriptive-statistics-using-the-tableone-package",
    "href": "labs/descriptive-statistics.html#descriptive-statistics-using-the-tableone-package",
    "title": "Descriptive statistics",
    "section": "Descriptive statistics using the ‘tableone’ package",
    "text": "Descriptive statistics using the ‘tableone’ package\nA convenient way to get descriptive statistics for a range of variables is to use the tableone package and the CreateTableOne() function. First let get some descriptives for the overall sample\n\n# install.packages(\"tableone\")\nlibrary(tableone)\n\n# define the variables you want\nvars &lt;- c(\n  \"lsas_screen\",\n  \"gad_screen\",\n  \"phq9_screen\",\n  \"bbq_screen\",\n  \"scs_screen\",\n  \"dmrsodf_screen\",\n  \"ders_screen\",\n  \"pid_5_screen\",\n  \"gender\",\n  \"education\",\n  \"income\")\n\nCreateTableOne(vars = vars, data = d_bl)\n\n                            \n                             Overall                                 \n  n                                         181                      \n  lsas_screen (mean (SD))                 84.75 (16.47)              \n  gad_screen (mean (SD))                  10.86 (4.51)               \n  phq9_screen (mean (SD))                  9.61 (4.37)               \n  bbq_screen (mean (SD))                  39.64 (16.48)              \n  scs_screen (mean (SD))                  27.66 (7.03)               \n  dmrsodf_screen (mean (SD)) 267220994518069.88 (3595090797150460.50)\n  ders_screen (mean (SD))                 49.20 (13.43)              \n  pid_5_screen (mean (SD))                23.93 (8.51)               \n  gender = Woman (%)                        122 (67.4)               \n  education (%)                                                      \n     Primary                                 68 (37.6)               \n     Secondary                               69 (38.1)               \n     University                              44 (24.3)               \n  income (%)                                                         \n     High                                    41 (22.7)               \n     Low                                     39 (21.5)               \n     Medium                                 101 (55.8)               \n\n\nWe can also do this stratified by a categorical variable using the strata argument. Let’s have it by treatment group.\n\nCreateTableOne(vars = vars, data = d_bl, strata = \"group\", test = FALSE)\n\n                            Stratified by group\n                             0                                       \n  n                                          60                      \n  lsas_screen (mean (SD))                 86.98 (18.99)              \n  gad_screen (mean (SD))                  11.18 (4.50)               \n  phq9_screen (mean (SD))                 10.35 (4.63)               \n  bbq_screen (mean (SD))                  37.38 (17.10)              \n  scs_screen (mean (SD))                  27.37 (7.04)               \n  dmrsodf_screen (mean (SD)) 806116666706155.25 (6244152850195287.00)\n  ders_screen (mean (SD))                 50.70 (13.72)              \n  pid_5_screen (mean (SD))                25.07 (9.46)               \n  gender = Woman (%)                         39 (65.0)               \n  education (%)                                                      \n     Primary                                 22 (36.7)               \n     Secondary                               27 (45.0)               \n     University                              11 (18.3)               \n  income (%)                                                         \n     High                                    14 (23.3)               \n     Low                                      7 (11.7)               \n     Medium                                  39 (65.0)               \n                            Stratified by group\n                             1                   2                  \n  n                                61                  60           \n  lsas_screen (mean (SD))       83.08 (12.97)       84.22 (16.95)   \n  gad_screen (mean (SD))        10.97 (4.44)        10.42 (4.63)    \n  phq9_screen (mean (SD))        9.15 (4.05)         9.35 (4.40)    \n  bbq_screen (mean (SD))        42.03 (17.09)       39.45 (15.10)   \n  scs_screen (mean (SD))        27.15 (7.31)        28.48 (6.76)    \n  dmrsodf_screen (mean (SD)) 44885.95 (16035.54) 44388.20 (14850.76)\n  ders_screen (mean (SD))       46.74 (13.60)       50.22 (12.83)   \n  pid_5_screen (mean (SD))      22.34 (7.80)        24.40 (8.08)    \n  gender = Woman (%)               43 (70.5)           40 (66.7)    \n  education (%)                                                     \n     Primary                       24 (39.3)           22 (36.7)    \n     Secondary                     19 (31.1)           23 (38.3)    \n     University                    18 (29.5)           15 (25.0)    \n  income (%)                                                        \n     High                          13 (21.3)           14 (23.3)    \n     Low                           19 (31.1)           13 (21.7)    \n     Medium                        29 (47.5)           33 (55.0)",
    "crumbs": [
      "Labs",
      "Descriptive statistics"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html",
    "href": "labs/tidy-data-manipulation.html",
    "title": "Tidy Data Manipulation",
    "section": "",
    "text": "In this lab, we will learn how to manipulate and reshape data using the tidyverse suite of packages. Data manipulation is a crucial skill in biostatistics because raw data is rarely in the format we need for analysis or visualization.\nFor this lab, we will continue working with the STePS dataset that we cleaned in Import and clean data and explored in Descriptive statistics. By the end of this lab, you will be able to:",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#selecting-columns-with-select",
    "href": "labs/tidy-data-manipulation.html#selecting-columns-with-select",
    "title": "Tidy Data Manipulation",
    "section": "Selecting columns with select()",
    "text": "Selecting columns with select()\nThe select() function allows us to choose which columns (variables) we want to keep in our dataset. This is useful when working with large datasets where you only need a few variables.\n\n# Select just ID, group, and baseline LSAS\ndf_basic &lt;- df_data |&gt;\n  select(id, group, lsas_screen)\n\nhead(df_basic)\n\n# A tibble: 6 × 3\n     id group lsas_screen\n  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1     1     2          63\n2     2     1          71\n3     3     0          98\n4     4     2          63\n5     5     2          74\n6     6     2          81\n\n\nWe can also use helper functions to select multiple columns at once:\n\n# Select all columns that start with \"lsas\"\ndf_lsas_all &lt;- df_data |&gt;\n  select(id, group, starts_with(\"lsas\"))\n\n# Check how many columns we have\nncol(df_lsas_all)\n\n[1] 14\n\nnames(df_lsas_all)\n\n [1] \"id\"          \"group\"       \"lsas_screen\" \"lsas_v1\"     \"lsas_v2\"    \n [6] \"lsas_v3\"     \"lsas_v4\"     \"lsas_v5\"     \"lsas_v6\"     \"lsas_v7\"    \n[11] \"lsas_v8\"     \"lsas_post\"   \"lsas_fu6\"    \"lsas_fu12\"  \n\n\n\n\n\n\n\n\nExercise 1: Practice selecting columns\n\n\n\n\n\nSelect only the ID, group, and all PHQ-9 variables (columns starting with “phq9”) from the dataset. How many columns does your new dataset have?",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#filtering-rows-with-filter",
    "href": "labs/tidy-data-manipulation.html#filtering-rows-with-filter",
    "title": "Tidy Data Manipulation",
    "section": "Filtering rows with filter()",
    "text": "Filtering rows with filter()\nThe filter() function allows us to select specific rows based on conditions. This is useful for creating subgroups or excluding certain participants.\n\n# Keep only participants in the guided treatment group (group == 2)\ndf_guided &lt;- df_data |&gt;\n  filter(group == 2)\n\nnrow(df_guided)\n\n[1] 60\n\n\nWe can use multiple conditions:\n\n# Keep participants in guided treatment with baseline LSAS &gt;= 60\ndf_guided_severe &lt;- df_data |&gt;\n  filter(group == 2, lsas_screen &gt;= 60)\n\nnrow(df_guided_severe)\n\n[1] 60\n\n\n\n\n\n\n\n\nTipCommon filter conditions\n\n\n\n\n== : equal to\n!= : not equal to\n\n&gt;, &gt;= : greater than (or equal)\n&lt;, &lt;= : less than (or equal)\n%in% : is in a list of values\nis.na() : is missing\n!is.na() : is not missing\n\n\n\n\n\n\n\n\n\nExercise 2: Practice filtering data\n\n\n\n\n\n\nFilter the data to include only participants with baseline LSAS scores between 50 and 80.\nFilter to include only participants in groups 1 or 2 (exclude waitlist).\nHow many participants meet both criteria?",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#arranging-rows-with-arrange",
    "href": "labs/tidy-data-manipulation.html#arranging-rows-with-arrange",
    "title": "Tidy Data Manipulation",
    "section": "Arranging rows with arrange()",
    "text": "Arranging rows with arrange()\nThe arrange() function sorts your data by one or more variables. This can be helpful for identifying extreme values or organizing data for presentation.\n\n# Sort by baseline LSAS score (lowest to highest)\ndf_sorted &lt;- df_data |&gt;\n  select(id, group, lsas_screen) |&gt;\n  arrange(lsas_screen)\n\nhead(df_sorted)\n\n# A tibble: 6 × 3\n     id group lsas_screen\n  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1    65     2          60\n2   124     1          60\n3   140     0          60\n4   146     0          60\n5   157     0          60\n6    58     0          61\n\n\nUse desc() for descending order:\n\n# Sort by baseline LSAS score (highest to lowest)\ndf_sorted_desc &lt;- df_data |&gt;\n  select(id, group, lsas_screen) |&gt;\n  arrange(desc(lsas_screen))\n\nhead(df_sorted_desc)\n\n# A tibble: 6 × 3\n     id group lsas_screen\n  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1   123     0         134\n2    23     0         131\n3    35     0         131\n4    53     0         128\n5   170     2         128\n6    55     2         125",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#basic-calculations",
    "href": "labs/tidy-data-manipulation.html#basic-calculations",
    "title": "Tidy Data Manipulation",
    "section": "Basic calculations",
    "text": "Basic calculations\n\ndf_with_change &lt;- df_data |&gt;\n  select(id, group, lsas_screen, lsas_post) |&gt;\n  mutate(\n    # Calculate change score (post - baseline)\n    lsas_change = lsas_post - lsas_screen,\n    # Calculate percentage change\n    lsas_pct_change = (lsas_change / lsas_screen) * 100\n  )\n\nhead(df_with_change)\n\n# A tibble: 6 × 6\n     id group lsas_screen lsas_post lsas_change lsas_pct_change\n  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1     1     2          63        50         -13           -20.6\n2     2     1          71        NA          NA            NA  \n3     3     0          98        77         -21           -21.4\n4     4     2          63        22         -41           -65.1\n5     5     2          74        NA          NA            NA  \n6     6     2          81        52         -29           -35.8",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#creating-categorical-variables",
    "href": "labs/tidy-data-manipulation.html#creating-categorical-variables",
    "title": "Tidy Data Manipulation",
    "section": "Creating categorical variables",
    "text": "Creating categorical variables\nWe often need to create categorical versions of continuous variables:\n\ndf_with_categories &lt;- df_data |&gt;\n  select(id, group, phq9_screen, gad_screen) |&gt;\n  mutate(\n    # Using case_when() for PHQ9 severity categories\n    phq9_severity = case_when(\n      phq9_screen &lt; 5 ~ \"Minimal\",\n      phq9_screen &lt; 10 ~ \"Mild\",\n      phq9_screen &lt; 15 ~ \"Moderate\",\n      phq9_screen &lt; 20 ~ \"Moderately severe\",\n      phq9_screen &gt;= 20 ~ \"Severe\",\n      is.na(phq9_screen) ~ \"Missing\"\n    ),\n    # Convert to factor\n    phq9_severity = factor(\n      phq9_severity,\n      levels = c(\n        \"Minimal\",\n        \"Mild\",\n        \"Moderate\",\n        \"Moderately severe\",\n        \"Severe\",\n        \"Missing\"\n      ),\n      ordered = TRUE\n    ),\n    # Create binary variable for high anxiety\n    high_anxiety = factor(\n      ifelse(gad_screen &gt;= 10, \"High\", \"Low\"),\n      levels = c(\"Low\", \"High\")\n    ),\n    # Create more readable group labels\n    group_label = factor(group,\n      levels = c(0, 1, 2),\n      labels = c(\"Waitlist\", \"Self-guided\", \"Therapist-guided\")\n    )\n  )\n\n# Check our new variables\ntable(df_with_categories$phq9_severity)\n\n\n          Minimal              Mild          Moderate Moderately severe \n               19                80                52                30 \n           Severe           Missing \n                0                 0 \n\ntable(df_with_categories$group_label)\n\n\n        Waitlist      Self-guided Therapist-guided \n              60               61               60 \n\n\nWe can check how our created PHQ9 categories map to the raw scores:\n\ndf_with_categories |&gt;\n  select(phq9_screen, phq9_severity) |&gt;\n  distinct(phq9_screen, phq9_severity) |&gt;\n  arrange(phq9_screen)\n\n# A tibble: 19 × 2\n   phq9_screen phq9_severity    \n         &lt;dbl&gt; &lt;ord&gt;            \n 1           1 Minimal          \n 2           2 Minimal          \n 3           3 Minimal          \n 4           4 Minimal          \n 5           5 Mild             \n 6           6 Mild             \n 7           7 Mild             \n 8           8 Mild             \n 9           9 Mild             \n10          10 Moderate         \n11          11 Moderate         \n12          12 Moderate         \n13          13 Moderate         \n14          14 Moderate         \n15          15 Moderately severe\n16          16 Moderately severe\n17          17 Moderately severe\n18          18 Moderately severe\n19          19 Moderately severe\n\n\n\n\n\n\n\n\nExercise 3: Create new variables\n\n\n\n\n\n\nCreate a new variable called gad_severity that categorizes GAD-7 scores as:\n\n“Minimal” (0-4)\n“Mild” (5-9)\n“Moderate” (10-14)\n“Severe” (15+)\n\nCreate a binary variable indicating whether someone has both high anxiety (GAD-7 ≥ 10) AND moderate-to-severe depression (PHQ-9 ≥ 10).",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#basic-summarization",
    "href": "labs/tidy-data-manipulation.html#basic-summarization",
    "title": "Tidy Data Manipulation",
    "section": "Basic summarization",
    "text": "Basic summarization\nWe can use the summarize() (from dplyr) function to calculate summary statistics for our data.\n\n# Calculate overall statistics\ndf_data |&gt;\n  summarize(\n    n_participants = n(),\n    mean_lsas = mean(lsas_screen, na.rm = TRUE),\n    sd_lsas = sd(lsas_screen, na.rm = TRUE),\n    median_lsas = median(lsas_screen, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 4\n  n_participants mean_lsas sd_lsas median_lsas\n           &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1            181      84.8    16.5          82",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#grouped-summarization",
    "href": "labs/tidy-data-manipulation.html#grouped-summarization",
    "title": "Tidy Data Manipulation",
    "section": "Grouped summarization",
    "text": "Grouped summarization\nMore often, we want to calculate statistics by groups:\n\n# Calculate statistics by treatment group\ngroup_stats &lt;- df_data |&gt;\n  group_by(group) |&gt;\n  summarize(\n    n_participants = n(),\n    mean_lsas = mean(lsas_screen, na.rm = TRUE),\n    sd_lsas = sd(lsas_screen, na.rm = TRUE),\n    median_lsas = median(lsas_screen, na.rm = TRUE),\n    .groups = \"drop\" # This removes the grouping\n  )\n\ngroup_stats\n\n# A tibble: 3 × 5\n  group n_participants mean_lsas sd_lsas median_lsas\n  &lt;dbl&gt;          &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1     0             60      87.0    19.0          83\n2     1             61      83.1    13.0          82\n3     2             60      84.2    17.0          81\n\n\nWe can also use the .by syntax which is often cleaner:\n\ndf_data |&gt;\n  summarize(\n    n_participants = n(),\n    mean_lsas = mean(lsas_screen, na.rm = TRUE),\n    sd_lsas = sd(lsas_screen, na.rm = TRUE),\n    .by = group\n  )\n\n# A tibble: 3 × 4\n  group n_participants mean_lsas sd_lsas\n  &lt;dbl&gt;          &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     2             60      84.2    17.0\n2     1             61      83.1    13.0\n3     0             60      87.0    19.0\n\n\n\n\n\n\n\n\nExercise 4: Practice summarizing data\n\n\n\n\n\n\nCalculate the mean and standard deviation of GAD-7 scores by treatment group.\nCalculate the number of participants and percentage with high anxiety (GAD-7 ≥ 10) by treatment group.",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#understanding-wide-vs.-long-format",
    "href": "labs/tidy-data-manipulation.html#understanding-wide-vs.-long-format",
    "title": "Tidy Data Manipulation",
    "section": "Understanding wide vs. long format",
    "text": "Understanding wide vs. long format\nOur data is currently in wide format, where each participant has one row, with repeated measures in separate columns.\n\nWide format: Each participant has one row, with repeated measures in separate columns\n\nLong format: Each measurement has its own row, with a column indicating the time point\n\n\n\n\n\n\n\nNoteWhen to use each format\n\n\n\n\nWide format: Calculating change scores, some statistical tests (e.g., SEM), demographic tables\nLong format: Plotting over time, mixed-effects models, most ggplot visualizations",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#converting-to-long-format-with-pivot_longer",
    "href": "labs/tidy-data-manipulation.html#converting-to-long-format-with-pivot_longer",
    "title": "Tidy Data Manipulation",
    "section": "Converting to long format with pivot_longer()",
    "text": "Converting to long format with pivot_longer()\nLet’s focus on the LSAS measurements across time. First, let’s see what LSAS columns we have:\n\n# Check what LSAS columns we have\ndf_data |&gt;\n  select(starts_with(\"lsas\")) |&gt;\n  names()\n\n [1] \"lsas_screen\" \"lsas_v1\"     \"lsas_v2\"     \"lsas_v3\"     \"lsas_v4\"    \n [6] \"lsas_v5\"     \"lsas_v6\"     \"lsas_v7\"     \"lsas_v8\"     \"lsas_post\"  \n[11] \"lsas_fu6\"    \"lsas_fu12\"  \n\n\nAll of the columns that start with “lsas” are LSAS scores at different time points. Now let’s convert to long format:\n\ndf_lsas_long &lt;- df_data |&gt;\n  select(id, group, starts_with(\"lsas\")) |&gt;\n  pivot_longer(\n    cols = starts_with(\"lsas\"), # Which columns to pivot\n    names_to = \"time_point\", # Name for the new column with time info\n    values_to = \"lsas_score\" # Name for the new column with values\n  )\n\nhead(df_lsas_long, 12)\n\n# A tibble: 12 × 4\n      id group time_point  lsas_score\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1     1     2 lsas_screen         63\n 2     1     2 lsas_v1             72\n 3     1     2 lsas_v2             64\n 4     1     2 lsas_v3             72\n 5     1     2 lsas_v4             61\n 6     1     2 lsas_v5             61\n 7     1     2 lsas_v6             46\n 8     1     2 lsas_v7             55\n 9     1     2 lsas_v8             49\n10     1     2 lsas_post           50\n11     1     2 lsas_fu6            33\n12     1     2 lsas_fu12           27",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#cleaning-the-time-variable",
    "href": "labs/tidy-data-manipulation.html#cleaning-the-time-variable",
    "title": "Tidy Data Manipulation",
    "section": "Cleaning the time variable",
    "text": "Cleaning the time variable\nThe time variable is not very readable. Let’s clean it up and create properly ordered factors:\n\ndf_lsas_long &lt;- df_lsas_long |&gt;\n  # Split the time_point column to separate \"lsas\" from the actual time\n  separate(time_point, into = c(\"measure\", \"time\"), sep = \"_\") |&gt;\n  # Create a cleaner time variable\n  mutate(\n    time_clean = case_when(\n      time == \"screen\" ~ \"Baseline\",\n      time == \"v1\" ~ \"Week 1\",\n      time == \"v2\" ~ \"Week 2\",\n      time == \"v3\" ~ \"Week 3\",\n      time == \"v4\" ~ \"Week 4\",\n      time == \"v5\" ~ \"Week 5\",\n      time == \"v6\" ~ \"Week 6\",\n      time == \"v7\" ~ \"Week 7\",\n      time == \"v8\" ~ \"Week 8\",\n      time == \"post\" ~ \"Post-treatment\",\n      time == \"fu6\" ~ \"6-month follow-up\",\n      time == \"fu12\" ~ \"12-month follow-up\"\n    ),\n    # Also create a numeric week variable for plotting\n    week_num = case_when(\n      time == \"screen\" ~ 0,\n      time == \"v1\" ~ 1,\n      time == \"v2\" ~ 2,\n      time == \"v3\" ~ 3,\n      time == \"v4\" ~ 4,\n      time == \"v5\" ~ 5,\n      time == \"v6\" ~ 6,\n      time == \"v7\" ~ 7,\n      time == \"v8\" ~ 8,\n      time == \"post\" ~ 9,\n      time == \"fu6\" ~ 33, # 9 + 6*4 weeks\n      time == \"fu12\" ~ 57 # 9 + 12*4 weeks\n    ),\n  ) |&gt;\n  # Remove the \"measure\" column since it's just \"lsas\" for all rows\n  select(-measure)\n\nhead(df_lsas_long)\n\n# A tibble: 6 × 6\n     id group time   lsas_score time_clean week_num\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     1     2 screen         63 Baseline          0\n2     1     2 v1             72 Week 1            1\n3     1     2 v2             64 Week 2            2\n4     1     2 v3             72 Week 3            3\n5     1     2 v4             61 Week 4            4\n6     1     2 v5             61 Week 5            5\n\n\n\nWorking with factors\nYou might notice that when we try to sort or plot this data, the time points don’t appear in chronological order. This is because R treats them as character strings and sorts them alphabetically. We need to convert them to factors with a specific order.\n\n\n\n\n\n\nNoteWhat are factors?\n\n\n\nFactors are R’s way of handling categorical data with a specific order or set of allowed values. They’re especially important in data analysis to:\n\nEnsuring consistent category names (no typos)\nControlling the order of categories in tables and plots\nProper statistical analysis of categorical variables\n\n\n\nLet’s see the problem first:\n\n# Check the current order\nunique(df_lsas_long$time_clean)\n\n [1] \"Baseline\"           \"Week 1\"             \"Week 2\"            \n [4] \"Week 3\"             \"Week 4\"             \"Week 5\"            \n [7] \"Week 6\"             \"Week 7\"             \"Week 8\"            \n[10] \"Post-treatment\"     \"6-month follow-up\"  \"12-month follow-up\"\n\n# See what happens when we sort\nsort(unique(df_lsas_long$time_clean))\n\n [1] \"12-month follow-up\" \"6-month follow-up\"  \"Baseline\"          \n [4] \"Post-treatment\"     \"Week 1\"             \"Week 2\"            \n [7] \"Week 3\"             \"Week 4\"             \"Week 5\"            \n[10] \"Week 6\"             \"Week 7\"             \"Week 8\"            \n\n\nNow let’s fix this by creating a properly ordered factor:\n\nlibrary(forcats) # Part of tidyverse, for working with factors\n\ndf_lsas_long &lt;- df_lsas_long |&gt;\n  mutate(\n    # Create a factor for time points with proper ordering\n    time_factor = factor(\n      time_clean,\n      levels = c(\n        \"Baseline\", \"Week 1\", \"Week 2\", \"Week 3\", \"Week 4\",\n        \"Week 5\", \"Week 6\", \"Week 7\", \"Week 8\",\n        \"Post-treatment\", \"6-month follow-up\", \"12-month follow-up\"\n      )\n    ),\n    # Create a factor for treatment groups with meaningful labels\n    group_factor = factor(\n      group,\n      levels = c(0, 1, 2),\n      labels = c(\"Waitlist\", \"Self-guided\", \"Therapist-guided\")\n    )\n  )\n\n# Check our factors\nlevels(df_lsas_long$time_factor)\n\n [1] \"Baseline\"           \"Week 1\"             \"Week 2\"            \n [4] \"Week 3\"             \"Week 4\"             \"Week 5\"            \n [7] \"Week 6\"             \"Week 7\"             \"Week 8\"            \n[10] \"Post-treatment\"     \"6-month follow-up\"  \"12-month follow-up\"\n\nlevels(df_lsas_long$group_factor)\n\n[1] \"Waitlist\"         \"Self-guided\"      \"Therapist-guided\"\n\n\nLet’s see how factors help by comparing how character vs factor variables are ordered:\n\n# First, let's see the unique values in each format\nunique(df_lsas_long$time_clean) |&gt; sort()\n\n [1] \"12-month follow-up\" \"6-month follow-up\"  \"Baseline\"          \n [4] \"Post-treatment\"     \"Week 1\"             \"Week 2\"            \n [7] \"Week 3\"             \"Week 4\"             \"Week 5\"            \n[10] \"Week 6\"             \"Week 7\"             \"Week 8\"            \n\nlevels(df_lsas_long$time_factor)\n\n [1] \"Baseline\"           \"Week 1\"             \"Week 2\"            \n [4] \"Week 3\"             \"Week 4\"             \"Week 5\"            \n [7] \"Week 6\"             \"Week 7\"             \"Week 8\"            \n[10] \"Post-treatment\"     \"6-month follow-up\"  \"12-month follow-up\"\n\n# Compare what happens when we arrange/sort the data\ndf_lsas_long |&gt;\n  select(time_clean, time_factor) |&gt;\n  arrange(time_clean) |&gt;\n  distinct(time_clean, time_factor) |&gt;\n  head(8)\n\n# A tibble: 8 × 2\n  time_clean         time_factor       \n  &lt;chr&gt;              &lt;fct&gt;             \n1 12-month follow-up 12-month follow-up\n2 6-month follow-up  6-month follow-up \n3 Baseline           Baseline          \n4 Post-treatment     Post-treatment    \n5 Week 1             Week 1            \n6 Week 2             Week 2            \n7 Week 3             Week 3            \n8 Week 4             Week 4            \n\ndf_lsas_long |&gt;\n  select(time_clean, time_factor) |&gt;\n  arrange(time_factor) |&gt;\n  distinct(time_clean, time_factor) |&gt;\n  head(8)\n\n# A tibble: 8 × 2\n  time_clean time_factor\n  &lt;chr&gt;      &lt;fct&gt;      \n1 Baseline   Baseline   \n2 Week 1     Week 1     \n3 Week 2     Week 2     \n4 Week 3     Week 3     \n5 Week 4     Week 4     \n6 Week 5     Week 5     \n7 Week 6     Week 6     \n8 Week 7     Week 7     \n\n\nNotice the difference:\n\nThe character variable sorts alphabetically: “12-month follow-up” comes before “6-month follow-up”\nThe factor variable sorts logically: Baseline → Week 1 → Week 2 → … → Post-treatment → Follow-ups\n\nThis is why factors are essential for any categorical variable where order matters!\n\n\n\n\n\n\nTipUseful factor functions from forcats\n\n\n\n\nfct_relevel(): Change the order of factor levels\nfct_reorder(): Reorder factor levels by another variable\nfct_recode(): Change factor level names\nfct_collapse(): Combine factor levels\nfct_lump(): Collapse least common levels into “Other”\n\n\n\n\n\nUsing factors in summaries\nNow our factors will behave properly in summaries and plots:\n\n# Summary by time point (now in correct order!)\ntime_summary &lt;- df_lsas_long |&gt;\n  summarize(\n    n_obs = sum(!is.na(lsas_score)),\n    mean_lsas = mean(lsas_score, na.rm = TRUE),\n    .by = time_factor\n  )\n\ntime_summary\n\n# A tibble: 12 × 3\n   time_factor        n_obs mean_lsas\n   &lt;fct&gt;              &lt;int&gt;     &lt;dbl&gt;\n 1 Baseline             181      84.8\n 2 Week 1               172      80.1\n 3 Week 2               158      79.3\n 4 Week 3               142      77.8\n 5 Week 4               142      75.6\n 6 Week 5               124      73.6\n 7 Week 6               132      70.2\n 8 Week 7               123      69.5\n 9 Week 8               119      67.8\n10 Post-treatment       169      67.1\n11 6-month follow-up    104      57.0\n12 12-month follow-up   101      55  \n\n\n\n\n\n\n\n\nExercise 5: Practice with factors\n\n\n\n\n\n\nCreate a factor for LSAS severity levels with the order: “Mild”, “Moderate”, “Severe”\nCreate a factor for depression categories using PHQ-9 scores with proper ordering\nUse fct_recode() to change “Self-guided” to “Self-guided CBT” in the group factor\nCheck that your factors appear in the correct order when you create a summary table\n\n\n\n\n\n\nWhy factors matter for visualization\nFactors ensure that our plots display categories in the correct order:\n\n# Compare plots with and without proper factor ordering\nlibrary(ggplot2)\n\n# Create sample data for demonstration\nplot_data &lt;- df_lsas_long |&gt;\n  summarize(\n    mean_lsas = mean(lsas_score, na.rm = TRUE),\n    .by = c(time_clean, time_factor, week_num, group_factor)\n  )\n\n# Plot with character variable (wrong order)\np1 &lt;- ggplot(plot_data, aes(x = time_clean, y = mean_lsas, color = group_factor)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = group_factor)) +\n  labs(title = \"Character variable (alphabetical order)\", x = \"Time\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Plot with factor variable (correct order)\np2 &lt;- ggplot(plot_data, aes(x = time_factor, y = mean_lsas, color = group_factor)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = group_factor)) +\n  labs(title = \"Factor variable (logical order)\", x = \"Time\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n# Display both plots\np1\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\n\n\nggplot(\n  plot_data,\n  aes(\n    x = week_num,\n    y = mean_lsas,\n    color = group_factor\n  )\n) +\n  geom_point(size = 3) +\n  geom_line(aes(group = group_factor)) +\n  scale_x_continuous(\n    breaks = plot_data$week_num, # Use numeric positions\n    labels = plot_data$time_clean # Use readable labels\n  ) +\n  labs(title = \"Factor variable + Numeric Positions\", x = \"Time\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    panel.grid.minor.x = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantKey points about factors\n\n\n\n\nAlways use factors for categorical variables that will be used in analysis or plotting\nSet the levels in logical order (not alphabetical)\nUse meaningful labels instead of numeric codes\nFactors control the order in tables, plots, and statistical output\nThe forcats package provides helpful functions for factor manipulation",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#summarizing-longitudinal-data",
    "href": "labs/tidy-data-manipulation.html#summarizing-longitudinal-data",
    "title": "Tidy Data Manipulation",
    "section": "Summarizing longitudinal data",
    "text": "Summarizing longitudinal data\nNow that we have our data in long format, we can easily calculate means by group and time:\n\nlsas_summary &lt;- df_lsas_long |&gt;\n  summarize(\n    n_obs = sum(!is.na(lsas_score)),\n    mean_lsas = mean(lsas_score, na.rm = TRUE),\n    sd_lsas = sd(lsas_score, na.rm = TRUE),\n    .by = c(group_factor, time_factor)\n  )\n\nlsas_summary\n\n# A tibble: 36 × 5\n   group_factor     time_factor    n_obs mean_lsas sd_lsas\n   &lt;fct&gt;            &lt;fct&gt;          &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Therapist-guided Baseline          60      84.2    17.0\n 2 Therapist-guided Week 1            59      78.3    19.2\n 3 Therapist-guided Week 2            51      77.5    20.8\n 4 Therapist-guided Week 3            48      75.7    21.1\n 5 Therapist-guided Week 4            49      72.0    19.1\n 6 Therapist-guided Week 5            45      68.9    23.6\n 7 Therapist-guided Week 6            47      61.9    20.9\n 8 Therapist-guided Week 7            48      61.8    23.9\n 9 Therapist-guided Week 8            43      57.5    21.8\n10 Therapist-guided Post-treatment    54      57.4    23.2\n# ℹ 26 more rows",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/tidy-data-manipulation.html#converting-back-to-wide-format-with-pivot_wider",
    "href": "labs/tidy-data-manipulation.html#converting-back-to-wide-format-with-pivot_wider",
    "title": "Tidy Data Manipulation",
    "section": "Converting back to wide format with pivot_wider()",
    "text": "Converting back to wide format with pivot_wider()\nSometimes we need to convert back to wide format, for example to create a table with means by group and time:\n\n# Create a wide format table with groups as columns (just means)\nlsas_wide_simple &lt;- lsas_summary |&gt;\n  select(time_factor, group_factor, mean_lsas) |&gt;\n  pivot_wider(\n    names_from = group_factor,\n    values_from = mean_lsas,\n    names_prefix = \"mean_\"\n  )\n\nlsas_wide_simple\n\n# A tibble: 12 × 4\n   time_factor        `mean_Therapist-guided` `mean_Self-guided` mean_Waitlist\n   &lt;fct&gt;                                &lt;dbl&gt;              &lt;dbl&gt;         &lt;dbl&gt;\n 1 Baseline                              84.2               83.1          87.0\n 2 Week 1                                78.3               78.5          83.5\n 3 Week 2                                77.5               78.4          81.9\n 4 Week 3                                75.7               77.3          80.3\n 5 Week 4                                72.0               71.2          82.3\n 6 Week 5                                68.9               69.6          81.0\n 7 Week 6                                61.9               67.8          80.0\n 8 Week 7                                61.8               69.5          78.0\n 9 Week 8                                57.5               67.1          77.9\n10 Post-treatment                        57.4               64.9          78.4\n11 6-month follow-up                     56.4               57.7         NaN  \n12 12-month follow-up                    55.7               54.2         NaN  \n\n# More complex example: keeping multiple statistics (mean, sd, and n)\nlsas_wide_detailed &lt;- lsas_summary |&gt;\n  pivot_wider(\n    names_from = group_factor,\n    values_from = c(mean_lsas, sd_lsas, n_obs),\n    names_sep = \"_\"\n  )\n\nlsas_wide_detailed |&gt;\n  knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime_factor\nmean_lsas_Therapist-guided\nmean_lsas_Self-guided\nmean_lsas_Waitlist\nsd_lsas_Therapist-guided\nsd_lsas_Self-guided\nsd_lsas_Waitlist\nn_obs_Therapist-guided\nn_obs_Self-guided\nn_obs_Waitlist\n\n\n\n\nBaseline\n84.22\n83.08\n86.98\n16.95\n12.97\n18.99\n60\n61\n60\n\n\nWeek 1\n78.27\n78.45\n83.47\n19.17\n14.41\n19.63\n59\n55\n58\n\n\nWeek 2\n77.53\n78.37\n81.95\n20.79\n16.92\n22.48\n51\n52\n55\n\n\nWeek 3\n75.67\n77.28\n80.27\n21.10\n18.22\n21.29\n48\n43\n51\n\n\nWeek 4\n71.98\n71.24\n82.35\n19.13\n17.59\n24.01\n49\n41\n52\n\n\nWeek 5\n68.91\n69.58\n81.02\n23.62\n20.87\n21.33\n45\n33\n46\n\n\nWeek 6\n61.87\n67.81\n80.04\n20.90\n19.88\n25.35\n47\n36\n49\n\n\nWeek 7\n61.75\n69.52\n77.98\n23.94\n21.46\n26.96\n48\n31\n44\n\n\nWeek 8\n57.47\n67.13\n77.87\n21.82\n21.87\n25.72\n43\n30\n46\n\n\nPost-treatment\n57.35\n64.91\n78.45\n23.19\n21.08\n25.44\n54\n57\n58\n\n\n6-month follow-up\n56.42\n57.71\nNaN\n25.81\n21.98\nNA\n53\n51\n0\n\n\n12-month follow-up\n55.71\n54.24\nNaN\n26.18\n25.15\nNA\n52\n49\n0\n\n\n\n\n\n\n\n\n\n\n\nTipAdvanced pivot_wider: Multiple value columns\n\n\n\nNotice how pivot_wider() can handle multiple value columns at once:\n\nvalues_from = c(mean_lsas, sd_lsas, n_obs): Pivots all three statistics\nnames_sep = \"_\": Controls how column names are created\nResult: Each group gets three columns (e.g., mean_lsas_Waitlist, sd_lsas_Waitlist, n_obs_Waitlist)\n\nThis is very useful for creating comprehensive summary tables!\n\n\n\n\n\n\n\n\nExercise 6: Practice reshaping data\n\n\n\n\n\n\nConvert the GAD-7 data to long format (select columns starting with “gad”).\nCalculate the mean GAD-7 score by group and time point.\nCreate a wide format table showing GAD-7 means with time points as rows and groups as columns.\n\n\n\n\n\n\n\n\n\n\nExercise 7: Comprehensive data manipulation\n\n\n\n\n\nCombine everything you’ve learned to:\n\nCreate a dataset with only baseline and post-treatment measurements for LSAS, GAD-7, and PHQ-9.\nCalculate change scores for each measure.\nCreate a summary table showing mean change scores by treatment group.\nCreate categories for treatment response (e.g., “Improved” if LSAS decreased by ≥10 points).\nCalculate the percentage of responders in each treatment group.\n\n\n\n\n\n# Using gt for better table formatting\nlibrary(gt)\n\nmy_table &lt;- lsas_wide_detailed |&gt;\n  gt() |&gt;\n  tab_spanner(\n    label = \"Waitlist\",\n    columns = contains(\"Waitlist\")\n  ) |&gt;\n  tab_spanner(\n    label = \"Self-guided\",\n    columns = contains(\"Self-guided\")\n  ) |&gt;\n  tab_spanner(\n    label = \"Therapist-guided\",\n    columns = contains(\"Therapist-guided\")\n  ) |&gt;\n  cols_label(\n    time_factor = \"Time Point\",\n    mean_lsas_Waitlist = \"M\",\n    sd_lsas_Waitlist = \"SD\",\n    n_obs_Waitlist = \"N\",\n    `mean_lsas_Self-guided` = \"M\",\n    `sd_lsas_Self-guided` = \"SD\",\n    `n_obs_Self-guided` = \"N\",\n    `mean_lsas_Therapist-guided` = \"M\",\n    `sd_lsas_Therapist-guided` = \"SD\",\n    `n_obs_Therapist-guided` = \"N\"\n  ) |&gt;\n  fmt_number(\n    columns = starts_with(\"mean_\") | starts_with(\"sd_\"),\n    decimals = 2\n  ) |&gt;\n  fmt_number(\n    columns = starts_with(\"n_obs\"),\n    decimals = 0\n  ) |&gt;\n  sub_missing(\n    columns = everything(),\n    missing_text = \"—\"\n  ) |&gt;\n  # Custom styling\n  tab_options(\n    table.border.top.style = \"solid\",\n    table.border.top.width = px(2),\n    table.border.bottom.style = \"solid\",\n    table.border.bottom.width = px(2),\n    table.border.left.style = \"none\",\n    table.border.right.style = \"none\",\n    heading.border.bottom.style = \"solid\",\n    heading.border.bottom.width = px(1),\n    column_labels.border.top.style = \"solid\",\n    column_labels.border.top.width = px(1),\n    column_labels.border.bottom.style = \"solid\",\n    column_labels.border.bottom.width = px(1),\n    table_body.border.bottom.style = \"none\",\n    table.font.size = 12\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_spanners()\n  ) |&gt;\n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_column_labels()\n  ) |&gt;\n  tab_style(\n    style = cell_text(align = \"center\"),\n    locations = cells_body(columns = -time_factor)\n  ) |&gt;\n  # Add table title and subtitle\n  tab_header(\n    title = \"Table 1\",\n    subtitle = \"LSAS Scores by Treatment Group and Time Point\"\n  ) |&gt;\n  # Add table note\n  tab_source_note(\n    source_note = \"Note. LSAS = Liebowitz Social Anxiety Scale; M = Mean; SD = Standard Deviation; N = Sample Size.\"\n  )\n\nmy_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1\n\n\nLSAS Scores by Treatment Group and Time Point\n\n\nTime Point\n\nTherapist-guided\n\n\nSelf-guided\n\n\nWaitlist\n\n\n\nM\nSD\nN\nM\nSD\nN\nM\nSD\nN\n\n\n\n\nBaseline\n84.22\n16.95\n60\n83.08\n12.97\n61\n86.98\n18.99\n60\n\n\nWeek 1\n78.27\n19.17\n59\n78.45\n14.41\n55\n83.47\n19.63\n58\n\n\nWeek 2\n77.53\n20.79\n51\n78.37\n16.92\n52\n81.95\n22.48\n55\n\n\nWeek 3\n75.67\n21.10\n48\n77.28\n18.22\n43\n80.27\n21.29\n51\n\n\nWeek 4\n71.98\n19.13\n49\n71.24\n17.59\n41\n82.35\n24.01\n52\n\n\nWeek 5\n68.91\n23.62\n45\n69.58\n20.87\n33\n81.02\n21.33\n46\n\n\nWeek 6\n61.87\n20.90\n47\n67.81\n19.88\n36\n80.04\n25.35\n49\n\n\nWeek 7\n61.75\n23.94\n48\n69.52\n21.46\n31\n77.98\n26.96\n44\n\n\nWeek 8\n57.47\n21.82\n43\n67.13\n21.87\n30\n77.87\n25.72\n46\n\n\nPost-treatment\n57.35\n23.19\n54\n64.91\n21.08\n57\n78.45\n25.44\n58\n\n\n6-month follow-up\n56.42\n25.81\n53\n57.71\n21.98\n51\n—\n—\n0\n\n\n12-month follow-up\n55.71\n26.18\n52\n54.24\n25.15\n49\n—\n—\n0\n\n\n\nNote. LSAS = Liebowitz Social Anxiety Scale; M = Mean; SD = Standard Deviation; N = Sample Size.\n\n\n\n\n\n\n\ngtsave(my_table, \"tables/my_table.docx\")",
    "crumbs": [
      "Labs",
      "Tidy data manipulation"
    ]
  },
  {
    "objectID": "labs/sampling.html",
    "href": "labs/sampling.html",
    "title": "Sampling from a population",
    "section": "",
    "text": "So far, we have restricted ourselves to describing the sample that we have. Often, however, we are not only interested about our sample, but want to make inferences about the population from which our sample came.",
    "crumbs": [
      "Labs",
      "Sampling from a population"
    ]
  },
  {
    "objectID": "labs/sampling.html#standard-error-of-a-mean",
    "href": "labs/sampling.html#standard-error-of-a-mean",
    "title": "Sampling from a population",
    "section": "Standard error of a mean",
    "text": "Standard error of a mean\nThe formula for the standard error of a mean, if we knew the population variance \\(\\sigma^2\\), is:\n\\[\nSE = {\\sqrt{\\sigma^2 / n}}\n\\]\nHowever, we rarely know the population standard deviation. Luckily we can use the sample variance \\(s^2\\) to estimate it:\n\\[\nSE = {\\sqrt{s^2 / n}}\n\\]\nBase R has no built-in functions for standard errors, but you can easily calculate them from formulas. To calculate the standard error of the LSAS scale at screening, we can use the following code:\n\nsd(df_data$lsas_screen)/sqrt(length(df_data$lsas_screen))\n\n[1] 1.224066\n\n# or we can create a function\nstderr &lt;- function(x) sd(x) / sqrt(length(x))\nstderr(df_data$lsas_screen)\n\n[1] 1.224066",
    "crumbs": [
      "Labs",
      "Sampling from a population"
    ]
  },
  {
    "objectID": "labs/sampling.html#standard-error-of-a-proportion",
    "href": "labs/sampling.html#standard-error-of-a-proportion",
    "title": "Sampling from a population",
    "section": "Standard error of a proportion",
    "text": "Standard error of a proportion\nFor a proportion, the standard error for the population is calculated by the formula:\n\\[\n\\mathrm{SE}(p) = \\sqrt{\\frac{p(1 - p)}{n}}\n\\]\nWe can get this in R using the following code:\n\n# Simulating a gender variable \nn &lt;- nrow(df_data) \ndf_data$gender &lt;- rbinom(n, 1, 0.7)\ndf_data$gender &lt;- ifelse(df_data$gender == 1, \"Woman\", \"Man\")\n\np &lt;- mean(df_data$gender==\"Man\") # using the mean function to get the probability \nn &lt;- nrow(df_data) # the number of observations (since we have no NA values) \nse &lt;- sqrt(p * (1 - p) / n) # standard error \nse\n\n[1] 0.03400798",
    "crumbs": [
      "Labs",
      "Sampling from a population"
    ]
  },
  {
    "objectID": "r-setup.html",
    "href": "r-setup.html",
    "title": "R and RStudio setup",
    "section": "",
    "text": "To participate in the lab sessions, you need to install R and RStudio on your computer. You can download R from CRAN and RStudio from RStudio. Both are free to use.",
    "crumbs": [
      "Course information",
      "R and RStudio setup"
    ]
  },
  {
    "objectID": "r-setup.html#execute-a-test-command-in-the-r-console",
    "href": "r-setup.html#execute-a-test-command-in-the-r-console",
    "title": "R and RStudio setup",
    "section": "Execute a test command in the R console",
    "text": "Execute a test command in the R console\nIf both R and RStudio are installed and talking to each other, you should see something like this in the Console pane:\n\n\n\nR console default\n\n\nTry running a simple command to see that R is working. For example, type 1 + 1 and press enter, or try print(\"Hello, world!\"). If you see the output below, you are good to go! See the steps below for setting up RStudio.",
    "crumbs": [
      "Course information",
      "R and RStudio setup"
    ]
  },
  {
    "objectID": "r-setup.html#disable-saving-workspace-and-history",
    "href": "r-setup.html#disable-saving-workspace-and-history",
    "title": "R and RStudio setup",
    "section": "Disable saving workspace and history",
    "text": "Disable saving workspace and history\nThere are a few things we want you to change in the default RStudio settings. To access the settings in RStudio, go to Tools -&gt; Global Options... in the menu bar or press Cmd + , (Mac) or Ctrl + , (Windows/Linux). Then navigate to the General tab. Make sure to set the options according to the image below:\n\nThe purpose of these settings is to ensure that you do not save the workspace image or history when you close RStudio, since we do not want to keep any temporary objects or commands that you have used during the session. This is important for reproducibility and to avoid cluttering your workspace with unnecessary files.",
    "crumbs": [
      "Course information",
      "R and RStudio setup"
    ]
  },
  {
    "objectID": "r-setup.html#working-in-projects",
    "href": "r-setup.html#working-in-projects",
    "title": "R and RStudio setup",
    "section": "Working in projects",
    "text": "Working in projects\nRStudio projects are a great way to organize your work. They allow you to keep all your files, data, and code in one place, making it easier to manage your projects. To create a new project, go to the upper right corner where it says “Project: (None)”. Then choose a location for your project and give it a name (maybe biostat-labs 🤓).",
    "crumbs": [
      "Course information",
      "R and RStudio setup"
    ]
  },
  {
    "objectID": "r-setup.html#folder-structure",
    "href": "r-setup.html#folder-structure",
    "title": "R and RStudio setup",
    "section": "Folder structure",
    "text": "Folder structure\nRegardless of the project you are working on, you will benefit from having a well-organized folder structure. The needs may vary depending on the project, but a good starting point is to have separate folders for data, scripts, documentation, and output. This will help you keep your files organized and make it easier to find what you need later on. In this course, we will be assuming you have the following folder structure:\nbiostat-labs/\n├── data/\n├── R/\n├── docs/\n├── output/\n├── README.md\n\ndata/: This folder can contain raw data, but if you have sensitive data you will likely store it on a secure server. It’s safe to store the data you are working with in this course here, however.\nR/: This folder contains the scripts you are working on.\ndocs/: This folder contains the documentation for your project. One good way to distinguish this from output is that the docs folder contains files and background information that is not generate by the code you are writing, for example documentation about questionnaires.\noutput/: This folder contains the output of your code, such as figures, tables, and reports. This is where you will save the results of your analysis. Feel free to create subfolders as needed.\nREADME.md: This file contains information about your project, such as the purpose of the project, how to run the code, and any other relevant information. It’s good practice to keep this file updated as it is often the “landing page” when others collaborate in your project.",
    "crumbs": [
      "Course information",
      "R and RStudio setup"
    ]
  },
  {
    "objectID": "labs/classification.html",
    "href": "labs/classification.html",
    "title": "Classification",
    "section": "",
    "text": "In this chapter, we will look at how we can work with classification problems in R. You will learn how to evaluate sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). These measures help us understand how well a model or test can identify or predict binary outcomes (e.g., disease status, treatment response). You will not work on the models themselves; that is the topic of upcoming courses and chapters.",
    "crumbs": [
      "Labs",
      "Classification"
    ]
  },
  {
    "objectID": "labs/classification.html#sensitivity",
    "href": "labs/classification.html#sensitivity",
    "title": "Classification",
    "section": "Sensitivity",
    "text": "Sensitivity\nSensitivity is the proportion of true positives out of all actual positives.\n\nsensitivity &lt;- true_pos / (true_pos + false_neg)\n\nSensitivity is 0.37.",
    "crumbs": [
      "Labs",
      "Classification"
    ]
  },
  {
    "objectID": "labs/classification.html#specificity",
    "href": "labs/classification.html#specificity",
    "title": "Classification",
    "section": "Specificity",
    "text": "Specificity\nSpecificity is the proportion of true negatives out of all actual negatives.\n\nspecificity &lt;- true_neg / (true_neg + false_pos)\n\nSpecificity is 0.94.",
    "crumbs": [
      "Labs",
      "Classification"
    ]
  },
  {
    "objectID": "labs/classification.html#positive-predictive-value",
    "href": "labs/classification.html#positive-predictive-value",
    "title": "Classification",
    "section": "Positive predictive value",
    "text": "Positive predictive value\nPositive predictive value is the proportion of true positives out of all predicted positives. See the difference compared to sensitivity?\n\npos_pred_val &lt;- true_pos / (true_pos + false_pos)\n\nPositive predictive value is 0.7.",
    "crumbs": [
      "Labs",
      "Classification"
    ]
  },
  {
    "objectID": "labs/classification.html#negative-predictive-value",
    "href": "labs/classification.html#negative-predictive-value",
    "title": "Classification",
    "section": "Negative predictive value",
    "text": "Negative predictive value\nNegative predictive value is the proportion of true negatives out of all predicted negatives. Again, look at the difference compared to specificity.\n\nneg_pred_val &lt;- true_neg / (true_neg + false_neg)\n\nNegative predictive value is 0.82.",
    "crumbs": [
      "Labs",
      "Classification"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html",
    "href": "labs/power-sample-size.html",
    "title": "Power Analysis and Sample Size Planning",
    "section": "",
    "text": "In this lab, we will learn about power analysis and sample size planning - fundamental concepts that help us design studies that can reliably detect meaningful effects. Think of power analysis as a way to evaluate the sensitivity of a statistical test, it helps you make informed decisions about your study design.\nPower analysis answers three important questions:\nThese questions are interconnected - changing one affects the others. Throughout this lab, you’ll use R to explore these relationships and build intuition about how they work together.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#learning-objectives",
    "href": "labs/power-sample-size.html#learning-objectives",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this lab, you will be able to:\n\nUnderstand the four components of power analysis and how they relate to each other\nUse R functions to calculate power and sample size\nDistinguish between different types of hypothesis tests (superiority, non-inferiority, equivalence) and when to use each\n\nEach learning objective will be reinforced through hands-on R coding exercises that build your understanding step by step.\n\n\n\n\n\n\nNoteWhy power analysis matters in biostatistics\n\n\n\nIn clinical research, inadequate sample sizes lead to:\n\nUnderpowered studies: Cannot detect clinically meaningful effects\nWasted resources: Time, money, and participant burden without informative results\nEthical concerns: Exposing participants to research without sufficient chance of meaningful findings",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#the-four-key-components",
    "href": "labs/power-sample-size.html#the-four-key-components",
    "title": "Power Analysis and Sample Size Planning",
    "section": "The four key components",
    "text": "The four key components\nLooking at the figure above, you can see that power analysis revolves around four interconnected components. This is like a statistical equation with four variables - if you know any three, you can solve for the fourth:\n\nEffect size (d): How large is the difference we want to detect? In clinical research, this is often the smallest difference that would be clinically meaningful.\nSample size (n): How many participants do we need per group? This is often what we’re trying to determine before starting data collection.\nSignificance level (α): The probability of falsely concluding there’s an effect when there isn’t one. Conventionally set at 0.05.\nPower (1-β): The probability of detecting an effect if it truly exists. Conventionally set at 0.8 or 0.9.\n\nFor more complex designs beyond simple t-tests, you’ll need to consider additional parameters, such as correlations between repeated measures in longitudinal studies.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#load-packages",
    "href": "labs/power-sample-size.html#load-packages",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Load packages",
    "text": "Load packages\nLet’s start by loading the R packages we’ll use throughout this lab.\n\nlibrary(tidyverse) # For data manipulation and visualization\nlibrary(pwr) # For power analysis calculations\n\nThe pwr package contains functions for different types of basic statistical tests.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#understanding-the-pwr.t.test-functione",
    "href": "labs/power-sample-size.html#understanding-the-pwr.t.test-functione",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Understanding the pwr.t.test() functione",
    "text": "Understanding the pwr.t.test() functione\nThe pwr.t.test() function calculates power for t-tests. Let’s start with a simple example:\n\n# Calculate power when we know effect size, sample size, and alpha\npower_result &lt;- pwr.t.test(\n  d = 0.5, # Medium effect size (Cohen's d)\n  n = 64, # Sample size per group\n  sig.level = 0.05, # Alpha level (Type I errors)\n  type = \"two.sample\", # Two independent groups\n  alternative = \"two.sided\" # Two-tailed test\n)\n\npower_result\n\n\n     Two-sample t test power calculation \n\n              n = 64\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8014596\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nInterpreting the result: This output tells us that with 64 participants per group, we have about 80% power to detect a medium effect (d = 0.5).",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#sample-size-calculation",
    "href": "labs/power-sample-size.html#sample-size-calculation",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Sample size calculation",
    "text": "Sample size calculation\nOften, the most practical question is: “How many participants do I need?” This is where power analysis becomes a planning tool. We simply omit the n parameter and let R calculate the required sample size for us:\n\n# How many participants do we need for 80% power to detect d = 0.5?\nsample_size_result &lt;- pwr.t.test(\n  d = 0.5, # Effect size we want to detect\n  power = 0.80, # Desired power level\n  sig.level = 0.05, # Alpha level\n  type = \"two.sample\", # Two independent groups\n  alternative = \"two.sided\" # Two-tailed test\n)\n\nsample_size_result\n\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nWe need approximately 64 participants per group (128 total) to have 80% power to detect a medium effect.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#effect-size-calculation",
    "href": "labs/power-sample-size.html#effect-size-calculation",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Effect size calculation",
    "text": "Effect size calculation\nSometimes you might have constraints on your sample size (due to budget, time, or participant availability) and want to know: “What’s the smallest effect I can reliably detect?” This helps set realistic expectations for your study:\n\n# What effect size can we detect with 50 participants per group?\neffect_size_result &lt;- pwr.t.test(\n  n = 50, # Available sample size per group\n  power = 0.80, # Desired power level\n  sig.level = 0.05, # Alpha level\n  type = \"two.sample\", # Two independent groups\n  alternative = \"two.sided\" # Two-tailed test\n)\n\neffect_size_result\n\n\n     Two-sample t test power calculation \n\n              n = 50\n              d = 0.565858\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nThis type of calculation helps you decide whether a study is worth conducting given your constraints, or whether you need to find ways to increase your sample size.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#power-curves-sample-size-vs-power",
    "href": "labs/power-sample-size.html#power-curves-sample-size-vs-power",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Power curves (sample size vs power)",
    "text": "Power curves (sample size vs power)\nPower curves are one of the most useful visualizations in research planning. They show you exactly how statistical power increases as you add more participants to your study. They help you find the sweet spot between adequate power and practical constraints.\nLet’s create a power curve showing how power changes with sample size for a medium effect (d = 0.5).\n\n# Create a range of sample sizes\nsample_sizes &lt;- seq(10, 100, by = 5)\n\n# Calculate power for each sample size (effect size = 0.5)\npower_data &lt;- map(\n  sample_sizes,\n  \\(n) {\n    result &lt;- pwr.t.test(\n      d = 0.5,\n      n = n,\n      sig.level = 0.05,\n      type = \"two.sample\",\n      alternative = \"two.sided\"\n    )\n    data.frame(\n      n = n,\n      power = result$power\n    )\n  }\n) |&gt; list_rbind()\n\n# Create the plot\nggplot(power_data, aes(x = n, y = power)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_hline(\n    yintercept = 0.8,\n    linetype = \"dashed\",\n    color = \"red\"\n  ) +\n  geom_vline(\n    xintercept = 64,\n    linetype = \"dashed\",\n    color = \"red\"\n  ) +\n  labs(\n    title = \"Power Curve for Two-Sample t-test\",\n    subtitle = \"Effect size (d) = 0.5, α = 0.05\",\n    x = \"Sample size (n per group)\",\n    y = \"Power (1 - β)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(\n    limits = c(0, 1),\n    labels = scales::percent_format()\n  )\n\n\n\n\n\n\n\nFigure 2: Power vs sample size for d=0.5 (two-sample t-test). Dashed lines mark ~64 per group for 80% power.\n\n\n\n\n\nThe dashed lines show that we need 64 participants per group to achieve 80% power.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#comparing-different-effect-sizes",
    "href": "labs/power-sample-size.html#comparing-different-effect-sizes",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Comparing different effect sizes",
    "text": "Comparing different effect sizes\nLet’s see how effect size affects the power curves:\n\n# Create data for multiple effect sizes\neffect_sizes &lt;- c(0.2, 0.5, 0.8)\nsample_sizes &lt;- seq(10, 150, by = 5)\n\npower_comparison &lt;- expand_grid(\n  n = sample_sizes,\n  d = effect_sizes\n) |&gt;\n  mutate(\n    power = map2_dbl(\n      d, n,\n      \\(d, n) {\n        pwr.t.test(\n          d = d,\n          n = n,\n          sig.level = 0.05,\n          type = \"two.sample\",\n          alternative = \"two.sided\"\n        )$power\n      }\n    ),\n    effect_label = factor(\n      d,\n      levels = c(0.2, 0.5, 0.8),\n      labels = c(\"Small (d=0.2)\", \"Medium (d=0.5)\", \"Large (d=0.8)\")\n    )\n  )\n\n# Create the comparison plot\nggplot(power_comparison, aes(x = n, y = power, color = effect_label)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", alpha = 0.7) +\n  labs(\n    title = \"Power Curves for Different Effect Sizes\",\n    subtitle = \"Two-sample t-test, α = 0.05\",\n    x = \"Sample size (n per group)\",\n    y = \"Power (1 - β)\",\n    color = \"Effect size\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format()) +\n  scale_color_viridis_d()\n\n\n\n\n\n\n\nFigure 3: Power curves for small, medium, and large effects; larger effects require smaller n for the same power.\n\n\n\n\n\nUnsurprisingly, the size of the effect you’re trying to detect dramatically impacts your sample size needs. However, in clinical studies this hypothetical effect size is usually based on the minimum clinically meaningful effect size.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#creating-a-data-simulation-function",
    "href": "labs/power-sample-size.html#creating-a-data-simulation-function",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Creating a data simulation function",
    "text": "Creating a data simulation function\nBefore we can simulate thousands of studies, we need a way to generate realistic data for each virtual study. Let’s create a function that produces data that mimics what you might see in a real two-group comparison study.\nThis function will be your “fake data factory” - each time you call it, it creates a new study with randomly generated participants:\n\n# Function to simulate data for a two-group study\nsimulate_study &lt;- function(n_per_group, effect_size, sd = 1) {\n  # Generate control group data (mean = 0)\n  control_group &lt;- rnorm(n_per_group, mean = 0, sd = sd)\n\n  # Generate treatment group data (mean = effect_size)\n  treatment_group &lt;- rnorm(n_per_group, mean = effect_size, sd = sd)\n\n  # Combine into a data frame\n  study_data &lt;- data.frame(\n    score = c(control_group, treatment_group),\n    group = factor(\n      rep(c(0, 1), each = n_per_group),\n      labels = c(\"Control\", \"Treatment\")\n    )\n  )\n\n  return(study_data)\n}\n\n# Test our function with a concrete example\nset.seed(123) # For reproducible results across runs\nexample_study &lt;- simulate_study(n_per_group = 30, effect_size = 0.5)\n\n# Look at the first few rows to see the structure\nhead(example_study)\n\n        score   group\n1 -0.56047565 Control\n2 -0.23017749 Control\n3  1.55870831 Control\n4  0.07050839 Control\n5  0.12928774 Control\n6  1.71506499 Control\n\n# Calculate means for each group to verify our effect size\nexample_study |&gt;\n  summarize(\n    mean_score = mean(score),\n    .by = group\n  )\n\n      group  mean_score\n1   Control -0.04710376\n2 Treatment  0.67833834\n\n\n\n\n\n\n\n\nNoteUnderstanding the output\n\n\n\nNotice how the function creates a dataset with 60 participants (30 per group) and the Treatment group has a higher mean score than the Control group. The difference between means approximates our specified effect size (0.5), though it won’t be exactly 0.5 due to random sampling variability.\nThis variability is crucial to understand, it’s why we need many participants and why we use statistical tests to account for uncertainty.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#running-a-single-simulated-study",
    "href": "labs/power-sample-size.html#running-a-single-simulated-study",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Running a single simulated study",
    "text": "Running a single simulated study\nNow let’s analyze one simulated study and see if we get a significant result:\n\n# Simulate one study\nset.seed(456)\nstudy_data &lt;- simulate_study(n_per_group = 30, effect_size = 0.5)\n\n# Run a t-test\nt_test_result &lt;- t.test(score ~ group, data = study_data, var.equal = TRUE)\nt_test_result\n\n\n    Two Sample t-test\n\ndata:  score by group\nt = -1.6249, df = 58, p-value = 0.1096\nalternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n95 percent confidence interval:\n -0.9472288  0.0984278\nsample estimates:\n  mean in group Control mean in group Treatment \n              0.2317430               0.6561435 \n\n# Extract key information\np_value &lt;- t_test_result$p.value\neffect_detected &lt;- p_value &lt; 0.05\n\ncat(\"P-value:\", round(p_value, 4), \"\\n\")\n\nP-value: 0.1096 \n\ncat(\"Significant result (p &lt; 0.05):\", effect_detected, \"\\n\")\n\nSignificant result (p &lt; 0.05): FALSE \n\n\nIn this single study, we did not detect a significant effect. This illustrates an important point - even when there’s a true effect and adequate power, individual studies can still “miss” the effect due to random sampling. But what happens when we run many studies with the same design?",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#simulating-many-studies-to-understand-power",
    "href": "labs/power-sample-size.html#simulating-many-studies-to-understand-power",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Simulating many studies to understand power",
    "text": "Simulating many studies to understand power\nWe’ll run 10,000 virtual studies with identical designs and count how many detect a significant effect. This proportion directly shows us the empirical power - and it should match our theoretical calculations.\nThe code below creates a simulation function that tracks multiple aspects of each study:\n\n# Function to run many simulated studies (including confidence intervals)\nrun_power_simulation &lt;- function(\n    n_per_group,\n    effect_size,\n    n_simulations = 1000,\n    alpha = 0.05,\n    conf_level = 0.95) {\n  # Store results for each simulation\n  results &lt;- data.frame(\n    simulation = 1:n_simulations,\n    p_value = numeric(n_simulations),\n    ci_lower = numeric(n_simulations),\n    ci_upper = numeric(n_simulations),\n    significant = logical(n_simulations),\n    effect = numeric(n_simulations),\n    se = numeric(n_simulations),\n    t_value = numeric(n_simulations)\n  )\n\n  # Run many simulated studies\n  for (i in 1:n_simulations) {\n    # Simulate data\n    study_data &lt;- simulate_study(n_per_group, effect_size)\n\n    # Run t-test (automatically gives us p-value and CI)\n    test_result &lt;- t.test(\n      score ~ relevel(group, ref = \"Treatment\"),\n      data = study_data,\n      conf.level = conf_level,\n      var.equal = TRUE\n    )\n\n    # Store results\n    results$p_value[i] &lt;- test_result$p.value\n    results$ci_lower[i] &lt;- test_result$conf.int[1]\n    results$ci_upper[i] &lt;- test_result$conf.int[2]\n    results$significant[i] &lt;- test_result$p.value &lt; alpha\n    results$effect[i] &lt;- test_result$estimate |&gt;\n      rev() |&gt;\n      diff()\n    results$se[i] &lt;- test_result$stderr\n    results$t_value[i] &lt;- test_result$statistic\n  }\n\n  return(results)\n}\n\n# Run simulation with medium effect size\nset.seed(787)\nsim_results &lt;- run_power_simulation(\n  n_per_group = 50,\n  effect_size = 0.5,\n  n_simulations = 10000\n)\n\nSince we simulated data where there truly is an effect (alternative hypothesis), empirical power is simply the proportion of studies that correctly detected this effect:\n\nempirical_power &lt;- mean(sim_results$p_value &lt; 0.05)\nempirical_power\n\n[1] 0.6964\n\n\nOut of 10,000 simulated studies, 69.6% found a statistically significant result. This is our empirical power - what actually happened when we “ran” thousands of studies.\nNow let’s compare our empirical results with the theoretical power we calculated earlier:\n\ntheoretical_power &lt;- pwr.t.test(\n  n = 50,\n  d = 0.5,\n  sig.level = 0.05,\n  type = \"two.sample\"\n)$power\ntheoretical_power\n\n[1] 0.6968934\n\n\nThe simulation results (69.6%) are very close to the theoretical power (69.7%). The small difference is expected due to simulation error.\n\n\n\n\n\n\nNotePedagogical choice: For-loops vs functional programming\n\n\n\n\n\nWe’re using for-loops here because they tend to be easier to understand when learning. You can see exactly what happens in each iteration. However, experienced R users often prefer functional programming approaches that are more concise.\nHere’s the same function using more advanced R techniques (including parallel processing):\n\n# Alternative: Using map() |&gt; list_rbind() from purrr (more advanced)\n# and parallel processing with mirai\nlibrary(mirai)\nrun_power_sim_functional &lt;- function(\n    n_per_group,\n    effect_size,\n    n_simulations = 1000,\n    alpha = 0.05) {\n  map(\n    1:n_simulations,\n    in_parallel( # nolint: object_usage_linter\n      \\(i) {\n        control_group &lt;- rnorm(n_per_group, mean = 0, sd = 1)\n        treatment_group &lt;- rnorm(n_per_group, mean = effect_size, sd = 1)\n\n        study_data &lt;- data.frame(\n          score = c(control_group, treatment_group),\n          group = rep(c(\"Control\", \"Treatment\"), each = n_per_group)\n        )\n\n        test_result &lt;- t.test(score ~ group, data = study_data)\n\n        tibble::tibble(\n          simulation = i,\n          p_value = test_result$p.value,\n          ci_lower = test_result$conf.int[1],\n          ci_upper = test_result$conf.int[2],\n          significant = test_result$p.value &lt; alpha\n        )\n      },\n      n_per_group = n_per_group,\n      effect_size = effect_size,\n      alpha = alpha\n    )\n  ) |&gt; list_rbind()\n}\n# Run on 8 CPU threads in parallel\ndaemons(8)\nrun_power_sim_functional(\n  n_per_group = sample_size_per_group,\n  effect_size = 0.5,\n  n_simulations = 10000\n)\n\n# A tibble: 10,000 × 5\n   simulation  p_value ci_lower ci_upper significant\n        &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;      \n 1          1 0.00868    -0.993  -0.148  TRUE       \n 2          2 0.105      -0.634   0.0607 FALSE      \n 3          3 0.756      -0.492   0.358  FALSE      \n 4          4 0.195      -0.639   0.132  FALSE      \n 5          5 0.00202    -1.12   -0.257  TRUE       \n 6          6 0.973      -0.392   0.379  FALSE      \n 7          7 0.100      -0.789   0.0704 FALSE      \n 8          8 0.000495   -1.11   -0.322  TRUE       \n 9          9 0.000787   -1.15   -0.313  TRUE       \n10         10 0.00398    -1.04   -0.204  TRUE       \n# ℹ 9,990 more rows\n\ndaemons(0)",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#visualizing-simulation-results",
    "href": "labs/power-sample-size.html#visualizing-simulation-results",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Visualizing simulation results",
    "text": "Visualizing simulation results\nLet’s visualize what happened in our 10,000 studies. Let’s overlay the simulation results on the theoretical null and alternative distributions. Before we can do this, we need to generate data from the null model.\n\nnull_results &lt;- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = 0,\n  n_simulations = 10000\n)\n\nThen we can plot the theoretical and empirical H0 and H1 distributions.\n\npower_plot +\n  # Add empirical histograms from simulations\n  geom_histogram(\n    data = null_results,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"gray30\",\n    alpha = 0.2,\n    color = \"gray30\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  ) +\n  geom_histogram(\n    data = sim_results,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"#27AE60\",\n    alpha = 0.2,\n    color = \"#1E8449\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  )\n\n\n\n\nEmpirical histograms from simulations (null and alternative) overlaid on theoretical curves.\n\n\n\n\nWe can see that the simulation results under the null and alternative models (the histograms) are very close to the theoretical distributions.\nAnother way to visualize the simulation results is to plot the distribution of p-values.\n\n# Plot distribution of p-values\nggplot(sim_results, aes(x = p_value)) +\n  geom_bar(aes(fill = significant), alpha = 0.7) +\n  labs(\n    title = \"Distribution of P-values from 10,000 Simulated Studies\",\n    subtitle = paste(\n      \"Effect size = 0.5, n = 50 per group, Power =\",\n      round(empirical_power, 3)\n    ),\n    x = \"P-value\",\n    y = \"Count\",\n    fill = \"Significant\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"FALSE\" = \"gray80\",\n      \"TRUE\" = PALETTE$regions[[\"power\"]]\n    )\n  ) +\n  scale_x_binned(n.breaks = 20) +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 4: Distribution of p-values from 10,000 Simulated Studies, showing significant results.\n\n\n\n\n\nLastly, we can visualize power by plotting the 95% confidence intervals. Power is the proportion of confidence intervals that exclude the null effect.\n\nsim_results &lt;- sim_results |&gt;\n  mutate(\n    ci_significant = (ci_lower &lt; 0 & ci_upper &lt; 0) |\n      (ci_lower &gt; 0 & ci_upper &gt; 0)\n  )\n\nsim_results |&gt;\n  summarize(\n    power_ci = mean(ci_significant),\n    power_pvalue = mean(p_value &lt; 0.05)\n  )\n\n  power_ci power_pvalue\n1   0.6964       0.6964\n\n\n\n\nCode\n# Create CI coverage plot for superiority trial\nplot_ci_coverage &lt;- function(\n    sim_data,\n    true_effect = 0,\n    null_effect = 0,\n    title = \"Confidence Interval Coverage\") {\n  # Determine if CI contains true effect\n  sim_data$covers_truth &lt;- (sim_data$ci_lower &lt;= true_effect) &\n    (sim_data$ci_upper &gt;= true_effect)\n\n  # Sort by significance for better visualization\n  sim_data &lt;- sim_data |&gt;\n    arrange(ci_significant) |&gt;\n    mutate(plot_order = row_number())\n\n  ggplot(sim_data, aes(y = plot_order)) + # nolint: object_usage_linter\n    geom_segment(\n      aes(x = ci_lower, xend = ci_upper, color = ci_significant), # nolint: object_usage_linter\n      alpha = 0.7,\n      linewidth = 0.8\n    ) +\n    geom_vline(\n      xintercept = true_effect,\n      color = \"gray50\",\n      linetype = \"dashed\",\n    ) +\n    geom_vline(\n      xintercept = null_effect,\n      color = \"black\",\n      linetype = \"solid\",\n      linewidth = 1\n    ) +\n    scale_color_manual(\n      values = c(\"TRUE\" = \"#27AE60\", \"FALSE\" = PALETTE$regions[[\"beta\"]]),\n      labels = c(\"TRUE\" = \"Significant\", \"FALSE\" = \"Non-significant\"),\n      name = \"Result\"\n    ) +\n    labs(\n      title = title,\n      subtitle = paste(\n        \"True effect =\", true_effect, \"| Power =\",\n        round(mean(sim_data$ci_significant), 3)\n      ),\n      x = \"Effect Size (95% Confidence Interval)\",\n      y = \"Study Number\"\n    ) +\n    theme_minimal() +\n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n}\nsim_results |&gt;\n  sample_n(200) |&gt;\n  plot_ci_coverage(true_effect = 0.5, title = \"95% Confidence Interval Power\")\n\n\n\n\n\n\n\n\nFigure 5: Confidence interval coverage: green lines meet success criteria; brown/red lines do not. Vertical lines mark true effect and null effect.\n\n\n\n\n\n\n\n\n\n\n\nTipKey insights from the simulation\n\n\n\n\nPower: About 70% of our simulated studies detected an effect.\nP-value distribution: As power increases, p-values tend to cluster near zero, but some studies still produce larger p-values by chance\nThe 20% that “fail”: Even with adequate power, studies will miss a real effect due to sampling variability\nWhy replication matters: Any single study might be in that unlucky 20%, which is why we need multiple studies to build scientific confidence",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#what-happens-when-theres-no-effect-type-i-error",
    "href": "labs/power-sample-size.html#what-happens-when-theres-no-effect-type-i-error",
    "title": "Power Analysis and Sample Size Planning",
    "section": "What happens when there’s no effect? (Type I error)",
    "text": "What happens when there’s no effect? (Type I error)\n\n\n\n\n\n\nImportantUnderstanding Type I and Type II errors\n\n\n\n\nType I error (α): Finding an effect when there isn’t one (false positive)\nType II error (β): Missing an effect when there is one (false negative)\nPower = 1 - β: Probability of correctly detecting a true effect\n\nNow let’s see these concepts in action through simulation.\n\n\nLet’s also check the results where we simulated studies from the null hypothesis (no difference between groups):\n\n# Calculate Type I errors\ntype_i_error &lt;- mean(null_results$significant)\n\ncat(\"Type I errors:\", round(type_i_error, 3), \"\\n\")\n\nType I errors: 0.048 \n\ncat(\"Expected Type I errors:\", 0.05, \"\\n\")\n\nExpected Type I errors: 0.05 \n\n# Visualize\nbreaks &lt;- seq(\n  min(null_results$p_value),\n  max(null_results$p_value),\n  length.out = 21\n)\nggplot(null_results, aes(x = p_value)) +\n  geom_bar(aes(fill = significant), alpha = 0.7) +\n  labs(\n    title = \"P-values When There's No True Effect\",\n    subtitle = paste(\"Type I errors =\", round(type_i_error, 3)),\n    x = \"P-value\",\n    y = \"Count\",\n    fill = \"Significant\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"FALSE\" = \"gray80\",\n      \"TRUE\" = PALETTE$regions[[\"alpha\"]]\n    )\n  ) +\n  scale_x_binned(n.breaks = 20) +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 6: Distribution of p-values when there is no true effect from 10,000 simulated studies.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#superiority-trials-standard-approach",
    "href": "labs/power-sample-size.html#superiority-trials-standard-approach",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Superiority trials (standard approach)",
    "text": "Superiority trials (standard approach)\nSuperiority trials are what most people think of as “typical” research studies. The goal is straightforward: prove that one treatment is better than another (or better than no treatment). This is what we’ve been practicing throughout this lab.\nTwo-tailed superiority test (tests for any difference):\n\\[\n\\begin{aligned}\n&\\text{Null hypothesis } (H_0): && d = 0 \\\\\n&\\text{Alternative hypothesis } (H_1): && d \\neq 0\n\\end{aligned}\n\\]\nOne-tailed superiority test (tests for improvement in specific direction):\n\\[\n\\begin{aligned}\n&\\text{Null hypothesis } (H_0): && d \\leq 0 \\\\\n&\\text{Alternative hypothesis } (H_1): && d &gt; 0\n\\end{aligned}\n\\]\nwhere \\(d\\) is the effect size (standardized difference between treatment and control groups).",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#non-inferiority-trials",
    "href": "labs/power-sample-size.html#non-inferiority-trials",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Non-inferiority trials",
    "text": "Non-inferiority trials\nNon-inferiority trials address a different question: “Is this new treatment not worse than the standard treatment by more than an acceptable margin?”\nWhy non-inferiority matters: Imagine you have a standard antidepressant that works well but has significant side effects. A new drug with fewer side effects might be valuable even if it’s slightly less effective - as long as it’s not too much worse.\n\n\n\n\n\n\nNoteNon-inferiority margin\n\n\n\nThe non-inferiority margin is the largest difference we would still consider acceptable. For example, if standard treatment reduces symptoms by 10 points, we might accept that new treatment is non-inferior if it’s no more than 2 points worse.\n\n\nFor non-inferiority, we typically use one-sided tests and need to specify the non-inferiority margin:\n\\[\n\\begin{aligned}\n&\\text{Null hypothesis } (H_0): && d \\leq -\\Delta \\\\\n&\\text{Alternative hypothesis } (H_1): && d &gt; -\\Delta\n\\end{aligned}\n\\]\nwhere \\(\\Delta\\) is the non-inferiority margin and \\(d\\) is the effect size.\nWe will: (1) build intuition with a conceptual non-inferiority figure, (2) check that simulated study statistics line up with the theory, (3) view confidence-interval criteria directly, and (4) compare empirical power with analytical power.\n\n\nCode\n# =============================================================================\n# SIMULATION PARAMETERS\n# =============================================================================\n\n\n# Study design parameters\nsample_size_per_group &lt;- 80\nnum_simulations &lt;- 10000\nconfidence_level &lt;- 0.9\nalpha_level &lt;- 0.05\n\n# Effect size parameters\nnull_effect_size &lt;- -0.2\nalternative_effect_size &lt;- 0.1\n\n# =============================================================================\n# RUN POWER SIMULATIONS\n# =============================================================================\n\n# Simulation under alternative hypothesis (with effect)\nsim_alternative_hypothesis &lt;- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = alternative_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# Simulation under null hypothesis (no effect)\nsim_null_hypothesis &lt;- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = null_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# =============================================================================\n# THEORETICAL DISTRIBUTION PARAMETERS\n# =============================================================================\n\n# Calculate standard error and degrees of freedom\nstandard_error &lt;- sqrt((1^2 + 1^2) / sample_size_per_group)\ndegrees_freedom &lt;- 2 * sample_size_per_group - 2\n\n# Distribution parameters for null and alternative hypotheses\nmu_null &lt;- null_effect_size / standard_error # Mean under H0\nsigma_null &lt;- 1 # SD under H0\nmu_alternative &lt;- alternative_effect_size / standard_error # Mean under HA\nsigma_alternative &lt;- 1 # SD under HA\n\n# Critical value for hypothesis test (two-tailed)\ncritical_value &lt;- qnorm(1 - alpha_level, mu_null, sigma_null)\n\n# =============================================================================\n# CREATE THEORETICAL DISTRIBUTION CURVES\n# =============================================================================\n\n# Define plot range (4 standard deviations from means)\nplot_range_multiplier &lt;- 4\nx_min_null &lt;- mu_null - sigma_null * plot_range_multiplier\nx_max_null &lt;- mu_null + sigma_null * plot_range_multiplier\nx_min_alt &lt;- mu_alternative - sigma_alternative * plot_range_multiplier\nx_max_alt &lt;- mu_alternative + sigma_alternative * plot_range_multiplier\n\n# Create x-axis sequence for plotting\nx_values &lt;- seq(min(x_min_null, x_min_alt), max(x_max_null, x_max_alt), 0.01)\n\n# Generate theoretical distributions\ndensity_null &lt;- dnorm(x_values, mu_null, sigma_null)\ndensity_alternative &lt;- dnorm(x_values, mu_alternative, sigma_alternative)\n\n# Create data frames for plotting\ndf_null_hypothesis &lt;- data.frame(x = x_values, y = density_null)\ndf_alternative_hypothesis &lt;- data.frame(x = x_values, y = density_alternative)\n\n# =============================================================================\n# CREATE POLYGONS FOR STATISTICAL REGIONS\n# =============================================================================\n\n# Alpha region polygon (Type I error)\nalpha_polygon_data &lt;- data.frame(\n  x = x_values,\n  y = pmin(density_null, density_alternative)\n)\nalpha_polygon_data &lt;- alpha_polygon_data[\n  alpha_polygon_data$x &gt;= critical_value,\n]\nalpha_polygon_data &lt;- rbind(\n  c(critical_value, 0),\n  c(critical_value, dnorm(critical_value, mu_null, sigma_null)),\n  alpha_polygon_data\n)\n\n# Beta region polygon (Type II error)\nbeta_polygon_data &lt;- df_alternative_hypothesis\nbeta_polygon_data &lt;- beta_polygon_data[beta_polygon_data$x &lt;= critical_value, ]\nbeta_polygon_data &lt;- rbind(beta_polygon_data, c(critical_value, 0))\n\n# Power region polygon (1 - beta)\npower_polygon_data &lt;- df_alternative_hypothesis\npower_polygon_data &lt;- power_polygon_data[\n  power_polygon_data$x &gt;= critical_value,\n]\npower_polygon_data &lt;- rbind(power_polygon_data, c(critical_value, 0))\n\n# =============================================================================\n# COMBINE POLYGONS FOR PLOTTING\n# =============================================================================\n\n# Add polygon identifiers\nalpha_polygon_data$region_type &lt;- \"alpha\"\nbeta_polygon_data$region_type &lt;- \"beta\"\npower_polygon_data$region_type &lt;- \"power\"\n\n# Combine all polygon data\nall_polygons &lt;- rbind(\n  alpha_polygon_data,\n  beta_polygon_data,\n  power_polygon_data\n)\n\n# Convert to factor with proper labels\nall_polygons$region_type &lt;- factor(\n  all_polygons$region_type,\n  levels = c(\"power\", \"beta\", \"alpha\"),\n  labels = c(\"power\", \"beta\", \"alpha\")\n)\n\n# =============================================================================\n# DEFINE COLOR PALETTE\n# =============================================================================\n\n# Color palette for the visualization\nPALETTE &lt;- list( # nolint: object_name_linter\n  # Hypothesis colors\n  hypothesis = c(\n    \"H0\" = \"black\",\n    \"HA\" = \"#981e0b\"\n  ),\n\n  # Statistical region colors\n  regions = c(\n    \"alpha\" = \"#0d6374\",\n    \"alpha2\" = \"#0d6374\",\n    \"beta\" = \"#be805e\",\n    \"power\" = \"#7cecee\"\n  ),\n\n  # Simulation curve colors\n  simulations = c(\n    \"null\" = \"green\",\n    \"alternative\" = \"red\"\n  )\n)\n\n# =============================================================================\n# CREATE POWER ANALYSIS VISUALIZATION\n# =============================================================================\n\npower_plot &lt;- ggplot(\n  all_polygons,\n  aes(x, y, fill = region_type, group = region_type)\n) +\n  # Add filled polygons for statistical regions\n  geom_polygon(\n    data = df_null_hypothesis,\n    aes(\n      x, y,\n      color = \"H0\",\n      group = NULL,\n      fill = NULL\n    ),\n    linetype = \"dotted\",\n    fill = \"gray80\",\n    linewidth = 1,\n    alpha = 0.5,\n    show.legend = FALSE\n  ) +\n  geom_polygon(show.legend = FALSE, alpha = 0.8) +\n  geom_line(\n    data = df_alternative_hypothesis,\n    aes(x, y, color = \"HA\", group = NULL, fill = NULL),\n    linewidth = 1,\n    linetype = \"dashed\",\n    color = \"gray60\",\n    inherit.aes = FALSE\n  ) +\n  # Add critical value line\n  geom_vline(\n    xintercept = critical_value, linewidth = 1,\n    linetype = \"dashed\"\n  ) +\n  # Add annotations\n  annotate(\n    \"segment\",\n    x = 0.2,\n    xend = 0.8,\n    y = 0.05,\n    yend = 0.02,\n    arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    label = \"alpha\",\n    x = 0, y = 0.05,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"segment\",\n    x = 3,\n    xend = 2,\n    y = 0.18,\n    yend = 0.1,\n    arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    label = \"Power\",\n    x = 3, y = 0.2,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"text\",\n    label = \"H[0]\",\n    x = mu_null,\n    y = dnorm(mu_null, mu_null, sigma_null),\n    vjust = -0.5,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"text\",\n    label = \"H[a]\",\n    x = mu_alternative,\n    y = dnorm(mu_null, mu_null, sigma_null),\n    vjust = -0.5,\n    parse = TRUE, size = 8\n  ) +\n  # Customize colors and styling\n  scale_color_manual(\n    \"Hypothesis\",\n    values = PALETTE$hypothesis\n  ) +\n  scale_fill_manual(\n    \"Statistical Region\",\n    values = PALETTE$regions\n  ) +\n  labs(\n    x = \"Test statistic (z)\",\n    y = \"Density\",\n    title = \"Statistical Power Analysis Visualization\",\n    subtitle = \"Standard Non-Inferiority Hypothesis Test (normal approximation)\"\n  ) +\n  ylim(c(0, max(dnorm(mu_null, mu_null, sigma_null) * 1.1))) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\npower_plot\n\n\n\n\n\n\n\n\nFigure 7: Non-inferiority conceptual plot: one-sided alpha region and power relative to the NI margin.\n\n\n\n\n\nBecause non-inferiority can also be assessed via confidence intervals, we visualize the CI criterion directly: success occurs when the lower bound of the confidence interval lies above the negative margin. This plot makes it clear why detecting non-inferiority requires careful consideration of the margin and effect size.\n\n# Create ni_sim with CI significance for non-inferiority\nni_sim &lt;- sim_alternative_hypothesis\nni_sim$ci_significant &lt;- ni_sim$ci_lower &gt; -0.2\n\nni_sim |&gt;\n  sample_n(200) |&gt;\n  plot_ci_coverage(\n    true_effect = 0.1,\n    null_effect = -0.2,\n    title = \"Non-inferiority\"\n  )\n\n\n\n\n\n\n\nFigure 8: Non-inferiority CI visualization: green intervals exclude values worse than the margin.\n\n\n\n\n\nFinally, we compare empirical power from simulation with the theoretical power. They should agree up to simulation error, which validates both our code and our understanding of the test.\n\n# Calculate theoretical power\ntheoretical_power &lt;- pwr.t.test(\n  d = 0.3, # margin of 0.2 + 0.1 true effect\n  n = 80,\n  sig.level = 0.05,\n  type = \"two.sample\",\n  alternative = \"greater\"\n)\n\nni_sim |&gt;\n  summarize(\n    power_ci = mean(ci_significant),\n    power_theoretical = theoretical_power$power\n  ) |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"method\",\n    values_to = \"power\"\n  ) |&gt;\n  gt() |&gt;\n  fmt_percent(\n    columns = \"power\",\n    decimals = 0\n  )\n\n\n\nTable 2: Comparison of empirical power from simulation with the theoretical power for non-inferiority test.\n\n\n\n\n\n\n\n\n\nmethod\npower\n\n\n\n\npower_ci\n58%\n\n\npower_theoretical\n60%\n\n\n\n\n\n\n\n\n\n\nNote that non-inferiority trials often require larger sample sizes than superiority trials because we’re trying to rule out a smaller difference.",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/power-sample-size.html#equivalence-trials",
    "href": "labs/power-sample-size.html#equivalence-trials",
    "title": "Power Analysis and Sample Size Planning",
    "section": "Equivalence trials",
    "text": "Equivalence trials\nEquivalence trials ask the most stringent question: “Are these two treatments essentially equally effective?” Unlike non-inferiority (which only rules out being worse), equivalence must rule out being either significantly better or significantly worse.\nWhen equivalence matters: Equivalence trials are useful when you want to demonstrate that the there is no relevant difference between two treatments. For example, demonstrating that psychodynamic therapy is equivalent to cognitive-behavioral therapy.\n\\[\n\\begin{aligned}\n&\\text{Null hypothesis } (H_0): && d \\leq -\\Delta \\quad \\text{or} \\quad d \\geq \\Delta \\\\\n&\\text{Alternative hypothesis } (H_1): && -\\Delta &lt; d &lt; \\Delta\n\\end{aligned}\n\\]\nwhere \\(\\Delta\\) is the equivalence margin and \\(d\\) is the effect size.\nWe will: (1) build intuition with a conceptual TOST figure, (2) check that simulated study statistics line up with the theory, (3) view confidence-interval criteria directly, and (4) compare empirical power with analytical TOST power.\n\n\n\n\n\n\nNoteEquivalence vs Non-inferiority\n\n\n\n\nNon-inferiority: “New treatment is not worse than standard (within margin)”\nEquivalence: “New treatment is similar to standard (within ± margin)”\n\nEquivalence is more stringent - we need to rule out both directions of difference.\n\n\nFor equivalence trials, we need to calculate power for both one-sided tests. The figure below shows the two one-sided critical regions (at ±margin, translated to the test-statistic scale) and the central region where we have power to conclude equivalence.\n\n\nCode\n# =============================================================================\n# SIMULATION PARAMETERS\n# =============================================================================\n\n# Study design parameters\nsample_size_per_group &lt;- 500\nnum_simulations &lt;- 10000\nconfidence_level &lt;- 0.9\nalpha_level &lt;- 0.05\n\n# Effect size parameters\nequivalence_bound &lt;- 0.2\nnull_effect_size &lt;- 0\nalternative_effect_size &lt;- 0.2\n\n# =============================================================================\n# RUN POWER SIMULATIONS\n# =============================================================================\n\n# Simulation under alternative hypothesis (effect within equivalence bounds)\nsim_alternative_hypothesis &lt;- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = null_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# Simulation under first null hypothesis (effect at positive bound)\nsim_null_hypothesis_pos &lt;- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = alternative_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# Simulation under second null hypothesis (effect at negative bound)\nsim_null_hypothesis_neg &lt;- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = -alternative_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# =============================================================================\n# THEORETICAL DISTRIBUTION PARAMETERS\n# =============================================================================\n\n# Calculate standard error and degrees of freedom\nstandard_error &lt;- sqrt(2 / sample_size_per_group)\ndegrees_freedom &lt;- 2 * sample_size_per_group - 2\n\n# Distribution parameters for null and alternative hypotheses\nmu_null &lt;- -equivalence_bound / standard_error # Mean under H0\nsigma_null &lt;- 1 # SD under H0\nmu_alternative &lt;- 0 # Mean under HA\nsigma_alternative &lt;- 1 # SD under HA\n\n# Critical value for hypothesis test\ncritical_value &lt;- qnorm(1 - alpha_level, mu_null, sigma_null)\n\n# =============================================================================\n# CREATE THEORETICAL DISTRIBUTION CURVES\n# =============================================================================\n\n# Define plot range (4 standard deviations from means)\nplot_range_multiplier &lt;- 4\nx_min_null &lt;- mu_null - sigma_null * plot_range_multiplier\nx_max_null &lt;- mu_null + sigma_null * plot_range_multiplier\nx_min_alt &lt;- abs(mu_null) - sigma_alternative * plot_range_multiplier\nx_max_alt &lt;- abs(mu_null) + sigma_alternative * plot_range_multiplier\n\n# Create x-axis sequence for plotting\nx_values &lt;- seq(\n  min(x_min_null, x_min_alt),\n  max(x_max_null, x_max_alt),\n  0.01\n)\n\n# Generate theoretical distributions\ndensity_null_neg &lt;- dnorm(x_values, mu_null, sigma_null)\ndensity_null_pos &lt;- dnorm(\n  x_values,\n  alternative_effect_size / standard_error,\n  sigma_null\n)\ndensity_alternative &lt;- dnorm(x_values, mu_alternative, sigma_alternative)\n\n# Create data frames for plotting\ndf_null_negative &lt;- data.frame(x = x_values, y = density_null_neg)\ndf_null_positive &lt;- data.frame(x = x_values, y = density_null_pos)\ndf_alternative &lt;- data.frame(x = x_values, y = density_alternative)\n\n# =============================================================================\n# CREATE POLYGONS FOR STATISTICAL REGIONS\n# =============================================================================\n\n# Alpha region polygon (Type I error)\nalpha_polygon_data &lt;- data.frame(\n  x = x_values,\n  y = pmin(density_null_neg, density_alternative)\n)\nalpha_polygon_data &lt;- alpha_polygon_data[\n  alpha_polygon_data$x &gt;= critical_value,\n]\nalpha_polygon_data &lt;- rbind(alpha_polygon_data, c(critical_value, 0))\n\n# Beta region polygon (Type II error)\nbeta_polygon_data &lt;- df_alternative\nbeta_polygon_data &lt;- beta_polygon_data[beta_polygon_data$x &lt;= critical_value, ]\nbeta_polygon_data &lt;- rbind(beta_polygon_data, c(critical_value, 0))\n\n# Power region polygon (1 - beta)\npower_polygon_data &lt;- df_alternative\npower_polygon_data &lt;- power_polygon_data[\n  power_polygon_data$x &gt;= critical_value &\n    power_polygon_data$x &lt;= abs(critical_value),\n]\npower_polygon_data &lt;- rbind(\n  c(critical_value, 0),\n  c(critical_value, dnorm(critical_value, mu_alternative, sigma_alternative)),\n  power_polygon_data,\n  c(\n    abs(critical_value),\n    dnorm(abs(critical_value), mu_alternative, sigma_alternative)\n  ),\n  c(abs(critical_value), 0)\n)\n\n# =============================================================================\n# COMBINE POLYGONS FOR PLOTTING\n# =============================================================================\n\n# Add polygon identifiers\nalpha_polygon_data$region_type &lt;- \"alpha\"\nbeta_polygon_data$region_type &lt;- \"beta\"\npower_polygon_data$region_type &lt;- \"power\"\n\n# Create second alpha region (mirrored)\nalpha_polygon_mirrored &lt;- alpha_polygon_data |&gt;\n  mutate(x = -1 * x, region_type = \"alpha_mirrored\")\n\nbeta_polygon_data_mirrored &lt;- beta_polygon_data |&gt;\n  mutate(x = -1 * x, region_type = \"beta_mirrored\")\n\n# Combine all polygon data\nall_polygons &lt;- rbind(\n  alpha_polygon_data,\n  alpha_polygon_mirrored,\n  beta_polygon_data,\n  beta_polygon_data_mirrored,\n  power_polygon_data\n)\n\n# Convert to factor with proper labels\nall_polygons$region_type &lt;- factor(\n  all_polygons$region_type,\n  levels = c(\"power\", \"beta\", \"beta_mirrored\", \"alpha\", \"alpha_mirrored\"),\n  labels = c(\"power\", \"beta\", \"beta2\", \"alpha\", \"alpha2\")\n)\n\n# =============================================================================\n# DEFINE COLOR PALETTE\n# =============================================================================\n\n# Color palette for the visualization\nPALETTE &lt;- list( # nolint: object_name_linter\n  # Hypothesis colors\n  hypothesis = c(\n    \"H0\" = \"gray60\",\n    \"HA\" = \"#437677\"\n  ),\n\n  # Statistical region colors\n  regions = c(\n    \"alpha\" = \"#0d6374\",\n    \"alpha2\" = \"#0d6374\",\n    \"beta\" = \"#be805e\",\n    \"beta2\" = \"#be805e\",\n    \"power\" = \"#529d9e\"\n  ),\n\n  # Simulation curve colors\n  simulations = c(\n    \"null\" = \"green\", # TOST null hypotheses (effect outside bounds)\n    \"alternative\" = \"#7cecee\" # TOST alternative hypothesis\n  )\n)\n\n# =============================================================================\n# CREATE POWER ANALYSIS VISUALIZATION\n# =============================================================================\n\npower_plot_eq &lt;- ggplot(\n  all_polygons,\n  aes(x, y, fill = region_type, group = region_type)\n) +\n  # Add theoretical distribution curves\n  geom_polygon(\n    data = df_null_negative,\n    aes(\n      x, y,\n      color = \"H0\",\n      group = NULL,\n      fill = NULL\n    ),\n    linetype = \"dotted\",\n    fill = \"gray80\",\n    linewidth = 1,\n    alpha = 0.5,\n    show.legend = FALSE\n  ) +\n  geom_polygon(\n    data = df_null_positive,\n    aes(\n      x, y,\n      color = \"H0\",\n      group = NULL,\n      fill = NULL\n    ),\n    linetype = \"dotted\",\n    fill = \"gray80\",\n    alpha = 0.5,\n    linewidth = 1,\n    show.legend = FALSE\n  ) +\n  # Add filled polygons for statistical regions\n  geom_polygon(\n    show.legend = FALSE,\n    alpha = 0.8\n  ) +\n  geom_line(\n    data = df_alternative,\n    aes(\n      x = x, y = y,\n      color = \"HA\",\n      group = NULL,\n      fill = NULL\n    ),\n    linewidth = 1,\n    linetype = \"dashed\",\n    color = \"gray60\",\n    inherit.aes = FALSE\n  ) +\n\n  # Add critical value lines\n  geom_vline(\n    xintercept = critical_value,\n    linewidth = 1,\n    linetype = \"dashed\"\n  ) +\n  geom_vline(\n    xintercept = -critical_value,\n    linewidth = 1,\n    linetype = \"dashed\"\n  ) +\n\n  # Customize colors and styling\n  scale_color_manual(\n    \"Hypothesis\",\n    values = PALETTE$hypothesis\n  ) +\n  scale_fill_manual(\n    \"Statistical Region\",\n    values = PALETTE$regions\n  ) +\n  labs(\n    x = \"Test statistic (z)\",\n    y = \"Density\",\n    title = \"Statistical Power Analysis Visualization\",\n    subtitle = \"TOST Equivalence Test (normal approximation)\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\n# Display the plot\npower_plot_eq\n\n\n\n\n\n\n\n\nFigure 9: Equivalence (TOST) conceptual plot: two one-sided critical regions and central power region within ±margin.\n\n\n\n\n\nNext, we verify that simulated study results behave as expected under the equivalence setup: when the true effect lies within ±margin, most statistics fall between the two critical values; when the true effect equals a margin (the nulls), they fall near the boundaries.\n\n\nCode\n# Add empirical density curves from simulations\npower_plot_eq +\n  # Add empirical histograms from simulations\n  geom_histogram(\n    data = sim_null_hypothesis_pos,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"gray30\",\n    alpha = 0.2,\n    color = \"gray30\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  ) +\n  geom_histogram(\n    data = sim_null_hypothesis_neg,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"gray30\",\n    alpha = 0.2,\n    color = \"gray30\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  ) +\n  geom_histogram(\n    data = sim_alternative_hypothesis,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"#27AE60\",\n    alpha = 0.2,\n    color = \"#1E8449\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  )\n\n\n\n\n\n\n\n\nFigure 10: Empirical histograms for equivalence simulations (null bounds and alternative within bounds).\n\n\n\n\n\nBecause equivalence can also be assessed via confidence intervals, we visualize the CI criterion directly: success occurs when the entire CI lies strictly within ±margin. This plot makes it clear why equivalence demands more precision (n) than superiority for the same α.\n\n\nCode\nsim_alternative_hypothesis |&gt;\n  mutate(\n    ci_significant = ci_lower &gt; -equivalence_bound &\n      ci_upper &lt; equivalence_bound\n  ) |&gt;\n  sample_n(200) |&gt;\n  plot_ci_coverage(\n    true_effect = 0,\n    null_effect = -equivalence_bound,\n    title = \"Equivalence Trial\"\n  ) +\n  geom_vline(xintercept = equivalence_bound, linewidth = 1)\n\n\n\n\n\n\n\n\nFigure 11: Equivalence CI visualization: success when the entire 90% CI lies within ±margin.\n\n\n\n\n\nFinally, we compare empirical power from simulation with the theoretical TOST power. They should agree up to simulation error, which validates both our code and our understanding of the test.\n\n# For equivalence with margin ±0.2, we use the TOSTER package\n# But we can also calculate manually\nlibrary(TOSTER)\n\n# Theoretical power calculation using TOST\ntheoretical_power &lt;- power_t_TOST(\n  n = sample_size_per_group,\n  low_eqbound = -equivalence_bound,\n  high_eqbound = equivalence_bound,\n  alpha = alpha_level\n)\n\n# Calculate empirical power from simulation\nsim_alternative_hypothesis |&gt;\n  summarize(\n    power_tost = mean(t_value &gt; critical_value & t_value &lt; abs(critical_value)),\n    power_ci = mean(\n      ci_lower &gt; -equivalence_bound & ci_upper &lt; equivalence_bound\n    ),\n    power_theoretical = theoretical_power$power\n  ) |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"method\",\n    values_to = \"power\"\n  ) |&gt;\n  gt() |&gt;\n  fmt_percent(\n    columns = \"power\",\n    decimals = 0\n  )\n\n\n\nTable 3: Comparison of empirical power from simulation with the theoretical TOST power.\n\n\n\n\n\n\n\n\n\nmethod\npower\n\n\n\n\npower_tost\n87%\n\n\npower_ci\n87%\n\n\npower_theoretical\n87%",
    "crumbs": [
      "Labs",
      "Power and sample size"
    ]
  },
  {
    "objectID": "labs/p-values-ci.html",
    "href": "labs/p-values-ci.html",
    "title": "P-values and confidence intervals",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\ndf_data &lt;- read_rds(here(\"data\", \"steps_baseline.rds\"))",
    "crumbs": [
      "Labs",
      "P-values and confidence intervals"
    ]
  },
  {
    "objectID": "labs/p-values-ci.html#numeric-variables",
    "href": "labs/p-values-ci.html#numeric-variables",
    "title": "P-values and confidence intervals",
    "section": "Numeric variables",
    "text": "Numeric variables\nFor instance we might have an hypothesis the the mean of LSAS-SR is 82 in the population. We can determine how probable our data would be under this hypothesis, by investigating how often we would expect to get our observed sample mean if this (null)hypothesis was true.\nFor this we would need the sampling distribution of mean LSAS-SR scores in samples of 181 people (the size, \\(n\\), of our sample), if the true population mean was 82. One way to get this would be to simulate say 10 000 samples of LSAS-SR scores, from a population with a true mean of 82. To simulate this, we also need to know the spread (standard deviation) of the true population. We don’t know this, but let’s assume it is the same as in our sample.\nBelow, the function rnorm() is used to take a random sample of 181 values from a normal distribution with a mean of 82 and a standard deviation 16.5 (same as in out sample). We use a for loop to repeat this sampling 10 000 times and save the mean values of each sample in the vector called means.\n\nn_samples &lt;- 1e4 # the number of samples\nsmp_size &lt;- 181 # the size of our samples\nmeans &lt;- rep(NA, n_samples) # an empty vector to contain our mean values\n\nfor (i in 1:n_samples) {\n  x &lt;- rnorm(smp_size, mean = 82, sd = sd(df_data$lsas_screen))\n  means[i] &lt;- mean(x)\n}\n\nhist(means, main = \"Simulated sampling distribution of LSAS means\")\n\n\n\n\n\n\n\n\nWe can use this simulated sampling distribution to see how probable our observed LSAS-SR mean is if the (null)hypothesis that the true mean is 82 would be correct. First let plot the sampling distribution again, and show the observed LSAS-SR mean as a vertical line.\n\nhist(means, main = \"Simulated sampling distribution of LSAS means\")\nabline(v = mean(df_data$lsas_screen), col = \"red\", lwd = 2, lty = 2) # vertical line showing the observed LSAS-SR mean\n\n\n\n\n\n\n\n\nWe can also quantify the probability by calculating, the proportion of times that a sample mean would be equal to or greater that our observed mean, IF the true population mean was 82. This quantity is the very (in)famous p-value.\n\nmean(means &gt;= mean(df_data$lsas_screen)) # proportion of simulated means that are larger than our observed mean\n\n[1] 0.0115\n\n\nIf we find this simulation exercise a bit tedious, we could also use theoretical distributions for the sample means to calculate our p-value. The t-distribution can be used to estimate the spread to the sample means when the population variance is unknown and the sample variance is used to approximate it. It is very similar to the normal distribution, but has heavier tails that accounts for the uncertainty produced by using the sample variance instead of the true population variance when estimating the standard error of the sampling distribution. However, when the sample size increase, the t-distribution will come closer and closer to a normal distribution (also known as a z-distribution when standardized to have mean=0 and sd=1).\n\n# Set up the plot range\nx_range &lt;- seq(-4, 4, length = 500)\n\n# Plot standard normal distribution\nplot(x_range, dnorm(x_range),\n  type = \"l\", lwd = 2, col = \"black\",\n  ylab = \"Density\", xlab = \"x\", main = \"t-Distribution vs Normal Distribution\"\n)\n\n# Add t-distributions with different degrees of freedom\nlines(x_range, dt(x_range, df = 1), col = \"red\", lwd = 2, lty = 2)\nlines(x_range, dt(x_range, df = 5), col = \"blue\", lwd = 2, lty = 3)\nlines(x_range, dt(x_range, df = 15), col = \"darkgreen\", lwd = 2, lty = 4)\nlines(x_range, dt(x_range, df = 30), col = \"purple\", lwd = 2, lty = 5)\n\n# Add a legend\nlegend(\"topright\",\n  legend = c(\"Normal (Z)\", \"t (df=1)\", \"t (df=5)\", \"t (df=15)\", \"t (df=30)\"),\n  col = c(\"black\", \"red\", \"blue\", \"darkgreen\", \"purple\"),\n  lwd = 2, lty = 1:5, bty = \"n\"\n)\n\n\n\n\n\n\n\n\nThe probability of getting a a sample mean that is greater or equal to our observed mean, given some true population mean, \\(\\mu\\), can be calculated by transforming our observed mean to a t-value and compare it to the t-distribution.\nThe t-value of our mean \\(\\bar{x}\\) under the null hypothesis that the population mean is \\(\\mu\\), is given by the formula:\n\\[\nt = \\frac{\\bar{x} - \\mu}{SE}\n\\]\nReplacing these Greek letters with our actual values \\(\\bar{x} = 84.75\\), \\(\\mu = 82\\), and \\(SE=1.22\\), we get:\n\nse &lt;- sd(df_data$lsas_screen) / sqrt(nrow(df_data))\nx_bar &lt;- mean(df_data$lsas_screen)\nt_value &lt;- (x_bar - 82) / se\nt_value\n\n[1] 2.24774\n\n\nNow let’s find the probability of getting a t-value larger or equal to this - our one-sided p-value! For this we use the pt() function, that provides the cumulative probability up until a given t-value. 1 minus this cumulative probability gives the probability of values equal or above the given t-value.\n\n1 - pt(t_value, df = 180)\n\n[1] 0.01290324\n\n\n\n\n\n\n\n\nNotez-values and t-values\n\n\n\nWhen the sample size increases, the t-distribution approaches the z-distribution and these estimates become very similar. As a general rule of thumb, it is fine to use z-values rather than t-values for sample sizes larger than 200.\n\n\nWe could also get a very similar p-value from the z-distribution (although we would assume we know the population variance for out calculation of the standard error). The pnorm() function gives the cumulative probability up to some specific value for the normal distribution, which when the mean is 0 and the SD is 1 is also called the z-distribution.\n\n1 - pnorm(t_value)\n\n[1] 0.01229639\n\n\nSinnce our sample is quite large, the p-value from the t-distribution and the z-distribution are very similar. Also very similar to our simulated p-value above! More conveniently, of course, we could get this p-value using the t.test() function.\n\nt.test(df_data$lsas_screen, mu = 82, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  df_data$lsas_screen\nt = 2.2477, df = 180, p-value = 0.0129\nalternative hypothesis: true mean is greater than 82\n95 percent confidence interval:\n 82.72756      Inf\nsample estimates:\nmean of x \n 84.75138 \n\n\n\n\n\n\n\n\nNoteOne-sided and two-sided p-values\n\n\n\nWhat we have now calculated in three different ways is the one-sided p-value. It’s one-sided since we only looked at the probability to get data equal to or greater than our observed data, given that the null-hypothesis was true.\nIf we wanted to see the probability of getting data equal or greater than our observed data OR equal or less than out observed data under the null-hypothesis, we would want a two-sided p-value. Since the sampling distribution is symmetrical, we could get this by multiplying of one-sided p-value by 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode for two-sided p-values\n\n# manual code\nse &lt;- sd(df_data$lsas_screen) / sqrt(nrow(df_data))\nx_bar &lt;- mean(df_data$lsas_screen)\nt_value &lt;- (x_bar - 82) / se\n(1 - pt(t_value, df = 180)) * 2\n\n[1] 0.02580648\n\n# or using the t-test function\nt.test(df_data$lsas_screen, mu = 82, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  df_data$lsas_screen\nt = 2.2477, df = 180, p-value = 0.02581\nalternative hypothesis: true mean is not equal to 82\n95 percent confidence interval:\n 82.33602 87.16675\nsample estimates:\nmean of x \n 84.75138",
    "crumbs": [
      "Labs",
      "P-values and confidence intervals"
    ]
  },
  {
    "objectID": "labs/p-values-ci.html#proportions",
    "href": "labs/p-values-ci.html#proportions",
    "title": "P-values and confidence intervals",
    "section": "Proportions",
    "text": "Proportions\nThis same logic applies to proportions, but we can’t use the t.test() function anymore. Instead we can use the prop.test() function.\nWe have no categorical variables in the STePs data, so let’s simulate a gender variable:\n\n# Simulating a gender variable\nn &lt;- nrow(df_data)\ndf_data$gender &lt;- rbinom(n, 1, 0.7)\ndf_data$gender &lt;- ifelse(df_data$gender == 1, \"Woman\", \"Man\")\n\nWe can the use the prop.test() function to get a one-sided or two sided p-value, given some assumed population proportion, say that 32% are men.\n\n#one-sided p-value\nprop.test(table(df_data$gender),\n          p=0.32,\n          alternative = \"less\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  table(df_data$gender), null probability 0.32\nX-squared = 0.32541, df = 1, p-value = 0.7158\nalternative hypothesis: true p is less than 0.32\n95 percent confidence interval:\n 0.0000000 0.4053286\nsample estimates:\n        p \n0.3425414 \n\n#two-sided p-value\n#one-sided p-value\nprop.test(table(df_data$gender),\n          p=0.32,\n          alternative = \"two.sided\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  table(df_data$gender), null probability 0.32\nX-squared = 0.32541, df = 1, p-value = 0.5684\nalternative hypothesis: true p is not equal to 0.32\n95 percent confidence interval:\n 0.2747480 0.4171381\nsample estimates:\n        p \n0.3425414",
    "crumbs": [
      "Labs",
      "P-values and confidence intervals"
    ]
  },
  {
    "objectID": "labs/p-values-ci.html#numeric-variables-1",
    "href": "labs/p-values-ci.html#numeric-variables-1",
    "title": "P-values and confidence intervals",
    "section": "Numeric variables",
    "text": "Numeric variables\nWhen the sample variance, \\(s\\), is used, we get the confidence intervals by taking the observed mean and adding or subtracting the t-value of the desired percentiles of the sampling distribution (indicated by the asterix) times the standard error, \\(\\frac{s}{\\sqrt{n}}\\)\n\\[ \\text{CI} = \\bar{x} \\pm t^*\\ \\frac{s}{\\sqrt{n}}\\]\nIf we knew the variance of the population, we could substitute the sample variance \\(s\\) for the population variance \\(\\sigma\\), and use z-values instead of t-values. For 95% confidence intervals, the z-value is 1.96.\n\\[\n\\text{CI} = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\n\\]\nNow let’s use these formulas to calculate the confidence interval of the mean of LSAS-SR\n\nt_value &lt;- qt(1 - 0.025, 180) # the t-value for a 95% confidence interval with 180 degrees of freedom\n\nse &lt;- sd(df_data$lsas_screen) / sqrt(nrow(df_data)) # standard error of LSAS-SR\n\nucl &lt;- mean(df_data$lsas_screen) + t_value * se # the upper confidence limit\nlcl &lt;- mean(df_data$lsas_screen) - t_value * se # the lower confidence limit\n\nprint(c(lcl, ucl))\n\n[1] 82.33602 87.16675\n\n\nWe can also use the t.test() function to get this interval\n\nt.test(df_data$lsas_screen, conf.level = 0.95) # For a 95% CI\n\n\n    One Sample t-test\n\ndata:  df_data$lsas_screen\nt = 69.238, df = 180, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 82.33602 87.16675\nsample estimates:\nmean of x \n 84.75138 \n\n\nLet’s also see what happens if we use z-values (for 95% confidence intervals, the z-value is approx 1.96)\n\nse &lt;- sd(df_data$lsas_screen) / sqrt(nrow(df_data)) # standard error of LSAS-SR\n\nucl &lt;- mean(df_data$lsas_screen) + 1.96 * se\nlcl &lt;- mean(df_data$lsas_screen) - 1.96 * se\n\nprint(c(lcl, ucl))\n\n[1] 82.35221 87.15055",
    "crumbs": [
      "Labs",
      "P-values and confidence intervals"
    ]
  },
  {
    "objectID": "labs/p-values-ci.html#proportions-1",
    "href": "labs/p-values-ci.html#proportions-1",
    "title": "P-values and confidence intervals",
    "section": "Proportions",
    "text": "Proportions\nThe logic of confidence intervals are the same for proportions, we take our observed value \\(\\pm\\) z times the standard error\nFor a proportion, the confidence intervals therefore becomes:\n\\[ \\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\]\n\n\n\n\n\n\nTipConfidence intervals for proportions\n\n\n\n\n\nThis confidence intervals for proportion, known as Wald confidence intervals, is easy to compute. However, since it uses the sample proportion to estimate the population proportion, they can be erratic, especially when \\(\\hat{p}\\) approach 0 or 1. We therefore recommend using more advanced confidence intervals, calculated by statistical software, for instance using the function prop.test().\n\n\n\nIf we want to get a confidence interval for a proportion in R, we can get this using the prop.test() function.\n\nprop.test(table(df_data$gender))\n\n\n    1-sample proportions test with continuity correction\n\ndata:  table(df_data$gender), null probability 0.5\nX-squared = 17.326, df = 1, p-value = 3.149e-05\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.2747480 0.4171381\nsample estimates:\n        p \n0.3425414",
    "crumbs": [
      "Labs",
      "P-values and confidence intervals"
    ]
  },
  {
    "objectID": "labs/testing-two-means.html",
    "href": "labs/testing-two-means.html",
    "title": "Testing two means and contingency tables",
    "section": "",
    "text": "In this chapter we focus on testing differences between groups. Previously we have calculated p-values and confidence intervals for one-sample means and proportions. In psychiatric research, we are more often interested in comparing means between groups. For instance, the post-treatment symptom levels between the treatment group and the control group.\nIn the STePS study, a primary comparison was the severity of social anxiety symptoms post-treatment between the self-guided and the therapist-guided treatment groups. As a first check, we can load the data and calculate get some descriptive statistics for the post-treatment LSAS-scores across the treatment groups.",
    "crumbs": [
      "Labs",
      "Testing two means"
    ]
  },
  {
    "objectID": "labs/testing-two-means.html#testing-dependent-means",
    "href": "labs/testing-two-means.html#testing-dependent-means",
    "title": "Testing two means and contingency tables",
    "section": "Testing dependent means",
    "text": "Testing dependent means\nThe independent group t-test assumes that the groups compared are independent, which is the case for the two treatment groups. Sometimes, however, we want to compare the means of dependent groups. This could be for instance the difference in means between two time-point of the same group. Then we can use a paired-sample t-test.\nFor a paired samples t-test, the statistic is:\n\\[\nt = \\frac{\\bar{D}}{SE_D}\n\\]\nwhere:\n\n\\(\\bar{D}\\) = mean of the difference scores\n\\(SE_D\\) = the standard error of the difference scores calculated as \\(s_D/ \\sqrt{n}\\) = the standard error of the differences, with \\(s_D\\) = the standard deviation of the differences and \\(n\\) = the number of paired observations\n\nLet’s use this to formula to test the difference in LSAS-scores from pre-to post-treatment.\n\ndiff_pre_post &lt;- df_data$lsas_post - df_data$lsas_screen\nmean_diff &lt;- mean(diff_pre_post, na.rm = TRUE)\nsd_diff &lt;- sd(diff_pre_post, na.rm = TRUE)\nn &lt;- sum(!is.na(diff_pre_post))\nse_diff &lt;- sd_diff / sqrt(n)\nt_value &lt;- mean_diff / se_diff\nt_value\n\n[1] -11.07385\n\n\nAnd get the two-tailed p-value from this as before\n\ndf &lt;- n - 1\np_value &lt;- 2 * (1 - pt(abs(t_value), df))\np_value\n\n[1] 0\n\n\nOr using the t.test() function with the the argument paired = TRUE.\n\nt.test(df_data$lsas_post, df_data$lsas_screen, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  df_data$lsas_post and df_data$lsas_screen\nt = -11.074, df = 168, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -21.05556 -14.68409\nsample estimates:\nmean difference \n      -17.86982 \n\n\nWe can also get the confidence intervals of this difference, using a familiar formula for large samples\n\\[\n\\bar{D} \\;\\pm\\; z_{\\alpha/2} \\cdot SE_D\n\\]\nFor smaller samples, we need use the t-distribution and find the right t-value for our degrees of freedom and coverage interval.\n\\[\nCI = \\bar{D} \\;\\pm\\; t_{\\alpha/2, \\, df} \\cdot SE_D\n\\]\nAnd again, \\(SE_D = {s_D}/ {\\sqrt{n}}\\)\n\ndiff_pre_post &lt;- df_data$lsas_post - df_data$lsas_screen\nmean_diff &lt;- mean(diff_pre_post, na.rm = TRUE)\nsd_diff &lt;- sd(diff_pre_post, na.rm = TRUE)\nn &lt;- sum(!is.na(diff_pre_post))\nse_diff &lt;- (sd_diff / sqrt(n)) # saving the standard error of the difference as a separate object for convenience\n\n# and putting it together\nlcl &lt;- mean_diff - 1.96 * se_diff\nucl &lt;- mean_diff + 1.96 * se_diff\nprint(c(lcl, ucl))\n\n[1] -21.03267 -14.70698\n\n\nor manually using the t-values\n\ndf &lt;- n - 1\nalpha &lt;- 0.05\nt_crit &lt;- qt(1 - alpha / 2, df)\nlcl &lt;- mean_diff - t_crit * se_diff\nucl &lt;- mean_diff + t_crit * se_diff\nprint(c(lcl, ucl))\n\n[1] -21.05556 -14.68409\n\n\nOr again, more conveniently using the t.test() function\n\nt.test(df_data$lsas_post, df_data$lsas_screen, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  df_data$lsas_post and df_data$lsas_screen\nt = -11.074, df = 168, p-value &lt; 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -21.05556 -14.68409\nsample estimates:\nmean difference \n      -17.86982",
    "crumbs": [
      "Labs",
      "Testing two means"
    ]
  },
  {
    "objectID": "labs/testing-two-means.html#contingency-tables-and-the-chi-squared-test",
    "href": "labs/testing-two-means.html#contingency-tables-and-the-chi-squared-test",
    "title": "Testing two means and contingency tables",
    "section": "Contingency tables and the Chi-squared test",
    "text": "Contingency tables and the Chi-squared test\nSay that we wanted to test the distribution of some categorical variable across two groups, the the methods presented above will not help. For this we need other tests.\nSay that we wanted to know if the gender distribution was similar across in the self-guided as in the therapist-guided treatment group of the the STEpS study. The first step to such an investigation would be to create a contingency table showing the gender distribution across the groups.\n\n# create a gender variable\ndf_data$gender &lt;- rbinom(nrow(df_data), 1, 0.7)\ndf_data$gender &lt;- ifelse(df_data$gender == 1, \"Woman\", \"Man\")\n\n\ntable(df_data$trt, df_data$gender) # number of person\n\n                  \n                   Man Woman\n  self-guided       15    46\n  therapist-guided  17    43\n  waitlist          15    45\n\nprop.table(table(df_data$trt, df_data$gender), margin = 1) # proportion in each treatment group\n\n                  \n                         Man     Woman\n  self-guided      0.2459016 0.7540984\n  therapist-guided 0.2833333 0.7166667\n  waitlist         0.2500000 0.7500000\n\n\nWe see that there are some small differences between the groups, but we have no statistical test of these differences. Say that we wanted to test the null hypothesis that there is no association between gender and the treatment group you end up in. This hypothesis should hold, due to the randomization of the STEpS study.\nOne way to test if the proportion of men and women differs between the groups is perform a Chi-squared test (\\(X^2-test\\)).\nThe formula for Chi-squared test statistic is:\n\\[\n\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\nwhere\n\\[\nE_{ij} = \\frac{(\\text{row total})_i \\cdot (\\text{column total})_j}{\\text{grand total}}\n\\]\nThis test compares the observed counts in each cell against what would be expected if values where distributed equally across the groups (i.e. across the cells of the contingency table). Coding this formula manually is a bit tricky, but here is the code for those interested.\n\n## Build the contingency table\ntbl &lt;- table(trt = df_data$trt, gender = df_data$gender)\n\n## Convert this table to matrix format for the following calculations\nO &lt;- as.matrix(tbl)\nO\n\n                  gender\ntrt                Man Woman\n  self-guided       15    46\n  therapist-guided  17    43\n  waitlist          15    45\n\n## Expected counts E_ij = (row total_i * col total_j) / grand total\nrow_tot &lt;- rowSums(O)\ncol_tot &lt;- colSums(O)\ngrand &lt;- sum(O)\nE &lt;- outer(row_tot, col_tot) / grand # outer is a function to multiply arayys\nE\n\n                      Man    Woman\nself-guided      15.83978 45.16022\ntherapist-guided 15.58011 44.41989\nwaitlist         15.58011 44.41989\n\n## Chi-square statistic: sum_{i,j} (O_ij - E_ij)^2 / E_ij\nchi_sq &lt;- sum((O - E)^2 / E)\n\n## Degrees of freedom: (r - 1)(c - 1)\nr &lt;- nrow(O)\nc &lt;- ncol(O)\ndf &lt;- (r - 1) * (c - 1)\n\n## p-value from chi-square distribution\np_value &lt;- pchisq(chi_sq, df = df, lower.tail = FALSE)\np_value\n\n[1] 0.8762959\n\n\nIn R, these calculations can be easily performed using the function chisq.test() that also provide tables for the observed and expected frequencies\n\nchisq.test(df_data$trt, df_data$gender)\n\n\n    Pearson's Chi-squared test\n\ndata:  df_data$trt and df_data$gender\nX-squared = 0.2641, df = 2, p-value = 0.8763\n\nchisq.test(df_data$trt, df_data$gender)$observed\n\n                  df_data$gender\ndf_data$trt        Man Woman\n  self-guided       15    46\n  therapist-guided  17    43\n  waitlist          15    45\n\nchisq.test(df_data$trt, df_data$gender)$expected\n\n                  df_data$gender\ndf_data$trt             Man    Woman\n  self-guided      15.83978 45.16022\n  therapist-guided 15.58011 44.41989\n  waitlist         15.58011 44.41989\n\n\nOur results tells us that the null hypothesis of no association cannot be rejected. In other words, our data would not be unexpected if there was no association between gender and treatment group in the underlying population (kind of a strange though experiment, as there are only treatment variables in treatment studies, not in the underlying population).\n\n\n\n\n\n\nCaution\n\n\n\nThe Chi-squared test is only valid if the counts in each cell is &gt;5. When cell counts are lower, Fisher’s exact test should be used instead. You can use the fisher.test() function",
    "crumbs": [
      "Labs",
      "Testing two means"
    ]
  },
  {
    "objectID": "labs/testing-two-means.html#other-non-parametric-tests",
    "href": "labs/testing-two-means.html#other-non-parametric-tests",
    "title": "Testing two means and contingency tables",
    "section": "Other non-parametric tests",
    "text": "Other non-parametric tests\nThere are a number of other non-parametric tests that can be used if our data does not fulfill the assumptions for parametric tests, like t-tests and z-tests. We won’t go through the formulas for all these, but show you how they can be implemented in R.\n\nSign test\nThe sign test is used to test whether the median of paired differences equals a hypothesized value (often 0), using only the signs of differences, (i.e. + or -). This test can be used even when the sample is small, and the underlying population distribution is assumed to be non-normal.\nThe test is performed by counting how often a difference between pairs of values are positive. If there was no systematic change, this should occur 50% of the time. We then test whether our actual proportion of positive signs is likely to come from an underlying population where 50% of the signs are positive.\nWe could use it to test the nyll hypothesis that the median difference of LSAS-score at between pre-treatment and post-treatment is 0 (analog to our paired t-test above).\n\n# Paired sign test: H0 median LSAS(pre - post) = 0\ndiff_pre_post &lt;- df_data$lsas_post - df_data$lsas_screen # create differnece scores\nn &lt;- sum(diff_pre_post != 0, na.rm = TRUE) # exclude ties (exact zeros)\ns &lt;- sum(diff_pre_post[diff_pre_post != 0] &gt; 0, na.rm = TRUE) # number of positive differences (again excluding ties)\nbinom.test(s, n, p = 0.5, alternative = \"two.sided\") # a test of the proportion\n\n\n    Exact binomial test\n\ndata:  s and n\nnumber of successes = 24, number of trials = 167, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.0943006 0.2062463\nsample estimates:\nprobability of success \n             0.1437126 \n\n\nThis null hypothesis seems very unlikely.\n\n\nWilcoxon signed rank test\nTests if the distribution of paired differences is symmetric around 0 (often framed as median difference = 0), using magnitudes and signs.\n\nAssumptions: Paired observations; differences are symmetrically distributed.\nPro’s: Unlike the t-test, it does not assume interval level data. It is also better powered that the sign test.\nWe can use this to test the null hypothesis that the median LSAS score is the same at post-treatment as at pre-treatment.\n\nwilcox.test(df_data$lsas_post, df_data$lsas_screen,\n  paired = TRUE,\n  alternative = \"two.sided\",\n  exact = FALSE\n)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  df_data$lsas_post and df_data$lsas_screen\nV = 1310, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nAgain, the null hypothesis seems very unlikely.\n\n\nWilcox rank-sum test\nThis test, aslo known as the Mann-Whitney U-test, test whether two independent samples come from the same distribution (often interpreted as a shift in location/medians).\n\nAssumptions: Independent samples; similar shapes are helpful for a “median shift” interpretation.\nUnlike the independent groups t-test, it does not assume interval level data. We can use this test to test the null hypothesis that LSAS scores at post-treatment have the same distribution in the self-guided and the therapist-guided treatment groups.\n\n# Two-sample Wilcoxon rank-sum test (unpaired)\n\nwilcox.test(lsas_post ~ trt,\n  data = df_data,\n  subset = trt %in% c(\"self-guided\", \"therapist-guided\"),\n  exact = FALSE\n)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  lsas_post by trt\nW = 1872, p-value = 0.04974\nalternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "Labs",
      "Testing two means"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research school in clinical psychiatry",
    "section": "",
    "text": "Welcome to the course materials for Biostatistics courses in the Research School in Clinical Psychiatry at Karolinska Institutet. This website contains all materials related to the afternoon lab sessions of the courses.\n\n\n\n\n\n\nNoteUsing AI tools\n\n\n\nWe will not police your use of generative AI tools (ChatGPT, Claude, Co-pilot etc) in these lab sessions. They are useful for generating code that works, but you are still responsible for making sure that it is the correct code. We use some of these tools in our own day-to-day work and find them useful. However, we often have to correct subtle errors that are not immediately obvious. So please use them with care.\n\n\n\nTeaching team\nThe course director is Matteo Bottai, Professor of Biostatistics at Karolinska Institutet. Course leaders are Kristoffer Magnusson, Fred Johansson and Oskar Flygare, all researchers at Centre for Psychiatry Research, Karolinska Institutet.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "exercises/lab-1-exercises.html",
    "href": "exercises/lab-1-exercises.html",
    "title": "Lab 1 exercises",
    "section": "",
    "text": "Tip\n\n\n\nThree underscores (___) in the code below indicate where you need to fill in the missing parts.\nWe begin by loading the necessary packages for these tasks. Do you experience any issues with loading the packages? If so, you may need to install them first using the install.packages() function in the console.\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)",
    "crumbs": [
      "Exercises",
      "Lab 1 Exercises"
    ]
  },
  {
    "objectID": "exercises/lab-1-exercises.html#read-steps-data",
    "href": "exercises/lab-1-exercises.html#read-steps-data",
    "title": "Lab 1 exercises",
    "section": "1.1 Read STePS data",
    "text": "1.1 Read STePS data\nComplete the code below to read the STePS dataset. We create a new object df_data where we continue with edits. The original data is stored in df_rawdata without any edits.\n\ndf_rawdata &lt;- read_csv2(here(___))\n\ndf_data &lt;- df_rawdata",
    "crumbs": [
      "Exercises",
      "Lab 1 Exercises"
    ]
  },
  {
    "objectID": "exercises/lab-1-exercises.html#inspect-data-using-glimpse-and-view",
    "href": "exercises/lab-1-exercises.html#inspect-data-using-glimpse-and-view",
    "title": "Lab 1 exercises",
    "section": "1.2 Inspect data using glimpse() and view()",
    "text": "1.2 Inspect data using glimpse() and view()\nCall the glimpse() and view() functions to check the structure of the dataset. You can also use the head(), names(), ncol(), and nrow() functions.\n\nglimpse(___)\n\n\nview(___)",
    "crumbs": [
      "Exercises",
      "Lab 1 Exercises"
    ]
  },
  {
    "objectID": "exercises/lab-1-exercises.html#clean-column-names-part-1",
    "href": "exercises/lab-1-exercises.html#clean-column-names-part-1",
    "title": "Lab 1 exercises",
    "section": "1.3 Clean column names part 1",
    "text": "1.3 Clean column names part 1\nAdd the clean_names() function to the code below. Compare the names before and after using the function.\n\nnames(df_data)\n\n\ndf_data &lt;- df_data |&gt; \n  ___\n\nType names(df_data) again to see the changes.",
    "crumbs": [
      "Exercises",
      "Lab 1 Exercises"
    ]
  },
  {
    "objectID": "exercises/lab-1-exercises.html#clean-column-names-part-2",
    "href": "exercises/lab-1-exercises.html#clean-column-names-part-2",
    "title": "Lab 1 exercises",
    "section": "1.4 Clean column names part 2",
    "text": "1.4 Clean column names part 2\nThe clean_names() is very helpful, but it didn’t sort all of the problems. We still have some inconsistent column names from the raw data. Add a fix for the PHQ-9 inconsistencies.\n\ndf_data &lt;- df_data |&gt; \n  rename_with(~ .x |&gt; \n    str_replace_all(\"screening\", \"screen\") |&gt; \n    str_replace_all(\"ders_16|ders16\", \"ders\") |&gt; \n    str_replace_all(___)\n  )\n\nnames(df_data)",
    "crumbs": [
      "Exercises",
      "Lab 1 Exercises"
    ]
  },
  {
    "objectID": "exercises/lab-1-exercises.html#fix-missing-values",
    "href": "exercises/lab-1-exercises.html#fix-missing-values",
    "title": "Lab 1 exercises",
    "section": "1.5 Fix missing values",
    "text": "1.5 Fix missing values\nCheck which columns have weird values for missing values. You can use the glimpse() function or view the data in the viewer. You can also use the unique() function to check which values are present in a column.\n\ndf_data &lt;- df_data |&gt; \n  mutate(across(c(___), ~na_if(., \"missing\")))",
    "crumbs": [
      "Exercises",
      "Lab 1 Exercises"
    ]
  },
  {
    "objectID": "exercises/lab-1-exercises.html#fix-column-types",
    "href": "exercises/lab-1-exercises.html#fix-column-types",
    "title": "Lab 1 exercises",
    "section": "1.6 Fix column types",
    "text": "1.6 Fix column types\nIn the demonstration, we showed how to change column types to numeric. Replicate the same thing here, but for factor columns instead. Use the mutate() and across() functions.\n\n# which columns should be numeric?\nnum_cols &lt;- c(\"lsas\", \"gad\", \"phq9\", \"bbq\", \"scs\", \"dmrsodf\", \"ders\", \"pid_5\")\n\n# which columns should be factors?\nfct_cols &lt;- c(___)\n\ndf_data &lt;- df_data |&gt; \n  mutate(\n    across(starts_with(num_cols), as.numeric),\n    # INSERT FACTOR FIX HERE\n    ___\n  )\n\nUse the glimpse() function to check that it worked.\n\n___",
    "crumbs": [
      "Exercises",
      "Lab 1 Exercises"
    ]
  },
  {
    "objectID": "exercises/lab-1-exercises.html#save-cleaned-data",
    "href": "exercises/lab-1-exercises.html#save-cleaned-data",
    "title": "Lab 1 exercises",
    "section": "1.7 Save cleaned data",
    "text": "1.7 Save cleaned data\nFinally, save the cleaned data to a new CSV file called steps_clean.csv in the /data folder. Use the write_csv() function from the readr package along with here(), just like we did in the beginning of this script.\n\n___",
    "crumbs": [
      "Exercises",
      "Lab 1 Exercises"
    ]
  },
  {
    "objectID": "exercises/lab-power-sample-size.html",
    "href": "exercises/lab-power-sample-size.html",
    "title": "Lab: Power and Sample Size",
    "section": "",
    "text": "In this lab, we’ll explore the basics of power analysis. We’ll use the pwr package to calculate power for a planned study.",
    "crumbs": [
      "Exercises",
      "Power and sample size"
    ]
  },
  {
    "objectID": "exercises/lab-power-sample-size.html#understanding-the-simulation-approach",
    "href": "exercises/lab-power-sample-size.html#understanding-the-simulation-approach",
    "title": "Lab: Power and Sample Size",
    "section": "5.1 Understanding the Simulation Approach",
    "text": "5.1 Understanding the Simulation Approach\nInstead of using formulas, we can:\n\nSimulate many datasets under specific conditions (effect size, sample size)\nRun a t-test on each simulated dataset\nSave key statistics (p-value, confidence interval, effect size, etc.)\nCalculate power = proportion of significant tests\n\n\nExercise 6 (Create a simulation function)  \n\n\nProblemHintsSolution\n\n\nCreate a function that runs many simulated studies and captures comprehensive results from each one. The function should:\n\nGenerate data for two groups with specified sample size and effect size\nRun a t-test on each simulated dataset\nStore p-values, confidence intervals, effect sizes, and other statistics\n\n\n\n\n\n\n\n\n\n\n\n\nIn simulate_study(), the treatment group should have a mean equal to the effect_size (since control has mean 0 and SD = 1).\nThe p-value from a t-test is accessed with $p.value.\nsimulate_study &lt;- function(n_per_group, effect_size) {\n  data.frame(\n    group = rep(c(\"Control\", \"Treatment\"), each = n_per_group),\n    score = c(\n      rnorm(n_per_group, mean = 0, sd = 1),\n      rnorm(n_per_group, mean = effect_size, sd = 1)\n    )\n  )\n}\n\n# In run_power_simulation:\nresults$p_value[i] &lt;- test_result$p.value\n\n\n\n\n\n\n# Function to simulate a single study\nsimulate_study &lt;- function(n_per_group, effect_size) {\n  data.frame(\n    group = rep(c(\"Control\", \"Treatment\"), each = n_per_group),\n    score = c(\n      rnorm(n_per_group, mean = 0, sd = 1),\n      rnorm(n_per_group, mean = effect_size, sd = 1) #&lt;1&gt;\n    )\n  )\n}\n\n# Function to run many simulated studies\nrun_power_simulation &lt;- function(\n    n_per_group,\n    effect_size,\n    n_simulations = 1000,\n    alpha = 0.05) {\n  \n  # Create storage for results\n  results &lt;- data.frame(\n    simulation = 1:n_simulations,\n    p_value = numeric(n_simulations),\n    ci_lower = numeric(n_simulations),\n    ci_upper = numeric(n_simulations),\n    significant = logical(n_simulations),\n    effect = numeric(n_simulations)\n  )\n  \n  # Run many simulated studies\n  for (i in 1:n_simulations) {\n    # Simulate data\n    study_data &lt;- simulate_study(n_per_group, effect_size)\n    \n    # Run t-test\n    test_result &lt;- t.test(score ~ group, data = study_data, var.equal = TRUE)\n    \n    # Store results\n    results$p_value[i] &lt;- test_result$p.value #&lt;2&gt;\n    results$ci_lower[i] &lt;- test_result$conf.int[1]\n    results$ci_upper[i] &lt;- test_result$conf.int[2]\n    results$significant[i] &lt;- test_result$p.value &lt; alpha\n    results$effect[i] &lt;- diff(test_result$estimate)\n  }\n  \n  return(results)\n}\n\n# Test the function\nset.seed(123)\nsim_results &lt;- run_power_simulation(\n  n_per_group = 30,\n  effect_size = 0.5,\n  n_simulations = 10  # Using fewer for speed\n)\n\n# Show first few results\nhead(sim_results)\n# Function to simulate a single study\nsimulate_study &lt;- function(n_per_group, effect_size) {\n  data.frame(\n    group = rep(c(\"Control\", \"Treatment\"), each = n_per_group),\n    score = c(\n      rnorm(n_per_group, mean = 0, sd = 1),\n1      rnorm(n_per_group, mean = effect_size, sd = 1)\n    )\n  )\n}\n\n# Function to run many simulated studies\nrun_power_simulation &lt;- function(\n    n_per_group,\n    effect_size,\n    n_simulations = 1000,\n    alpha = 0.05) {\n  \n  # Create storage for results\n  results &lt;- data.frame(\n    simulation = 1:n_simulations,\n    p_value = numeric(n_simulations),\n    ci_lower = numeric(n_simulations),\n    ci_upper = numeric(n_simulations),\n    significant = logical(n_simulations),\n    effect = numeric(n_simulations)\n  )\n  \n  # Run many simulated studies\n  for (i in 1:n_simulations) {\n    # Simulate data\n    study_data &lt;- simulate_study(n_per_group, effect_size)\n    \n    # Run t-test\n    test_result &lt;- t.test(score ~ group, data = study_data, var.equal = TRUE)\n    \n    # Store results\n2    results$p_value[i] &lt;- test_result$p.value\n    results$ci_lower[i] &lt;- test_result$conf.int[1]\n    results$ci_upper[i] &lt;- test_result$conf.int[2]\n    results$significant[i] &lt;- test_result$p.value &lt; alpha\n    results$effect[i] &lt;- diff(test_result$estimate)\n  }\n  \n  return(results)\n}\n\n# Test the function\nset.seed(123)\nsim_results &lt;- run_power_simulation(\n  n_per_group = 30,\n  effect_size = 0.5,\n  n_simulations = 10  # Using fewer for speed\n)\n\n# Show first few results\nhead(sim_results)\n\n\n\n1\n\nTreatment group has mean = effect_size (Cohen’s d = 0.5 when SD = 1)\n\n2\n\nExtract p-value from the t-test result\n\n\n\n\nThis function returns a data frame with comprehensive information about each simulated study, making it easy to analyze power and other properties.\n\n\n\n\n\n\n\n\nExercise 7 (Analyze simulation results and calculate power)  \n\n\nProblemHintsSolution\n\n\nNow use the simulation function to run 1000 studies and analyze the results. Calculate the statistical power and create visualizations showing the distribution of p-values and confidence intervals.\n\n\n\n\n\n\n\n\n\n\n\nThe sim_results data frame has a column called p_value with the p-value from each simulation.\nTo calculate power: - Compare each p-value to 0.05 - Take the mean of the TRUE/FALSE values (TRUE = 1, FALSE = 0)\nsimulated_power &lt;- mean(sim_results$p_value &lt; 0.05)\n\n\n\n\n\n\nlibrary(ggplot2)\n\n# Run simulation\nset.seed(123)\nsim_results &lt;- run_power_simulation(\n  n_per_group = 30,\n  effect_size = 0.5,\n  n_simulations = 1000\n)\n\n# Calculate power (proportion of p-values &lt; 0.05)\nsimulated_power &lt;- mean(sim_results$p_value &lt; 0.05) #&lt;1&gt;\n\ncat(\"Simulated power:\", round(simulated_power, 3), \"\\n\")\n\n# Compare to analytical power\nanalytical_power &lt;- pwr.t.test(\n  n = 30,\n  d = 0.5,\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\ncat(\"Analytical power:\", round(analytical_power, 3), \"\\n\")\nlibrary(ggplot2)\n\n# Run simulation\nset.seed(123)\nsim_results &lt;- run_power_simulation(\n  n_per_group = 30,\n  effect_size = 0.5,\n  n_simulations = 1000\n)\n\n# Calculate power (proportion of p-values &lt; 0.05)\n1simulated_power &lt;- mean(sim_results$p_value &lt; 0.05)\n\ncat(\"Simulated power:\", round(simulated_power, 3), \"\\n\")\n\n# Compare to analytical power\nanalytical_power &lt;- pwr.t.test(\n  n = 30,\n  d = 0.5,\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\ncat(\"Analytical power:\", round(analytical_power, 3), \"\\n\")\n\n\n\n1\n\nCalculate proportion of p-values below 0.05 (mean of TRUE/FALSE values)\n\n\n\n\nThe simulated power should be very close to the analytical power (~0.47). This means about 47% of our simulated studies correctly detected the effect (had p &lt; 0.05).\n\n\n\n\n\n\n\n\nExercise 8 (Calculate power using confidence intervals)  \n\n\nProblemHintsSolution\n\n\nAnother way to calculate power is through confidence intervals! A study detects an effect when the 95% CI excludes zero (the null hypothesis value). Calculate power by determining what proportion of confidence intervals do NOT contain zero.\n\n\n\n\n\n\n\n\n\n\n\nA confidence interval excludes zero if: - The lower bound is above 0 (positive effect), OR - The upper bound is below 0 (negative effect)\nThis is equivalent to rejecting the null hypothesis (H₀: difference = 0).\nci_excludes_zero &lt;- sim_results$ci_lower &gt; 0 | sim_results$ci_upper &lt; 0\n\n\n\n\n\n\n\n# Calculate power from confidence intervals\n# A CI that excludes 0 means we reject the null hypothesis\nci_excludes_zero &lt;- sim_results$ci_lower &gt; 0 | sim_results$ci_upper &lt; 0  #&lt;1&gt;\npower_from_ci &lt;- mean(ci_excludes_zero)\n\ncat(\"Power from p-values:\", round(simulated_power, 3), \"\\n\")\ncat(\"Power from CIs:     \", round(power_from_ci, 3), \"\\n\")\ncat(\"These should be identical!\\n\")\n\n# Calculate power from confidence intervals\n# A CI that excludes 0 means we reject the null hypothesis\n1ci_excludes_zero &lt;- sim_results$ci_lower &gt; 0 | sim_results$ci_upper &lt; 0\npower_from_ci &lt;- mean(ci_excludes_zero)\n\ncat(\"Power from p-values:\", round(simulated_power, 3), \"\\n\")\ncat(\"Power from CIs:     \", round(power_from_ci, 3), \"\\n\")\ncat(\"These should be identical!\\n\")\n\n\n\n1\n\nCI excludes 0 if lower bound &gt; 0 OR upper bound &lt; 0\n\n\n\n\nKey insight: Power can be calculated two equivalent ways: - From p-values: Proportion where p &lt; 0.05 - From confidence intervals: Proportion where CI excludes 0\nThese give identical results because they’re testing the same hypothesis! A 95% CI that excludes 0 corresponds exactly to p &lt; 0.05.",
    "crumbs": [
      "Exercises",
      "Power and sample size"
    ]
  },
  {
    "objectID": "exercises/lab-webr-p-value-ci.html",
    "href": "exercises/lab-webr-p-value-ci.html",
    "title": "Lab webr p-values and confidence intervals",
    "section": "",
    "text": "Exercise 1HintsSolution\n\n\nCalculate the two-sided p-value for the null hypothesis that the mean PHQ-9 value in the underlying population is 9, and describe in words what this number mean\n\n\n\n\n\n\n\n\n\n\n\nYou can calculate it manually using the formula in the P-values and confidence intervals chapter or use the t.test() function\nt.test(df_data$phq9_screen,\n       mu=___,\n       alternative = \"____\")\n\n\n\n\nThe full solution is:\n#calculate manually\nx_bar &lt;- mean(df_data$phq9_screen)\nse &lt;- sd(df_data$phq9_screen) / sqrt(nrow(df_data))\nt_value &lt;- (x_bar - 9) / se\n(1 - pt(t_value, df = 180))*2\n\n#or using the t.test function\n\nt.test(df_data$phq9_screen,\n       mu=9,\n       alternative = \"two.sided\")\n\n\n\n\n\n\n\n\nExercise 2HintsSolution\n\n\nCalculate the 95% confidence interval for PHQ-9, and describe in words what the numbers mean.\n\n\n\n\n\n\n\n\n\n\n\nYou can calculate it manually using the formula in the P-values and confidence intervals chapter or again use the t.test() function\nt.test(df_data$phq9_screen,\n       mu=___,\n       alternative = \"____\")\n\n\n\n\nThe full solution is:\nx_bar &lt;- mean(df_data$phq9_screen)\nse &lt;- sd(df_data$phq9_screen) / sqrt(nrow(df_data))\nz &lt;- 1.96\n\n#upper confidence limit\nucl &lt;- x_bar + z*se\n#lower confidence limit\nlcl &lt;- x_bar - z*se\n\nprint(c(lcl, ucl))\n\n#or\nt.test(df_data$phq9_screen,\n       mu=9,\n       alternative = \"two.sided\")\n\n\n\n\n\n\n\n\nExercise 3HintsSolution\n\n\nCalculate the p-value for getting our observed proportion of men, \\(\\hat{p}\\), if the the true population proportion, \\(p\\), is 40% or more the prop.test() function\n\n\n\n\n\n\n\n\n\n\n\nprop.test(x= sum(df_data$gender==\"Man\"), \n          n= length(____), \n          p=___,\n          alternative=\"less\")\n\n\n\n\nThe full solution is:\n# Simulating a gender variable\nn &lt;- nrow(df_data)\ndf_data$gender &lt;- rbinom(n, 1, 0.7)\ndf_data$gender &lt;- ifelse(df_data$gender == 1, \"Woman\", \"Man\")\n\nprop.test(x= sum(df_data$gender==\"Man\"), \n          n= length(df_data$gender), \n          p=0.4,\n          alternative=\"less\")\n\n\n\n\n\n\n\n\nExercise 4HintsSolution\n\n\nCalculate the p-value for getting our observed proportion of men, \\(\\hat{p}\\), if the the true population proportion, \\(p\\), is 40% or more the prop.test() function\n\n\n\n\n\n\n\n\n\n\n\nYou can combine the prop.test() function with the table() function for easier syntax\nprop.test(table(____))\n\n\n\n\nThe full solution is:\n# Simulating a gender variable\nn &lt;- nrow(df_data)\ndf_data$gender &lt;- rbinom(n, 1, 0.7)\ndf_data$gender &lt;- ifelse(df_data$gender == 1, \"Woman\", \"Man\")\n\n#getting ci \nprop.test(table(df_data$gender))\n\n\n\n\n\nExercise 5HintsSolution\n\n\nReason about the the meaning and interpretation of the confidence intervals you have calculated in the context of how the actual STePs study was performed. The study can be found at: https://www.nature.com/articles/s44184-024-00063-0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBONUS Exercise 6HintsSolution\n\n\nCalculate the 95% Wald confidence interval using the formula for the confidence interval and compare it to what you got using the prop.test() function\n\n\n\n\n\n\n\n\n\n\n\nyou can use the formula for the standard error of the proportion: \\[ \\mathrm{SE}(p) = \\sqrt{\\frac{p(1 - p)}{n}} \\]\nand combine with the formula for the z-scores\n\\[ z= \\frac{p - \\hat{p}}{SE} \\]\np_hat &lt;- mean(df_data$gender==\"___\")\nn = nrow(____)\nse &lt;- sqrt(p_hat*(1-p_hat)/n)\nz= 1.96\n\n#upper confidence limit\nucl &lt;- p_hat + __*__\n\n#lower confidence limit\nlcl &lt;- p_hat - __*s__\n\nprint(c(lcl,ucl))\n\n\n\n\nThe full solution is:\n# manual Wald CI\np_hat &lt;- mean(df_data$gender==\"Man\")\nn = nrow(df_data)\nse &lt;- sqrt(p_hat*(1-p_hat)/n)\nz= 1.96\n#upper coinfidence limit\nucl &lt;- p_hat + z*se\n#lower confidence limit\nlcl &lt;- p_hat - z*se\n\nprint(c(lcl,ucl))\n\n# and from prop.test\nprop.test(table(df_data$gender))\n\n\n\n\n\n\n\n\nBONUS Exercise 7HintsSolution\n\n\nModify the simulation code for the sampling distribution to determine what would happen to the p-value if the sample size was 10, 100 or 1000\n\n\n\n\n\n\n\n\n\n\n\nn_samples &lt;- 1e4 # the number of samples\nsmp_size &lt;- ____ # the size of our samples\nmeans &lt;- rep(NA, n_samples) # an empty vector to contain our mean values\n\nfor (i in 1:n_samples) {\n  x &lt;- rnorm(smp_size, mean = 82, sd = sd(df_data$lsas_screen))\n  means[i] &lt;- mean(x)\n}\n\nmean(means &gt;= mean(df_data$lsas_screen)) # proportion of simulated means that are larger than our observed mean\n\n\n\n\nThe full solution is:\n# for a sample size of 10\nn_samples &lt;- 1e4 # the number of samples\nsmp_size &lt;- 10 # the size of our samples\nmeans &lt;- rep(NA, n_samples) # an empty vector to contain our mean values\n\nfor (i in 1:n_samples) {\n  x &lt;- rnorm(smp_size, mean = 82, sd = sd(df_data$lsas_screen))\n  means[i] &lt;- mean(x)\n}\n\nmean(means &gt;= mean(df_data$lsas_screen)) # proportion of simulated means that are larger than our observed mean\n\n# for a sample size of 100\n\nn_samples &lt;- 1e4 # the number of samples\nsmp_size &lt;- 100 # the size of our samples\nmeans &lt;- rep(NA, n_samples) # an empty vector to contain our mean values\n\nfor (i in 1:n_samples) {\n  x &lt;- rnorm(smp_size, mean = 82, sd = sd(df_data$lsas_screen))\n  means[i] &lt;- mean(x)\n}\n\nmean(means &gt;= mean(df_data$lsas_screen)) # proportion of simulated means that are larger than our observed mean\n\n\n# for a sample size of 1000 \n\nn_samples &lt;- 1e4 # the number of samples\nsmp_size &lt;- 1000 # the size of our samples\nmeans &lt;- rep(NA, n_samples) # an empty vector to contain our mean values\n\nfor (i in 1:n_samples) {\n  x &lt;- rnorm(smp_size, mean = 82, sd = sd(df_data$lsas_screen))\n  means[i] &lt;- mean(x)\n}\n\nmean(means &gt;= mean(df_data$lsas_screen)) # proportion of simulated means that are larger than our observed mean",
    "crumbs": [
      "Exercises",
      "P-values and confidence intervals"
    ]
  },
  {
    "objectID": "exercises/lab-webr-classification.html",
    "href": "exercises/lab-webr-classification.html",
    "title": "Lab: Classification",
    "section": "",
    "text": "In this lab, we’ll explore classification metrics using the PHQ-9 scale. We’ll practice calculating sensitivity, specificity, positive predictive value, and negative predictive value. The companion chapter Classification explains the concepts in more detail, so use it as a reference if you get stuck.\n\n\n\n\n\n\nTip\n\n\n\nWhile you can complete all the exercises in your browser, we recommend also practicing in RStudio. Using an editor like RStudio will help you build real-world skills for writing, running, and saving your R code.\n\n\n\n1 Load and prepare the data\nFirst, let’s load the data and create the binary outcome variable we’ll use for classification.\n\nExercise 1 (Create a binary variable of PHQ-9 at post-treatment)  \n\n\nProblemHintsSolution\n\n\nFor the exercises in this chapter, you will work with PHQ-9 instead of LSAS. Create a binary outcome variable phq9_post_bin that is “Low” if phq9_post is less than 10, and “High” otherwise.\n\n\n\n\n\n\n\n\n\n\n\nThe cutoff for PHQ-9 is 10. Use if_else() to create the binary variable.\n\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# load data\ndf_data &lt;- read_csv(here(\"data\", \"steps_clean.csv\"))\n\n# create binary outcome variable\ndf_data &lt;- df_data |&gt;\n  mutate(\n    phq9_post_bin = if_else(phq9_post &lt;10, \"Low\", \"High\"),\n    phq9_post_bin = factor(phq9_post_bin, levels = c(\"Low\", \"High\"))\n  )\n\n# Check the distribution\ntable(df_data$phq9_post_bin)\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# load data\ndf_data &lt;- read_csv(here(\"data\", \"steps_clean.csv\"))\n\n# create binary outcome variable\ndf_data &lt;- df_data |&gt;\n  mutate(\n    phq9_post_bin = if_else(phq9_post &lt;10, \"Low\", \"High\"),\n    phq9_post_bin = factor(phq9_post_bin, levels = c(\"Low\", \"High\"))\n  )\n\n# Check the distribution\ntable(df_data$phq9_post_bin)\n\n\n\nPHQ-9 cutoff of 10 separates low from high depression symptoms\n\nThe binary variable is now ready for classification analysis.\n\n\n\n\n\n\n\n\n\n2 Fit a logistic regression model\nNow let’s fit a logistic regression model to predict the binary outcome. We will use a few functions from the tidymodels package to achieve this, but regression models can be fit with other packages as well. Note also that regression models are not the focus of this course, so we will not go into the details of fitting them here.\n\nExercise 2 (Fit a logistic regression model (PHQ-9))  \n\n\nProblemHintsSolution\n\n\nFit a logistic regression model to predict phq9_post_bin using phq9_screen and group. Create a confusion matrix for the predictions.\n\n\n\n\n\n\n\n\n\n\n\nThe tidymodels workflow is:\n1. Specify the model type: logistic_reg()\n2. Set the engine: set_engine(\"glm\")\n3. Set the mode: set_mode(\"classification\")\n4. Fit the model: fit(formula, data)\n5. Make predictions: predict(model, new_data)\n6. Create confusion matrix: conf_mat(data, truth, estimate)\nThe missing step here is the formula. The typical formula is outcome ~ predictors.\n\n\n\n\n\n\nlibrary(tidymodels)\n\n# Fit logistic regression model\nphq9_fit &lt;- logistic_reg() |&gt; #&lt;1&gt;\n  set_engine(\"glm\") |&gt; #&lt;2&gt;\n  set_mode(\"classification\") |&gt; #&lt;3&gt;\n  fit(phq9_post_bin ~ phq9_screen + group, data = df_data) #&lt;4&gt;\n\n# Make predictions\nphq9_pred &lt;- predict(phq9_fit, new_data = df_data) |&gt; #&lt;5&gt;\n  bind_cols(df_data)\n\n# Create confusion matrix\nphq9_conf_mat &lt;- conf_mat(phq9_pred, truth = phq9_post_bin, estimate = .pred_class) #&lt;6&gt;\n\n# Display confusion matrix\nphq9_conf_mat\nlibrary(tidymodels)\n\n# Fit logistic regression model\n1phq9_fit &lt;- logistic_reg() |&gt;\n2  set_engine(\"glm\") |&gt;\n3  set_mode(\"classification\") |&gt;\n4  fit(phq9_post_bin ~ phq9_screen + group, data = df_data)\n\n# Make predictions\n5phq9_pred &lt;- predict(phq9_fit, new_data = df_data) |&gt;\n  bind_cols(df_data)\n\n# Create confusion matrix\n6phq9_conf_mat &lt;- conf_mat(phq9_pred, truth = phq9_post_bin, estimate = .pred_class)\n\n# Display confusion matrix\nphq9_conf_mat\n\n\n\n1\n\nSpecify logistic regression model\n\n2\n\nUse GLM engine\n\n3\n\nSet to classification mode\n\n4\n\nFit model with predictors\n\n5\n\nMake predictions on the data\n\n6\n\nCreate confusion matrix\n\n\n\n\nThe confusion matrix shows how well our model predicts the binary outcome.\n\n\n\n\n\n\n\n\n\n3 Calculate classification metrics manually\nNow let’s calculate the key classification metrics manually by looking at the confusion matrix. We will look at sensitivity, specificity, positive predictive value, and negative predictive value. All you need are the four numbers from the confusion matrix!\nIf you need to refresh your memory about which numbers are needed for which metric, look in the companion chapter Classification.\n\nExercise 3 (Calculate sensitivity)  \n\n\nProblemHintsSolution\n\n\nWe begin by extracting all values from the confusion matrix. Then we will use the relevant values for each metric.\nCalculate the sensitivity (True Positive Rate) for the PHQ-9 predictions. Extract the values from the confusion matrix first.\n\n\n\n\n\n\n\n\n\n\n\nTo calculate sensitivity you need the true positives and false negatives.\n\n\n\n\n\n\n\n# Extract values from confusion matrix\ntrue_pos &lt;- phq9_conf_mat$table[1, 1]  # True positives\nfalse_pos &lt;- phq9_conf_mat$table[2, 1] # False positives\nfalse_neg &lt;- phq9_conf_mat$table[1, 2] # False negatives\ntrue_neg &lt;- phq9_conf_mat$table[2, 2]  # True negatives\n\n# Calculate sensitivity (True Positive Rate)\nsensitivity &lt;- true_pos / (true_pos + false_neg) #&lt;1&gt;\n\n# Display result\nsensitivity\n\n# Extract values from confusion matrix\ntrue_pos &lt;- phq9_conf_mat$table[1, 1]  # True positives\nfalse_pos &lt;- phq9_conf_mat$table[2, 1] # False positives\nfalse_neg &lt;- phq9_conf_mat$table[1, 2] # False negatives\ntrue_neg &lt;- phq9_conf_mat$table[2, 2]  # True negatives\n\n# Calculate sensitivity (True Positive Rate)\n1sensitivity &lt;- true_pos / (true_pos + false_neg)\n\n# Display result\nsensitivity\n\n\n\n1\n\nSensitivity: proportion of actual positives correctly identified\n\n\n\n\nSensitivity tells us how well our model identifies true positive cases.\n\n\n\n\n\n\n\n\nExercise 4 (Calculate specificity)  \n\n\nProblemHintsSolution\n\n\nCalculate the specificity (True Negative Rate) for the PHQ-9 predictions.\n\n\n\n\n\n\n\n\n\n\n\nTo calculate specificity you need the true negatives and false positives.\n\n\n\n\n\n\n\n# Calculate specificity (True Negative Rate)\nspecificity &lt;- true_neg / (true_neg + false_pos) #&lt;1&gt;\n\n# Display result\nspecificity\n\n# Calculate specificity (True Negative Rate)\n1specificity &lt;- true_neg / (true_neg + false_pos)\n\n# Display result\nspecificity\n\n\n\n1\n\nSpecificity: proportion of actual negatives correctly identified\n\n\n\n\nSpecificity tells us how well our model identifies true negative cases.\n\n\n\n\n\n\n\n\nExercise 5 (Calculate positive predictive value)  \n\n\nProblemHintsSolution\n\n\nCalculate the positive predictive value (PPV) for the PHQ-9 predictions.\n\n\n\n\n\n\n\n\n\n\n\nTo calculate PPV you need the true positives and false positives.\n\n\n\n\n\n\n\n# Calculate positive predictive value (Precision)\nppv &lt;- true_pos / (true_pos + false_pos) #&lt;1&gt;\n\n# Display result\nppv\n\n# Calculate positive predictive value (Precision)\n1ppv &lt;- true_pos / (true_pos + false_pos)\n\n# Display result\nppv\n\n\n\n1\n\nPPV: proportion of predicted positives that are actually positive\n\n\n\n\nPPV tells us how reliable our positive predictions are.\n\n\n\n\n\n\n\n\nExercise 6 (Calculate negative predictive value)  \n\n\nProblemHintsSolution\n\n\nCalculate the negative predictive value (NPV) for the PHQ-9 predictions.\n\n\n\n\n\n\n\n\n\n\n\nTo calculate NPV you need the true negatives and false negatives.\n\n\n\n\n\n\n\n# Calculate negative predictive value\nnpv &lt;- true_neg / (true_neg + false_neg) #&lt;1&gt;\n\n# Display result\nnpv\n\n# Calculate negative predictive value\n1npv &lt;- true_neg / (true_neg + false_neg)\n\n# Display result\nnpv\n\n\n\n1\n\nNPV: proportion of predicted negatives that are actually negative\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 Summary\nIn this lab, you learned:\n\nBinary outcome creation: How to create binary outcomes from continuous variables using meaningful cutoffs\nLogistic regression: How to fit classification models using tidymodels\nConfusion matrices: How to create and interpret confusion matrices\nClassification metrics: How to calculate sensitivity, specificity, PPV, and NPV using the confusion matrix",
    "crumbs": [
      "Exercises",
      "Classification"
    ]
  }
]