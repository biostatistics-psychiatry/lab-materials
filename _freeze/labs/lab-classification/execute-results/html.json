{
  "hash": "8175afbc3dc22344e46d4ba8abf1919f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab: Classification\"\nnumber-sections: true\nformat: live-html\nengine: knitr\nwebr:\n  packages:\n    - dplyr\n    - tidymodels\n    - here\nresources:\n  - ../data/steps_clean.csv\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n#| output: false\nwebr::install(\"gradethis\", quiet = TRUE)\nlibrary(gradethis)\noptions(webr.exercise.checker = function(\n  label, user_code, solution_code, check_code, envir_result, evaluate_result,\n  envir_prep, last_value, engine, stage, ...\n) {\n  if (is.null(check_code)) {\n    # No grading code, so just skip grading\n    invisible(NULL)\n  } else if (is.null(label)) {\n    list(\n      correct = FALSE,\n      type = \"warning\",\n      message = \"All exercises must have a label.\"\n    )\n  } else if (is.null(solution_code)) {\n    list(\n      correct = FALSE,\n      type = \"warning\",\n      message = htmltools::tags$div(\n        htmltools::tags$p(\"A problem occurred grading this exercise.\"),\n        htmltools::tags$p(\n          \"No solution code was found. Note that grading exercises using the \",\n          htmltools::tags$code(\"gradethis\"),\n          \"package requires a model solution to be included in the document.\"\n        )\n      )\n    )\n  } else {\n    gradethis::gradethis_exercise_checker(\n      label = label, solution_code = solution_code, user_code = user_code,\n      check_code = check_code, envir_result = envir_result,\n      evaluate_result = evaluate_result, envir_prep = envir_prep,\n      last_value = last_value, stage = stage, engine = engine)\n  }\n})\n```\n:::\n\n\n\n\n\nIn this lab, we'll explore classification metrics using the PHQ-9 scale. We'll practice calculating sensitivity, specificity, positive predictive value, and negative predictive value. The companion chapter [Classification](../chapters/classification.qmd) explains the concepts in more detail, so use it as a reference if you get stuck.\n\n::: {.callout-tip}\nWhile you can complete all the exercises in your browser, we recommend also practicing in RStudio. Using an editor like RStudio will help you build real-world skills for writing, running, and saving your R code.\n:::\n\n# Load and prepare the data\n\nFirst, let's load the data and create the binary outcome variable we'll use for classification.\n\n::: {#exr-phq9-binary}\n## Create a binary variable of PHQ-9 at post-treatment\n:::\n\n:::: {.panel-tabset}\n## Problem\n\nFor the exercises in this chapter, you will work with PHQ-9 instead of LSAS. Create a binary outcome variable `phq9_post_bin` that is \"Low\" if `phq9_post` is less than 10, and \"High\" otherwise.\n\n\n\n\n::: {.cell exercise='ex_1' envir='class_env'}\n```{webr}\n#| label: ex-1\n#| exercise: ex_1\n#| envir: class_env\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# load data\ndf_data <- read_csv(here(\"data\", \"steps_clean.csv\"))\n\n# create binary outcome variable\ndf_data <- df_data |>\n  mutate(\n    phq9_post_bin = _____(phq9_post <__, \"Low\", \"High\"),\n    phq9_post_bin = factor(phq9_post_bin, levels = c(\"Low\", \"High\"))\n  )\n\n# Check the distribution\ntable(df_data$phq9_post_bin)\n```\n:::\n\n\n\n\n## Hints\n\n::: {.hint exercise=\"ex_1\"}\n\nThe cutoff for PHQ-9 is 10. Use `if_else()` to create the binary variable.\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_1\"}\n\n\n\n\n::: {.cell exercise='ex_1' solution='true'}\n```{webr}\n#| label: ex-1-solution\n#| exercise: ex_1\n#| solution: true\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(here)\n\n# load data\ndf_data <- read_csv(here(\"data\", \"steps_clean.csv\"))\n\n# create binary outcome variable\ndf_data <- df_data |>\n  mutate(\n    phq9_post_bin = if_else(phq9_post <10, \"Low\", \"High\"),\n    phq9_post_bin = factor(phq9_post_bin, levels = c(\"Low\", \"High\"))\n  )\n\n# Check the distribution\ntable(df_data$phq9_post_bin)\n```\n:::\n\n\n\n1. PHQ-9 cutoff of 10 separates low from high depression symptoms\n\nThe binary variable is now ready for classification analysis.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_1' check='true'}\n```{webr}\n#| label: ex-1-gradethis\n#| exercise: ex_1\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n# Fit a logistic regression model\n\nNow let's fit a logistic regression model to predict the binary outcome. We will use a few functions from the `tidymodels` package to achieve this, but regression models can be fit with other packages as well. Note also that regression models are not the focus of this course, so we will not go into the details of fitting them here.\n\n::: {#exr-phq9-logistic}\n## Fit a logistic regression model (PHQ-9)\n:::\n\n:::: {.panel-tabset}\n## Problem\n\nFit a logistic regression model to predict `phq9_post_bin` using `phq9_screen` and `group`. Create a confusion matrix for the predictions.\n\n\n\n\n::: {.cell exercise='ex_2' envir='class_env'}\n```{webr}\n#| label: ex-2\n#| exercise: ex_2\n#| envir: class_env\n\n# load the tidymodels package\nlibrary(tidymodels)\n\n# fit logistic regression model\nphq9_fit <- logistic_reg() |>\n  set_engine(\"glm\") |>\n  set_mode(\"classification\") |>\n  fit(___ ~ ___ + ___, data = df_data)\n\n# Make predictions\nphq9_pred <- predict(phq9_fit, new_data = df_data) |>\n  bind_cols(df_data)\n\n# Create confusion matrix\nphq9_conf_mat <- conf_mat(phq9_pred, truth = phq9_post_bin, estimate = .pred_class)\n\n# Display confusion matrix\nphq9_conf_mat\n```\n:::\n\n\n\n\n## Hints\n\n::: {.hint exercise=\"ex_2\"}\n\nThe `tidymodels` workflow is:  \n1. Specify the model type: `logistic_reg()`  \n2. Set the engine: `set_engine(\"glm\")`  \n3. Set the mode: `set_mode(\"classification\")`  \n4. Fit the model: `fit(formula, data)`  \n5. Make predictions: `predict(model, new_data)`  \n6. Create confusion matrix: `conf_mat(data, truth, estimate)`  \n\nThe missing step here is the formula. The typical formula is `outcome ~ predictors`.\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_2\"}\n\n\n\n\n::: {.cell exercise='ex_2' solution='true'}\n```{webr}\n#| label: ex-2-solution\n#| exercise: ex_2\n#| solution: true\nlibrary(tidymodels)\n\n# Fit logistic regression model\nphq9_fit <- logistic_reg() |> #<1>\n  set_engine(\"glm\") |> #<2>\n  set_mode(\"classification\") |> #<3>\n  fit(phq9_post_bin ~ phq9_screen + group, data = df_data) #<4>\n\n# Make predictions\nphq9_pred <- predict(phq9_fit, new_data = df_data) |> #<5>\n  bind_cols(df_data)\n\n# Create confusion matrix\nphq9_conf_mat <- conf_mat(phq9_pred, truth = phq9_post_bin, estimate = .pred_class) #<6>\n\n# Display confusion matrix\nphq9_conf_mat\n```\n:::\n\n\n\n1. Specify logistic regression model\n2. Use GLM engine\n3. Set to classification mode\n4. Fit model with predictors\n5. Make predictions on the data\n6. Create confusion matrix\n\nThe confusion matrix shows how well our model predicts the binary outcome.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_2' check='true'}\n```{webr}\n#| label: ex-2-gradethis\n#| exercise: ex_2\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n# Calculate classification metrics manually\n\nNow let's calculate the key classification metrics manually by looking at the confusion matrix. We will look at *sensitivity*, *specificity*, *positive predictive value*, and *negative predictive value*. All you need are the four numbers from the confusion matrix!\n\nIf you need to refresh your memory about which numbers are needed for which metric, look in the companion chapter [Classification](../chapters/classification.qmd#evaluate-classification-manually).\n\n::: {#exr-sensitivity}\n## Calculate sensitivity\n:::\n\n:::: {.panel-tabset}\n## Problem\n\nWe begin by extracting all values from the confusion matrix. Then we will use the relevant values for each metric.\n\nCalculate the sensitivity (True Positive Rate) for the PHQ-9 predictions. Extract the values from the confusion matrix first.\n\n\n\n\n::: {.cell exercise='ex_3' envir='class_env'}\n```{webr}\n#| label: ex-3\n#| exercise: ex_3\n#| envir: class_env\n\n# Extract values from confusion matrix\ntrue_pos <- phq9_conf_mat$table[1, 1]  # True positives\nfalse_pos <- phq9_conf_mat$table[2, 1] # False positives\nfalse_neg <- phq9_conf_mat$table[1, 2] # False negatives\ntrue_neg <- phq9_conf_mat$table[2, 2]  # True negatives\n\n# Calculate sensitivity (True Positive Rate)\nsensitivity <- ____ / (____ + ____)\n\n# Display result\nsensitivity\n```\n:::\n\n\n\n\n## Hints\n\n::: {.hint exercise=\"ex_3\"}\n\nTo calculate sensitivity you need the *true positives* and *false negatives*.\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_3\"}\n\n\n\n\n::: {.cell exercise='ex_3' solution='true'}\n```{webr}\n#| label: ex-3-solution\n#| exercise: ex_3\n#| solution: true\n\n# Extract values from confusion matrix\ntrue_pos <- phq9_conf_mat$table[1, 1]  # True positives\nfalse_pos <- phq9_conf_mat$table[2, 1] # False positives\nfalse_neg <- phq9_conf_mat$table[1, 2] # False negatives\ntrue_neg <- phq9_conf_mat$table[2, 2]  # True negatives\n\n# Calculate sensitivity (True Positive Rate)\nsensitivity <- true_pos / (true_pos + false_neg) #<1>\n\n# Display result\nsensitivity\n```\n:::\n\n\n\n1. Sensitivity: proportion of actual positives correctly identified\n\nSensitivity tells us how well our model identifies true positive cases.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_3' check='true'}\n```{webr}\n#| label: ex-3-gradethis\n#| exercise: ex_3\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n::: {#exr-specificity}\n## Calculate specificity\n:::\n\n:::: {.panel-tabset}\n## Problem\n\nCalculate the specificity (True Negative Rate) for the PHQ-9 predictions.\n\n\n\n\n::: {.cell exercise='ex_4' envir='class_env'}\n```{webr}\n#| label: ex-4\n#| exercise: ex_4\n#| envir: class_env\n\n# Calculate specificity (True Negative Rate)\nspecificity <- ____ / (____ + ____)\n\n# Display result\nspecificity\n```\n:::\n\n\n\n\n## Hints\n\n::: {.hint exercise=\"ex_4\"}\n\nTo calculate specificity you need the *true negatives* and *false positives*.\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_4\"}\n\n\n\n\n::: {.cell exercise='ex_4' solution='true'}\n```{webr}\n#| label: ex-4-solution\n#| exercise: ex_4\n#| solution: true\n\n# Calculate specificity (True Negative Rate)\nspecificity <- true_neg / (true_neg + false_pos) #<1>\n\n# Display result\nspecificity\n```\n:::\n\n\n\n1. Specificity: proportion of actual negatives correctly identified\n\nSpecificity tells us how well our model identifies true negative cases.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_4' check='true'}\n```{webr}\n#| label: ex-4-gradethis\n#| exercise: ex_4\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n::: {#exr-ppv}\n## Calculate positive predictive value\n:::\n\n:::: {.panel-tabset}\n## Problem\n\nCalculate the positive predictive value (PPV) for the PHQ-9 predictions.\n\n\n\n\n::: {.cell exercise='ex_5' envir='class_env'}\n```{webr}\n#| label: ex-5\n#| exercise: ex_5\n#| envir: class_env\n\n# Calculate positive predictive value (Precision)\nppv <- ____ / (____ + ____)\n\n# Display result\nppv\n```\n:::\n\n\n\n\n## Hints\n\n::: {.hint exercise=\"ex_5\"}\n\nTo calculate PPV you need the *true positives* and *false positives*.\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_5\"}\n\n\n\n\n::: {.cell exercise='ex_5' solution='true'}\n```{webr}\n#| label: ex-5-solution\n#| exercise: ex_5\n#| solution: true\n\n# Calculate positive predictive value (Precision)\nppv <- true_pos / (true_pos + false_pos) #<1>\n\n# Display result\nppv\n```\n:::\n\n\n\n1. PPV: proportion of predicted positives that are actually positive\n\nPPV tells us how reliable our positive predictions are.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_5' check='true'}\n```{webr}\n#| label: ex-5-gradethis\n#| exercise: ex_5\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n::: {#exr-npv}\n## Calculate negative predictive value\n:::\n\n:::: {.panel-tabset}\n## Problem\n\nCalculate the negative predictive value (NPV) for the PHQ-9 predictions.\n\n\n\n\n::: {.cell exercise='ex_6' envir='class_env'}\n```{webr}\n#| label: ex-6\n#| exercise: ex_6\n#| envir: class_env\n\n# Calculate negative predictive value\nnpv <- ____ / (____ + ____)\n\n# Display result\nnpv\n```\n:::\n\n\n\n\n## Hints\n\n::: {.hint exercise=\"ex_6\"}\n\nTo calculate NPV you need the *true negatives* and *false negatives*.\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_6\"}\n\n\n\n\n::: {.cell exercise='ex_6' solution='true'}\n```{webr}\n#| label: ex-6-solution\n#| exercise: ex_6\n#| solution: true\n\n# Calculate negative predictive value\nnpv <- true_neg / (true_neg + false_neg) #<1>\n\n# Display result\nnpv\n```\n:::\n\n\n\n1. NPV: proportion of predicted negatives that are actually negative\n\n:::\n\n\n\n\n::: {.cell exercise='ex_6' check='true'}\n```{webr}\n#| label: ex-6-gradethis\n#| exercise: ex_6\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n# Summary\n\nIn this lab, you learned:\n\n1. **Binary outcome creation**: How to create binary outcomes from continuous variables using meaningful cutoffs\n2. **Logistic regression**: How to fit classification models using `tidymodels`\n3. **Confusion matrices**: How to create and interpret confusion matrices\n4. **Classification metrics**: How to calculate sensitivity, specificity, PPV, and NPV using the confusion matrix\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}