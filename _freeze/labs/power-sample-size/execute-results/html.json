{
  "hash": "b36e7a53be59ba0719e59374cf644cb7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power Analysis and Sample Size Planning\"\n---\n\n\n\n\nIn this lab, we will learn about power analysis and sample size planning - fundamental concepts that help us design studies that can reliably detect meaningful effects. Think of power analysis as a way to evaluate the sensitivity of a statistical test, it helps you make informed decisions about your study design.\n\nPower analysis answers three important questions:\n\n- How many participants do I need for my study? (sample size planning)\n- What is the smallest effect I can reliably detect with my sample size?\n- How likely am I to detect an effect if it truly exists? (statistical power)\n\nThese questions are interconnected - changing one affects the others. Throughout this lab, you'll use R to explore these relationships and build intuition about how they work together.\n\n## Learning objectives\n\nBy the end of this lab, you will be able to:\n\n- Understand the four components of power analysis and how they relate to each other\n- Use R functions to calculate power and sample size\n- Distinguish between different types of hypothesis tests (superiority, non-inferiority, equivalence) and when to use each\n\nEach learning objective will be reinforced through hands-on R coding exercises that build your understanding step by step.\n\n::: callout-note\n## Why power analysis matters in biostatistics\n\nIn clinical research, inadequate sample sizes lead to:\n\n- **Underpowered studies**: Cannot detect clinically meaningful effects\n- **Wasted resources**: Time, money, and participant burden without informative results\n- **Ethical concerns**: Exposing participants to research without sufficient chance of meaningful findings\n:::\n\n# The fundamentals of power analysis\n\nBefore diving into R code, let's build intuition about power analysis using a visualization. The figure below shows the theoretical foundation that underlies all power calculations. Don't worry if it looks complex at first - we'll break down each component as we work through the lab.\n\n::: callout-note\n## Learning approach: Start simple\n\nIn this lab we uses simple, common assumptions to help you understand core concepts:\n\n- **Normal outcomes**: We assume data follows a normal distribution\n- **Independent observations**: Each participant's data is independent of others\n- **Equal variances**: Both groups have similar variability\n- **Standardized effect sizes**: We use Cohen's d, a common measure in psychology\n- **Two equal-sized groups**: Treatment vs. control with same sample sizes\n- **t-tests as foundation**: We start here because the concepts transfer to more complex analyses\n\nThese assumptions make the math manageable while you learn. Real-world studies often require more complex approaches, but the fundamental concepts remain the same.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(knitr)\n\n# =============================================================================\n# SIMULATION PARAMETERS\n# =============================================================================\n\n# Study design parameters\nsample_size_per_group <- 50\nnum_simulations <- 10000\nconfidence_level <- 0.95\nalpha_level <- 0.05\n\n# Effect size parameters\nnull_effect_size <- 0\nalternative_effect_size <- 0.5\n\n# =============================================================================\n# THEORETICAL DISTRIBUTION PARAMETERS\n# =============================================================================\n\n# Calculate standard error and degrees of freedom\nstandard_error <- sqrt((1^2 + 1^2) / sample_size_per_group)\ndegrees_freedom <- 2 * sample_size_per_group - 2\n\n# Distribution parameters for null and alternative hypotheses\nmu_null <- null_effect_size / standard_error # Mean under H0\nsigma_null <- 1 # SD under H0\nmu_alternative <- alternative_effect_size / standard_error # Mean under HA\nsigma_alternative <- 1 # SD under HA\n\n# Critical value for hypothesis test (two-tailed)\ncritical_value <- qnorm(1 - (alpha_level / 2), mu_null, sigma_null)\n\n# =============================================================================\n# CREATE THEORETICAL DISTRIBUTION CURVES\n# =============================================================================\n\n# Define plot range (4 standard deviations from means)\nplot_range_multiplier <- 4\nx_min_null <- mu_null - sigma_null * plot_range_multiplier\nx_max_null <- mu_null + sigma_null * plot_range_multiplier\nx_min_alt <- mu_alternative - sigma_alternative * plot_range_multiplier\nx_max_alt <- mu_alternative + sigma_alternative * plot_range_multiplier\n\n# Create x-axis sequence for plotting\nx_values <- seq(min(x_min_null, x_min_alt), max(x_max_null, x_max_alt), 0.01)\n\n# Generate theoretical distributions\ndensity_null <- dnorm(x_values, mu_null, sigma_null)\ndensity_alternative <- dnorm(x_values, mu_alternative, sigma_alternative)\n\n# Create data frames for plotting\ndf_null_hypothesis <- data.frame(x = x_values, y = density_null)\ndf_alternative_hypothesis <- data.frame(x = x_values, y = density_alternative)\n\n# =============================================================================\n# CREATE POLYGONS FOR STATISTICAL REGIONS\n# =============================================================================\n\n# Alpha region polygon (Type I error)\nalpha_polygon_data <- data.frame(\n  x = x_values,\n  y = pmin(density_null, density_alternative)\n)\nalpha_polygon_data <- alpha_polygon_data[\n  alpha_polygon_data$x >= critical_value,\n]\nalpha_polygon_data <- rbind(\n  c(critical_value, 0),\n  c(critical_value, dnorm(critical_value, mu_null, sigma_null)),\n  alpha_polygon_data\n)\n\n# Beta region polygon (Type II error)\nbeta_polygon_data <- df_alternative_hypothesis\nbeta_polygon_data <- beta_polygon_data[beta_polygon_data$x <= critical_value, ]\nbeta_polygon_data <- rbind(beta_polygon_data, c(critical_value, 0))\n\n# Power region polygon (1 - beta)\npower_polygon_data <- df_alternative_hypothesis\npower_polygon_data <- power_polygon_data[\n  power_polygon_data$x >= critical_value,\n]\npower_polygon_data <- rbind(power_polygon_data, c(critical_value, 0))\n\n# =============================================================================\n# COMBINE POLYGONS FOR PLOTTING\n# =============================================================================\n\n# Add polygon identifiers\nalpha_polygon_data$region_type <- \"alpha\"\nbeta_polygon_data$region_type <- \"beta\"\npower_polygon_data$region_type <- \"power\"\n\n\n# Create second alpha region (mirrored)\nalpha_polygon_mirrored <- alpha_polygon_data |>\n  mutate(x = -1 * x, region_type = \"alpha_mirrored\") |>\n  filter(x >= min(x_values))\n# Combine all polygon data\nall_polygons <- rbind(\n  alpha_polygon_data,\n  alpha_polygon_mirrored,\n  beta_polygon_data,\n  power_polygon_data\n)\n\n# Convert to factor with proper labels\nall_polygons$region_type <- factor(\n  all_polygons$region_type,\n  levels = c(\"power\", \"beta\", \"alpha\", \"alpha_mirrored\"),\n  labels = c(\"power\", \"beta\", \"alpha\", \"alpha2\")\n)\n\n# =============================================================================\n# DEFINE COLOR PALETTE\n# =============================================================================\n\n# Color palette for the visualization\nPALETTE <- list( # nolint: object_name_linter\n  # Hypothesis colors\n  hypothesis = c(\n    \"H0\" = \"black\",\n    \"HA\" = \"#981e0b\"\n  ),\n\n  # Statistical region colors\n  regions = c(\n    \"alpha\" = \"#0d6374\",\n    \"alpha2\" = \"#0d6374\",\n    \"beta\" = \"#be805e\",\n    \"power\" = \"#7cecee\"\n  ),\n\n  # Simulation curve colors\n  simulations = c(\n    \"null\" = \"green\",\n    \"alternative\" = \"red\"\n  )\n)\n\n# =============================================================================\n# CREATE POWER ANALYSIS VISUALIZATION\n# =============================================================================\n\npower_plot <- ggplot(\n  all_polygons,\n  aes(x, y, fill = region_type, group = region_type)\n) +\n  # Add filled polygons for statistical regions\n  geom_polygon(\n    data = df_null_hypothesis,\n    aes(\n      x, y,\n      color = \"H0\",\n      group = NULL,\n      fill = NULL\n    ),\n    linetype = \"dotted\",\n    fill = \"gray80\",\n    linewidth = 1,\n    alpha = 0.5,\n    show.legend = FALSE\n  ) +\n  geom_polygon(show.legend = FALSE, alpha = 0.8) +\n  geom_line(\n    data = df_alternative_hypothesis,\n    aes(x, y, color = \"HA\", group = NULL, fill = NULL),\n    linewidth = 1,\n    linetype = \"dashed\",\n    color = \"gray60\",\n    inherit.aes = FALSE\n  ) +\n  # Add critical value line\n  geom_vline(\n    xintercept = critical_value, linewidth = 1,\n    linetype = \"dashed\"\n  ) +\n  # Add annotations\n  annotate(\n    \"segment\",\n    x = 1.5,\n    xend = 2.2,\n    y = 0.05,\n    yend = 0.02,\n    arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    label = \"frac(alpha,2)\",\n    x = 1.3, y = 0.05,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"segment\",\n    x = 5,\n    xend = 3,\n    y = 0.18,\n    yend = 0.1,\n    arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    label = \"Power\",\n    x = 5, y = 0.2,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"text\",\n    label = \"H[0]\",\n    x = mu_null,\n    y = dnorm(mu_null, mu_null, sigma_null),\n    vjust = -0.5,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"text\",\n    label = \"H[a]\",\n    x = mu_alternative,\n    y = dnorm(mu_null, mu_null, sigma_null),\n    vjust = -0.5,\n    parse = TRUE, size = 8\n  ) +\n  # Customize colors and styling\n  scale_color_manual(\n    \"Hypothesis\",\n    values = PALETTE$hypothesis\n  ) +\n  scale_fill_manual(\n    \"Statistical Region\",\n    values = PALETTE$regions\n  ) +\n  labs(\n    x = \"Test statistic (z)\",\n    y = \"Density\",\n    title = \"Statistical Power Analysis Visualization\",\n    subtitle = \"Standard Two-Tailed Hypothesis Test (normal approximation)\"\n  ) +\n  ylim(c(0, max(dnorm(mu_null, mu_null, sigma_null) * 1.1))) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\n# Display the plot\nprint(power_plot)\n```\n\n::: {.cell-output-display}\n![Conceptual power regions: alpha (false positive), beta (false negative),\nand power, with critical value lines.](power-sample-size_files/figure-html/fig-power-nhst-plot-1.png){#fig-power-nhst-plot width=960}\n:::\n:::\n\n\n\n\n## The four key components\n\nLooking at the figure above, you can see that power analysis revolves around four interconnected components. This is like a statistical equation with four variables - if you know any three, you can solve for the fourth:\n\n1. **Effect size (d)**: How large is the difference we want to detect? In clinical research, this is often the smallest difference that would be clinically meaningful.\n\n2. **Sample size (n)**: How many participants do we need per group? This is often what we're trying to determine before starting data collection.\n\n3. **Significance level (α)**: The probability of falsely concluding there's an effect when there isn't one. Conventionally set at 0.05.\n\n4. **Power (1-β)**: The probability of detecting an effect if it truly exists. Conventionally set at 0.8 or 0.9.\n\nFor more complex designs beyond simple t-tests, you'll need to consider additional parameters, such as correlations between repeated measures in longitudinal studies.\n\n## Load packages\n\nLet's start by loading the R packages we'll use throughout this lab.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # For data manipulation and visualization\nlibrary(pwr) # For power analysis calculations\n```\n:::\n\n\n\n\nThe `pwr` package contains functions for different types of basic statistical tests. \n\n# Basic power analysis for t-tests\n\nNow that we understand the conceptual framework of power analysis, let's learn how to perform actual power calculations in R. We'll start with one of the most common scenarios: comparing means between two groups using t-tests.\n\n::: callout-note\n## Why use the t-test for power analysis examples?\n\nWe use the t-test to demonstrate power analysis because it is one of the simplest and most widely used statistical tests for comparing means between two groups. The t-test has well-understood mathematical properties, and its power calculations are straightforward and supported by standard R functions like `pwr.t.test()`. By focusing on the t-test, we can clearly illustrate the core concepts of power, effect size, sample size, and significance level without the added complexity of more advanced statistical models.\n:::\n\n## Understanding the `pwr.t.test()` functione\n\nThe `pwr.t.test()` function calculates power for t-tests. Let's start with a simple example:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate power when we know effect size, sample size, and alpha\npower_result <- pwr.t.test(\n  d = 0.5, # Medium effect size (Cohen's d)\n  n = 64, # Sample size per group\n  sig.level = 0.05, # Alpha level (Type I errors)\n  type = \"two.sample\", # Two independent groups\n  alternative = \"two.sided\" # Two-tailed test\n)\n\npower_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 64\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8014596\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n\n**Interpreting the result**: This output tells us that with 64 participants per group, we have about 80% power to detect a medium effect (d = 0.5). \n\n## Sample size calculation\n\nOften, the most practical question is: \"How many participants do I need?\" This is where power analysis becomes a planning tool. We simply omit the `n` parameter and let R calculate the required sample size for us:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many participants do we need for 80% power to detect d = 0.5?\nsample_size_result <- pwr.t.test(\n  d = 0.5, # Effect size we want to detect\n  power = 0.80, # Desired power level\n  sig.level = 0.05, # Alpha level\n  type = \"two.sample\", # Two independent groups\n  alternative = \"two.sided\" # Two-tailed test\n)\n\nsample_size_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 63.76561\n              d = 0.5\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n\nWe need approximately 64 participants per group (128 total) to have 80% power to detect a medium effect.\n\n## Effect size calculation\n\nSometimes you might have constraints on your sample size (due to budget, time, or participant availability) and want to know: \"What's the smallest effect I can reliably detect?\" This helps set realistic expectations for your study:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# What effect size can we detect with 50 participants per group?\neffect_size_result <- pwr.t.test(\n  n = 50, # Available sample size per group\n  power = 0.80, # Desired power level\n  sig.level = 0.05, # Alpha level\n  type = \"two.sample\", # Two independent groups\n  alternative = \"two.sided\" # Two-tailed test\n)\n\neffect_size_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 50\n              d = 0.565858\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n\nThis type of calculation helps you decide whether a study is worth conducting given your constraints, or whether you need to find ways to increase your sample size.\n\n# Visualizing power relationships\nVisualizations can help you understand the relationships between power, sample size, and effect size. These plots will help build intuition about how changes in one parameter affect the others.\n\n## Power curves (sample size vs power)\n\nPower curves are one of the most useful visualizations in research planning. They show you exactly how statistical power increases as you add more participants to your study. They help you find the sweet spot between adequate power and practical constraints.\n\nLet's create a power curve showing how power changes with sample size for a medium effect (d = 0.5).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a range of sample sizes\nsample_sizes <- seq(10, 100, by = 5)\n\n# Calculate power for each sample size (effect size = 0.5)\npower_data <- map(\n  sample_sizes,\n  \\(n) {\n    result <- pwr.t.test(\n      d = 0.5,\n      n = n,\n      sig.level = 0.05,\n      type = \"two.sample\",\n      alternative = \"two.sided\"\n    )\n    data.frame(\n      n = n,\n      power = result$power\n    )\n  }\n) |> list_rbind()\n\n# Create the plot\nggplot(power_data, aes(x = n, y = power)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_hline(\n    yintercept = 0.8,\n    linetype = \"dashed\",\n    color = \"red\"\n  ) +\n  geom_vline(\n    xintercept = 64,\n    linetype = \"dashed\",\n    color = \"red\"\n  ) +\n  labs(\n    title = \"Power Curve for Two-Sample t-test\",\n    subtitle = \"Effect size (d) = 0.5, α = 0.05\",\n    x = \"Sample size (n per group)\",\n    y = \"Power (1 - β)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(\n    limits = c(0, 1),\n    labels = scales::percent_format()\n  )\n```\n\n::: {.cell-output-display}\n![Power vs sample size for d=0.5 (two-sample t-test). Dashed lines mark ~64\nper group for 80% power.](power-sample-size_files/figure-html/fig-power-curve-1.png){#fig-power-curve width=672}\n:::\n:::\n\n\n\n\nThe dashed lines show that we need 64 participants per group to achieve 80% power.\n\n## Comparing different effect sizes\n\nLet's see how effect size affects the power curves:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data for multiple effect sizes\neffect_sizes <- c(0.2, 0.5, 0.8)\nsample_sizes <- seq(10, 150, by = 5)\n\npower_comparison <- expand_grid(\n  n = sample_sizes,\n  d = effect_sizes\n) |>\n  mutate(\n    power = map2_dbl(\n      d, n,\n      \\(d, n) {\n        pwr.t.test(\n          d = d,\n          n = n,\n          sig.level = 0.05,\n          type = \"two.sample\",\n          alternative = \"two.sided\"\n        )$power\n      }\n    ),\n    effect_label = factor(\n      d,\n      levels = c(0.2, 0.5, 0.8),\n      labels = c(\"Small (d=0.2)\", \"Medium (d=0.5)\", \"Large (d=0.8)\")\n    )\n  )\n\n# Create the comparison plot\nggplot(power_comparison, aes(x = n, y = power, color = effect_label)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", alpha = 0.7) +\n  labs(\n    title = \"Power Curves for Different Effect Sizes\",\n    subtitle = \"Two-sample t-test, α = 0.05\",\n    x = \"Sample size (n per group)\",\n    y = \"Power (1 - β)\",\n    color = \"Effect size\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(limits = c(0, 1), labels = scales::percent_format()) +\n  scale_color_viridis_d()\n```\n\n::: {.cell-output-display}\n![Power curves for small, medium, and large effects; larger effects require\nsmaller n for the same power.](power-sample-size_files/figure-html/fig-effect-size-comparison-1.png){#fig-effect-size-comparison width=672}\n:::\n:::\n\n\n\n\nUnsurprisingly, the size of the effect you're trying to detect dramatically impacts your sample size needs. However, in clinical studies this hypothetical effect size is usually based on the minimum clinically meaningful effect size.\n\n# Understanding power through simulation\n\nWhile mathematical formulas give us precise power calculations, simulations help us truly understand what these numbers mean in practice. When we say a study has \"80% power\", what does that actually look like when we run the study many times?\n\nThink of simulation as running thousands of virtual experiments. By doing this, we can see power in action: out of 1000 studies with 80% power, approximately 800 will detect the effect and 200 will miss it. \n\nThis section introduces you to key programming concepts - writing functions, using loops, and generating random data. Even if you have no intention of becoming a programmer, these concepts can be useful to know about.\n\n::: callout-note\n## Why simulate?\n\nSimulations help us understand:\n\n- What \"80% power\" actually means in practice\n- Why sometimes we get significant results and sometimes we don't\n- How sample size and effect size affect our chances of success\n- R programming concepts: functions, loops, and data generation\n- For complex designs simulation is usually the only way - or the simplest way - to calculate power\n:::\n\n## Creating a data simulation function\n\nBefore we can simulate thousands of studies, we need a way to generate realistic data for each virtual study. Let's create a function that produces data that mimics what you might see in a real two-group comparison study.\n\nThis function will be your \"fake data factory\" - each time you call it, it creates a new study with randomly generated participants:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to simulate data for a two-group study\nsimulate_study <- function(n_per_group, effect_size, sd = 1) {\n  # Generate control group data (mean = 0)\n  control_group <- rnorm(n_per_group, mean = 0, sd = sd)\n\n  # Generate treatment group data (mean = effect_size)\n  treatment_group <- rnorm(n_per_group, mean = effect_size, sd = sd)\n\n  # Combine into a data frame\n  study_data <- data.frame(\n    score = c(control_group, treatment_group),\n    group = factor(\n      rep(c(0, 1), each = n_per_group),\n      labels = c(\"Control\", \"Treatment\")\n    )\n  )\n\n  return(study_data)\n}\n\n# Test our function with a concrete example\nset.seed(123) # For reproducible results across runs\nexample_study <- simulate_study(n_per_group = 30, effect_size = 0.5)\n\n# Look at the first few rows to see the structure\nhead(example_study)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        score   group\n1 -0.56047565 Control\n2 -0.23017749 Control\n3  1.55870831 Control\n4  0.07050839 Control\n5  0.12928774 Control\n6  1.71506499 Control\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate means for each group to verify our effect size\nexample_study |>\n  summarize(\n    mean_score = mean(score),\n    .by = group\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      group  mean_score\n1   Control -0.04710376\n2 Treatment  0.67833834\n```\n\n\n:::\n:::\n\n\n\n\n::: callout-note\n## Understanding the output\nNotice how the function creates a dataset with 60 participants (30 per group) and the Treatment group has a higher mean score than the Control group. The difference between means approximates our specified effect size (0.5), though it won't be exactly 0.5 due to random sampling variability.\n\nThis variability is crucial to understand, it's why we need many participants and why we use statistical tests to account for uncertainty.\n:::\n\n\n## Running a single simulated study\n\nNow let's analyze one simulated study and see if we get a significant result:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simulate one study\nset.seed(456)\nstudy_data <- simulate_study(n_per_group = 30, effect_size = 0.5)\n\n# Run a t-test\nt_test_result <- t.test(score ~ group, data = study_data, var.equal = TRUE)\nt_test_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  score by group\nt = -1.6249, df = 58, p-value = 0.1096\nalternative hypothesis: true difference in means between group Control and group Treatment is not equal to 0\n95 percent confidence interval:\n -0.9472288  0.0984278\nsample estimates:\n  mean in group Control mean in group Treatment \n              0.2317430               0.6561435 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract key information\np_value <- t_test_result$p.value\neffect_detected <- p_value < 0.05\n\ncat(\"P-value:\", round(p_value, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nP-value: 0.1096 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Significant result (p < 0.05):\", effect_detected, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSignificant result (p < 0.05): FALSE \n```\n\n\n:::\n:::\n\n\n\n\nIn this single study, we did not detect a significant effect. This illustrates an important point - even when there's a true effect and adequate power, individual studies can still \"miss\" the effect due to random sampling. But what happens when we run many studies with the same design?\n\n## Simulating many studies to understand power\n\nWe'll run 10,000 virtual studies with identical designs and count how many detect a significant effect. This proportion directly shows us the empirical power - and it should match our theoretical calculations.\n\nThe code below creates a simulation function that tracks multiple aspects of each study:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to run many simulated studies (including confidence intervals)\nrun_power_simulation <- function(\n    n_per_group,\n    effect_size,\n    n_simulations = 1000,\n    alpha = 0.05,\n    conf_level = 0.95) {\n  # Store results for each simulation\n  results <- data.frame(\n    simulation = 1:n_simulations,\n    p_value = numeric(n_simulations),\n    ci_lower = numeric(n_simulations),\n    ci_upper = numeric(n_simulations),\n    significant = logical(n_simulations),\n    effect = numeric(n_simulations),\n    se = numeric(n_simulations),\n    t_value = numeric(n_simulations)\n  )\n\n  # Run many simulated studies\n  for (i in 1:n_simulations) {\n    # Simulate data\n    study_data <- simulate_study(n_per_group, effect_size)\n\n    # Run t-test (automatically gives us p-value and CI)\n    test_result <- t.test(\n      score ~ relevel(group, ref = \"Treatment\"),\n      data = study_data,\n      conf.level = conf_level,\n      var.equal = TRUE\n    )\n\n    # Store results\n    results$p_value[i] <- test_result$p.value\n    results$ci_lower[i] <- test_result$conf.int[1]\n    results$ci_upper[i] <- test_result$conf.int[2]\n    results$significant[i] <- test_result$p.value < alpha\n    results$effect[i] <- test_result$estimate |>\n      rev() |>\n      diff()\n    results$se[i] <- test_result$stderr\n    results$t_value[i] <- test_result$statistic\n  }\n\n  return(results)\n}\n\n# Run simulation with medium effect size\nset.seed(787)\nsim_results <- run_power_simulation(\n  n_per_group = 50,\n  effect_size = 0.5,\n  n_simulations = 10000\n)\n```\n:::\n\n\n\n\nSince we simulated data where there truly is an effect (alternative hypothesis), **empirical power** is simply the proportion of studies that correctly detected this effect:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nempirical_power <- mean(sim_results$p_value < 0.05)\nempirical_power\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6964\n```\n\n\n:::\n:::\n\n\n\n\nOut of 10,000 simulated studies, 69.6% found a statistically significant result. This is our empirical power - what actually happened when we \"ran\" thousands of studies.\n\nNow let's compare our empirical results with the theoretical power we calculated earlier:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheoretical_power <- pwr.t.test(\n  n = 50,\n  d = 0.5,\n  sig.level = 0.05,\n  type = \"two.sample\"\n)$power\ntheoretical_power\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6968934\n```\n\n\n:::\n:::\n\n\n\n\nThe simulation results (69.6%) are very close to the theoretical power (69.7%). The small difference is expected due to simulation error.\n\n::: {.callout-note collapse=\"true\"}\n## Pedagogical choice: For-loops vs functional programming\n\nWe're using **for-loops** here because they tend to be easier to understand when learning. You can see exactly what happens in each iteration. However, experienced R users often prefer **functional programming** approaches that are more concise.\n\nHere's the same function using more advanced R techniques (including parallel processing):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Alternative: Using map() |> list_rbind() from purrr (more advanced)\n# and parallel processing with mirai\nlibrary(mirai)\nrun_power_sim_functional <- function(\n    n_per_group,\n    effect_size,\n    n_simulations = 1000,\n    alpha = 0.05) {\n  map(\n    1:n_simulations,\n    in_parallel( # nolint: object_usage_linter\n      \\(i) {\n        control_group <- rnorm(n_per_group, mean = 0, sd = 1)\n        treatment_group <- rnorm(n_per_group, mean = effect_size, sd = 1)\n\n        study_data <- data.frame(\n          score = c(control_group, treatment_group),\n          group = rep(c(\"Control\", \"Treatment\"), each = n_per_group)\n        )\n\n        test_result <- t.test(score ~ group, data = study_data)\n\n        tibble::tibble(\n          simulation = i,\n          p_value = test_result$p.value,\n          ci_lower = test_result$conf.int[1],\n          ci_upper = test_result$conf.int[2],\n          significant = test_result$p.value < alpha\n        )\n      },\n      n_per_group = n_per_group,\n      effect_size = effect_size,\n      alpha = alpha\n    )\n  ) |> list_rbind()\n}\n# Run on 8 CPU threads in parallel\ndaemons(8)\nrun_power_sim_functional(\n  n_per_group = sample_size_per_group,\n  effect_size = 0.5,\n  n_simulations = 10000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 × 5\n   simulation  p_value ci_lower ci_upper significant\n        <int>    <dbl>    <dbl>    <dbl> <lgl>      \n 1          1 0.00868    -0.993  -0.148  TRUE       \n 2          2 0.105      -0.634   0.0607 FALSE      \n 3          3 0.756      -0.492   0.358  FALSE      \n 4          4 0.195      -0.639   0.132  FALSE      \n 5          5 0.00202    -1.12   -0.257  TRUE       \n 6          6 0.973      -0.392   0.379  FALSE      \n 7          7 0.100      -0.789   0.0704 FALSE      \n 8          8 0.000495   -1.11   -0.322  TRUE       \n 9          9 0.000787   -1.15   -0.313  TRUE       \n10         10 0.00398    -1.04   -0.204  TRUE       \n# ℹ 9,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\ndaemons(0)\n```\n:::\n\n\n\n\n:::\n\n## Visualizing simulation results\n\nLet's visualize what happened in our 10,000 studies. Let's overlay the simulation results on the theoretical null and alternative distributions. Before we can do this, we need to generate data from the null model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_results <- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = 0,\n  n_simulations = 10000\n)\n```\n:::\n\n\n\n\nThen we can plot the theoretical and empirical H0 and H1 distributions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower_plot +\n  # Add empirical histograms from simulations\n  geom_histogram(\n    data = null_results,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"gray30\",\n    alpha = 0.2,\n    color = \"gray30\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  ) +\n  geom_histogram(\n    data = sim_results,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"#27AE60\",\n    alpha = 0.2,\n    color = \"#1E8449\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  )\n```\n\n::: {.cell-output-display}\n![Empirical histograms from simulations (null and alternative) overlaid on\ntheoretical curves.](power-sample-size_files/figure-html/power-simulation-nhst-plot-1.png){width=960}\n:::\n:::\n\n\n\n\nWe can see that the simulation results under the null and alternative models (the histograms) are very close to the theoretical distributions.\n\nAnother way to visualize the simulation results is to plot the distribution of p-values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot distribution of p-values\nggplot(sim_results, aes(x = p_value)) +\n  geom_bar(aes(fill = significant), alpha = 0.7) +\n  labs(\n    title = \"Distribution of P-values from 10,000 Simulated Studies\",\n    subtitle = paste(\n      \"Effect size = 0.5, n = 50 per group, Power =\",\n      round(empirical_power, 3)\n    ),\n    x = \"P-value\",\n    y = \"Count\",\n    fill = \"Significant\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"FALSE\" = \"gray80\",\n      \"TRUE\" = PALETTE$regions[[\"power\"]]\n    )\n  ) +\n  scale_x_binned(n.breaks = 20) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Distribution of p-values from 10,000 Simulated Studies, showing\nsignificant results.](power-sample-size_files/figure-html/fig-visualize-simulations-1.png){#fig-visualize-simulations width=960}\n:::\n:::\n\n\n\n\nLastly, we can visualize power by plotting the 95% confidence intervals. Power is the proportion of confidence intervals that exclude the null effect.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_results <- sim_results |>\n  mutate(\n    ci_significant = (ci_lower < 0 & ci_upper < 0) |\n      (ci_lower > 0 & ci_upper > 0)\n  )\n\nsim_results |>\n  summarize(\n    power_ci = mean(ci_significant),\n    power_pvalue = mean(p_value < 0.05)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  power_ci power_pvalue\n1   0.6964       0.6964\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create CI coverage plot for superiority trial\nplot_ci_coverage <- function(\n    sim_data,\n    true_effect = 0,\n    null_effect = 0,\n    title = \"Confidence Interval Coverage\") {\n  # Determine if CI contains true effect\n  sim_data$covers_truth <- (sim_data$ci_lower <= true_effect) &\n    (sim_data$ci_upper >= true_effect)\n\n  # Sort by significance for better visualization\n  sim_data <- sim_data |>\n    arrange(ci_significant) |>\n    mutate(plot_order = row_number())\n\n  ggplot(sim_data, aes(y = plot_order)) + # nolint: object_usage_linter\n    geom_segment(\n      aes(x = ci_lower, xend = ci_upper, color = ci_significant), # nolint: object_usage_linter\n      alpha = 0.7,\n      linewidth = 0.8\n    ) +\n    geom_vline(\n      xintercept = true_effect,\n      color = \"gray50\",\n      linetype = \"dashed\",\n    ) +\n    geom_vline(\n      xintercept = null_effect,\n      color = \"black\",\n      linetype = \"solid\",\n      linewidth = 1\n    ) +\n    scale_color_manual(\n      values = c(\"TRUE\" = \"#27AE60\", \"FALSE\" = PALETTE$regions[[\"beta\"]]),\n      labels = c(\"TRUE\" = \"Significant\", \"FALSE\" = \"Non-significant\"),\n      name = \"Result\"\n    ) +\n    labs(\n      title = title,\n      subtitle = paste(\n        \"True effect =\", true_effect, \"| Power =\",\n        round(mean(sim_data$ci_significant), 3)\n      ),\n      x = \"Effect Size (95% Confidence Interval)\",\n      y = \"Study Number\"\n    ) +\n    theme_minimal() +\n    theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())\n}\nsim_results |>\n  sample_n(200) |>\n  plot_ci_coverage(true_effect = 0.5, title = \"95% Confidence Interval Power\")\n```\n\n::: {.cell-output-display}\n![Confidence interval coverage: green lines meet success criteria;\nbrown/red lines do not. Vertical lines mark true effect and null effect.](power-sample-size_files/figure-html/fig-ci-coverage-plot-1.png){#fig-ci-coverage-plot width=576}\n:::\n:::\n\n\n\n\n::: callout-tip\n## Key insights from the simulation\n\n1. **Power**: About 70% of our simulated studies detected an effect.\n2. **P-value distribution**: As power increases, p-values tend to cluster near zero, but some studies still produce larger p-values by chance\n3. **The 20% that \"fail\"**: Even with adequate power, studies will miss a real effect due to sampling variability\n4. **Why replication matters**: Any single study might be in that unlucky 20%, which is why we need multiple studies to build scientific confidence\n:::\n\n## What happens when there's no effect? (Type I error)\n\n::: callout-important\n## Understanding Type I and Type II errors\n\n- **Type I error (α)**: Finding an effect when there isn't one (false positive)\n- **Type II error (β)**: Missing an effect when there is one (false negative)\n- **Power = 1 - β**: Probability of correctly detecting a true effect\n\nNow let's see these concepts in action through simulation.\n:::\n\nLet's also check the results where we simulated studies from the null hypothesis (no difference between groups):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate Type I errors\ntype_i_error <- mean(null_results$significant)\n\ncat(\"Type I errors:\", round(type_i_error, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nType I errors: 0.048 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Expected Type I errors:\", 0.05, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected Type I errors: 0.05 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualize\nbreaks <- seq(\n  min(null_results$p_value),\n  max(null_results$p_value),\n  length.out = 21\n)\nggplot(null_results, aes(x = p_value)) +\n  geom_bar(aes(fill = significant), alpha = 0.7) +\n  labs(\n    title = \"P-values When There's No True Effect\",\n    subtitle = paste(\"Type I errors =\", round(type_i_error, 3)),\n    x = \"P-value\",\n    y = \"Count\",\n    fill = \"Significant\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"FALSE\" = \"gray80\",\n      \"TRUE\" = PALETTE$regions[[\"alpha\"]]\n    )\n  ) +\n  scale_x_binned(n.breaks = 20) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Distribution of p-values when there is no true effect from 10,000\nsimulated studies.\n](power-sample-size_files/figure-html/fig-null-simulation-pcurve-plot-1.png){#fig-null-simulation-pcurve-plot width=960}\n:::\n:::\n\n\n\n\n\n# Understanding different types of hypothesis tests\n\nSo far, we've focused on **superiority tests** - the most common type of analysis where you're testing whether one treatment is better than another. But clinical research often involves different questions that require different statistical approaches. This section introduces you to three types of hypothesis tests, each designed to answer different research questions:\n\n\n\n\n::: {#tbl-trial-comparison-table .cell}\n::: {.cell-output-display}\n\n\nTable: Comparison of Trial Types\n\n|Trial_Type      |Question                                             |Hypothesis_Test            |Success_Criteria           |\n|:---------------|:----------------------------------------------------|:--------------------------|:--------------------------|\n|Superiority     |Is treatment better than control?                    |Two-sided t-test           |CI excludes 0              |\n|Non-inferiority |Is treatment not worse than control (within margin)? |One-sided t-test vs margin |CI lower bound > -margin   |\n|Equivalence     |Are treatments equivalent (within ±margin)?          |Two one-sided tests (TOST) |CI entirely within ±margin |\n\n\n:::\n:::\n\n\n\n\n## Superiority trials (standard approach)\n\nSuperiority trials are what most people think of as \"typical\" research studies. The goal is straightforward: prove that one treatment is better than another (or better than no treatment). This is what we've been practicing throughout this lab.\n\n**Two-tailed superiority test** (tests for any difference):\n\n$$\n\\begin{aligned}\n&\\text{Null hypothesis } (H_0): && d = 0 \\\\\n&\\text{Alternative hypothesis } (H_1): && d \\neq 0\n\\end{aligned}\n$$\n\n**One-tailed superiority test** (tests for improvement in specific direction):\n\n$$\n\\begin{aligned}\n&\\text{Null hypothesis } (H_0): && d \\leq 0 \\\\\n&\\text{Alternative hypothesis } (H_1): && d > 0\n\\end{aligned}\n$$\n\nwhere $d$ is the effect size (standardized difference between treatment and control groups).\n\n## Non-inferiority trials\n\nNon-inferiority trials address a different question: \"Is this new treatment not worse than the standard treatment by more than an acceptable margin?\"\n\n**Why non-inferiority matters**: Imagine you have a standard antidepressant that works well but has significant side effects. A new drug with fewer side effects might be valuable even if it's slightly less effective - as long as it's not *too much* worse.\n\n\n::: callout-note\n## Non-inferiority margin\n\nThe non-inferiority margin is the largest difference we would still consider acceptable. For example, if standard treatment reduces symptoms by 10 points, we might accept that new treatment is non-inferior if it's no more than 2 points worse.\n:::\n\nFor non-inferiority, we typically use one-sided tests and need to specify the non-inferiority margin:\n\n$$\n\\begin{aligned}\n&\\text{Null hypothesis } (H_0): && d \\leq -\\Delta \\\\\n&\\text{Alternative hypothesis } (H_1): && d > -\\Delta\n\\end{aligned}\n$$\n\nwhere $\\Delta$ is the non-inferiority margin and $d$ is the effect size.\n\nWe will: (1) build intuition with a conceptual non-inferiority figure, (2) check that simulated study statistics line up with the theory, (3) view confidence-interval criteria directly, and (4) compare empirical power with analytical power.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# =============================================================================\n# SIMULATION PARAMETERS\n# =============================================================================\n\n\n# Study design parameters\nsample_size_per_group <- 80\nnum_simulations <- 10000\nconfidence_level <- 0.9\nalpha_level <- 0.05\n\n# Effect size parameters\nnull_effect_size <- -0.2\nalternative_effect_size <- 0.1\n\n# =============================================================================\n# RUN POWER SIMULATIONS\n# =============================================================================\n\n# Simulation under alternative hypothesis (with effect)\nsim_alternative_hypothesis <- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = alternative_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# Simulation under null hypothesis (no effect)\nsim_null_hypothesis <- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = null_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# =============================================================================\n# THEORETICAL DISTRIBUTION PARAMETERS\n# =============================================================================\n\n# Calculate standard error and degrees of freedom\nstandard_error <- sqrt((1^2 + 1^2) / sample_size_per_group)\ndegrees_freedom <- 2 * sample_size_per_group - 2\n\n# Distribution parameters for null and alternative hypotheses\nmu_null <- null_effect_size / standard_error # Mean under H0\nsigma_null <- 1 # SD under H0\nmu_alternative <- alternative_effect_size / standard_error # Mean under HA\nsigma_alternative <- 1 # SD under HA\n\n# Critical value for hypothesis test (two-tailed)\ncritical_value <- qnorm(1 - alpha_level, mu_null, sigma_null)\n\n# =============================================================================\n# CREATE THEORETICAL DISTRIBUTION CURVES\n# =============================================================================\n\n# Define plot range (4 standard deviations from means)\nplot_range_multiplier <- 4\nx_min_null <- mu_null - sigma_null * plot_range_multiplier\nx_max_null <- mu_null + sigma_null * plot_range_multiplier\nx_min_alt <- mu_alternative - sigma_alternative * plot_range_multiplier\nx_max_alt <- mu_alternative + sigma_alternative * plot_range_multiplier\n\n# Create x-axis sequence for plotting\nx_values <- seq(min(x_min_null, x_min_alt), max(x_max_null, x_max_alt), 0.01)\n\n# Generate theoretical distributions\ndensity_null <- dnorm(x_values, mu_null, sigma_null)\ndensity_alternative <- dnorm(x_values, mu_alternative, sigma_alternative)\n\n# Create data frames for plotting\ndf_null_hypothesis <- data.frame(x = x_values, y = density_null)\ndf_alternative_hypothesis <- data.frame(x = x_values, y = density_alternative)\n\n# =============================================================================\n# CREATE POLYGONS FOR STATISTICAL REGIONS\n# =============================================================================\n\n# Alpha region polygon (Type I error)\nalpha_polygon_data <- data.frame(\n  x = x_values,\n  y = pmin(density_null, density_alternative)\n)\nalpha_polygon_data <- alpha_polygon_data[\n  alpha_polygon_data$x >= critical_value,\n]\nalpha_polygon_data <- rbind(\n  c(critical_value, 0),\n  c(critical_value, dnorm(critical_value, mu_null, sigma_null)),\n  alpha_polygon_data\n)\n\n# Beta region polygon (Type II error)\nbeta_polygon_data <- df_alternative_hypothesis\nbeta_polygon_data <- beta_polygon_data[beta_polygon_data$x <= critical_value, ]\nbeta_polygon_data <- rbind(beta_polygon_data, c(critical_value, 0))\n\n# Power region polygon (1 - beta)\npower_polygon_data <- df_alternative_hypothesis\npower_polygon_data <- power_polygon_data[\n  power_polygon_data$x >= critical_value,\n]\npower_polygon_data <- rbind(power_polygon_data, c(critical_value, 0))\n\n# =============================================================================\n# COMBINE POLYGONS FOR PLOTTING\n# =============================================================================\n\n# Add polygon identifiers\nalpha_polygon_data$region_type <- \"alpha\"\nbeta_polygon_data$region_type <- \"beta\"\npower_polygon_data$region_type <- \"power\"\n\n# Combine all polygon data\nall_polygons <- rbind(\n  alpha_polygon_data,\n  beta_polygon_data,\n  power_polygon_data\n)\n\n# Convert to factor with proper labels\nall_polygons$region_type <- factor(\n  all_polygons$region_type,\n  levels = c(\"power\", \"beta\", \"alpha\"),\n  labels = c(\"power\", \"beta\", \"alpha\")\n)\n\n# =============================================================================\n# DEFINE COLOR PALETTE\n# =============================================================================\n\n# Color palette for the visualization\nPALETTE <- list( # nolint: object_name_linter\n  # Hypothesis colors\n  hypothesis = c(\n    \"H0\" = \"black\",\n    \"HA\" = \"#981e0b\"\n  ),\n\n  # Statistical region colors\n  regions = c(\n    \"alpha\" = \"#0d6374\",\n    \"alpha2\" = \"#0d6374\",\n    \"beta\" = \"#be805e\",\n    \"power\" = \"#7cecee\"\n  ),\n\n  # Simulation curve colors\n  simulations = c(\n    \"null\" = \"green\",\n    \"alternative\" = \"red\"\n  )\n)\n\n# =============================================================================\n# CREATE POWER ANALYSIS VISUALIZATION\n# =============================================================================\n\npower_plot <- ggplot(\n  all_polygons,\n  aes(x, y, fill = region_type, group = region_type)\n) +\n  # Add filled polygons for statistical regions\n  geom_polygon(\n    data = df_null_hypothesis,\n    aes(\n      x, y,\n      color = \"H0\",\n      group = NULL,\n      fill = NULL\n    ),\n    linetype = \"dotted\",\n    fill = \"gray80\",\n    linewidth = 1,\n    alpha = 0.5,\n    show.legend = FALSE\n  ) +\n  geom_polygon(show.legend = FALSE, alpha = 0.8) +\n  geom_line(\n    data = df_alternative_hypothesis,\n    aes(x, y, color = \"HA\", group = NULL, fill = NULL),\n    linewidth = 1,\n    linetype = \"dashed\",\n    color = \"gray60\",\n    inherit.aes = FALSE\n  ) +\n  # Add critical value line\n  geom_vline(\n    xintercept = critical_value, linewidth = 1,\n    linetype = \"dashed\"\n  ) +\n  # Add annotations\n  annotate(\n    \"segment\",\n    x = 0.2,\n    xend = 0.8,\n    y = 0.05,\n    yend = 0.02,\n    arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    label = \"alpha\",\n    x = 0, y = 0.05,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"segment\",\n    x = 3,\n    xend = 2,\n    y = 0.18,\n    yend = 0.1,\n    arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1\n  ) +\n  annotate(\n    \"text\",\n    label = \"Power\",\n    x = 3, y = 0.2,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"text\",\n    label = \"H[0]\",\n    x = mu_null,\n    y = dnorm(mu_null, mu_null, sigma_null),\n    vjust = -0.5,\n    parse = TRUE, size = 8\n  ) +\n  annotate(\n    \"text\",\n    label = \"H[a]\",\n    x = mu_alternative,\n    y = dnorm(mu_null, mu_null, sigma_null),\n    vjust = -0.5,\n    parse = TRUE, size = 8\n  ) +\n  # Customize colors and styling\n  scale_color_manual(\n    \"Hypothesis\",\n    values = PALETTE$hypothesis\n  ) +\n  scale_fill_manual(\n    \"Statistical Region\",\n    values = PALETTE$regions\n  ) +\n  labs(\n    x = \"Test statistic (z)\",\n    y = \"Density\",\n    title = \"Statistical Power Analysis Visualization\",\n    subtitle = \"Standard Non-Inferiority Hypothesis Test (normal approximation)\"\n  ) +\n  ylim(c(0, max(dnorm(mu_null, mu_null, sigma_null) * 1.1))) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\npower_plot\n```\n\n::: {.cell-output-display}\n![Non-inferiority conceptual plot: one-sided alpha region and power relative\nto the NI margin.](power-sample-size_files/figure-html/fig-non-inferiority-nhst-plot-1.png){#fig-non-inferiority-nhst-plot width=960}\n:::\n:::\n\n\n\n\nBecause non-inferiority can also be assessed via confidence intervals, we visualize the CI criterion directly: success occurs when the lower bound of the confidence interval lies above the negative margin. This plot makes it clear why detecting non-inferiority requires careful consideration of the margin and effect size.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create ni_sim with CI significance for non-inferiority\nni_sim <- sim_alternative_hypothesis\nni_sim$ci_significant <- ni_sim$ci_lower > -0.2\n\nni_sim |>\n  sample_n(200) |>\n  plot_ci_coverage(\n    true_effect = 0.1,\n    null_effect = -0.2,\n    title = \"Non-inferiority\"\n  )\n```\n\n::: {.cell-output-display}\n![Non-inferiority CI visualization: green intervals exclude values worse\nthan the margin.](power-sample-size_files/figure-html/fig-ni-ci-coverage-plot-1.png){#fig-ni-ci-coverage-plot width=576}\n:::\n:::\n\n\n\n\nFinally, we compare empirical power from simulation with the theoretical power. They should agree up to simulation error, which validates both our code and our understanding of the test.\n\n\n\n\n::: {#tbl-non-inferiority-power-comparison .cell tbl-cap='Comparison of empirical power from simulation with the theoretical power\nfor non-inferiority test.'}\n\n```{.r .cell-code}\n# Calculate theoretical power\ntheoretical_power <- pwr.t.test(\n  d = 0.3, # margin of 0.2 + 0.1 true effect\n  n = 80,\n  sig.level = 0.05,\n  type = \"two.sample\",\n  alternative = \"greater\"\n)\n\nni_sim |>\n  summarize(\n    power_ci = mean(ci_significant),\n    power_theoretical = theoretical_power$power\n  ) |>\n  pivot_longer(\n    cols = everything(),\n    names_to = \"method\",\n    values_to = \"power\"\n  ) |>\n  gt() |>\n  fmt_percent(\n    columns = \"power\",\n    decimals = 0\n  )\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"pddkqglpiy\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#pddkqglpiy table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#pddkqglpiy thead, #pddkqglpiy tbody, #pddkqglpiy tfoot, #pddkqglpiy tr, #pddkqglpiy td, #pddkqglpiy th {\n  border-style: none;\n}\n\n#pddkqglpiy p {\n  margin: 0;\n  padding: 0;\n}\n\n#pddkqglpiy .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#pddkqglpiy .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#pddkqglpiy .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#pddkqglpiy .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#pddkqglpiy .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#pddkqglpiy .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#pddkqglpiy .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#pddkqglpiy .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#pddkqglpiy .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#pddkqglpiy .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#pddkqglpiy .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#pddkqglpiy .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#pddkqglpiy .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#pddkqglpiy .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#pddkqglpiy .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pddkqglpiy .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#pddkqglpiy .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#pddkqglpiy .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#pddkqglpiy .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pddkqglpiy .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#pddkqglpiy .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pddkqglpiy .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#pddkqglpiy .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pddkqglpiy .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#pddkqglpiy .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#pddkqglpiy .gt_left {\n  text-align: left;\n}\n\n#pddkqglpiy .gt_center {\n  text-align: center;\n}\n\n#pddkqglpiy .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#pddkqglpiy .gt_font_normal {\n  font-weight: normal;\n}\n\n#pddkqglpiy .gt_font_bold {\n  font-weight: bold;\n}\n\n#pddkqglpiy .gt_font_italic {\n  font-style: italic;\n}\n\n#pddkqglpiy .gt_super {\n  font-size: 65%;\n}\n\n#pddkqglpiy .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#pddkqglpiy .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#pddkqglpiy .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#pddkqglpiy .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#pddkqglpiy .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#pddkqglpiy .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#pddkqglpiy .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#pddkqglpiy .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#pddkqglpiy div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"method\">method</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"power\">power</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"method\" class=\"gt_row gt_left\">power_ci</td>\n<td headers=\"power\" class=\"gt_row gt_right\">58%</td></tr>\n    <tr><td headers=\"method\" class=\"gt_row gt_left\">power_theoretical</td>\n<td headers=\"power\" class=\"gt_row gt_right\">60%</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n\nNote that non-inferiority trials often require larger sample sizes than superiority trials because we're trying to rule out a smaller difference.\n\n## Equivalence trials\n\nEquivalence trials ask the most stringent question: \"Are these two treatments essentially equally effective?\" Unlike non-inferiority (which only rules out being worse), equivalence must rule out being either significantly better *or* significantly worse.\n\n**When equivalence matters**: Equivalence trials are useful when you want to demonstrate that the there is no relevant difference between two treatments. For example, demonstrating that psychodynamic therapy is equivalent to cognitive-behavioral therapy.\n\n$$\n\\begin{aligned}\n&\\text{Null hypothesis } (H_0): && d \\leq -\\Delta \\quad \\text{or} \\quad d \\geq \\Delta \\\\\n&\\text{Alternative hypothesis } (H_1): && -\\Delta < d < \\Delta\n\\end{aligned}\n$$\n\nwhere $\\Delta$ is the equivalence margin and $d$ is the effect size.\n\nWe will: (1) build intuition with a conceptual TOST figure, (2) check that simulated study statistics line up with the theory, (3) view confidence-interval criteria directly, and (4) compare empirical power with analytical TOST power.\n\n::: callout-note\n## Equivalence vs Non-inferiority\n\n- **Non-inferiority**: \"New treatment is not worse than standard (within margin)\"\n- **Equivalence**: \"New treatment is similar to standard (within ± margin)\"\n\nEquivalence is more stringent - we need to rule out both directions of difference.\n:::\n\nFor equivalence trials, we need to calculate power for both one-sided tests. The figure below shows the two one-sided critical regions (at ±margin, translated to the test-statistic scale) and the central region where we have power to conclude equivalence.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# =============================================================================\n# SIMULATION PARAMETERS\n# =============================================================================\n\n# Study design parameters\nsample_size_per_group <- 500\nnum_simulations <- 10000\nconfidence_level <- 0.9\nalpha_level <- 0.05\n\n# Effect size parameters\nequivalence_bound <- 0.2\nnull_effect_size <- 0\nalternative_effect_size <- 0.2\n\n# =============================================================================\n# RUN POWER SIMULATIONS\n# =============================================================================\n\n# Simulation under alternative hypothesis (effect within equivalence bounds)\nsim_alternative_hypothesis <- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = null_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# Simulation under first null hypothesis (effect at positive bound)\nsim_null_hypothesis_pos <- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = alternative_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# Simulation under second null hypothesis (effect at negative bound)\nsim_null_hypothesis_neg <- run_power_simulation(\n  n_per_group = sample_size_per_group,\n  effect_size = -alternative_effect_size,\n  n_simulations = num_simulations,\n  conf_level = confidence_level\n)\n\n# =============================================================================\n# THEORETICAL DISTRIBUTION PARAMETERS\n# =============================================================================\n\n# Calculate standard error and degrees of freedom\nstandard_error <- sqrt(2 / sample_size_per_group)\ndegrees_freedom <- 2 * sample_size_per_group - 2\n\n# Distribution parameters for null and alternative hypotheses\nmu_null <- -equivalence_bound / standard_error # Mean under H0\nsigma_null <- 1 # SD under H0\nmu_alternative <- 0 # Mean under HA\nsigma_alternative <- 1 # SD under HA\n\n# Critical value for hypothesis test\ncritical_value <- qnorm(1 - alpha_level, mu_null, sigma_null)\n\n# =============================================================================\n# CREATE THEORETICAL DISTRIBUTION CURVES\n# =============================================================================\n\n# Define plot range (4 standard deviations from means)\nplot_range_multiplier <- 4\nx_min_null <- mu_null - sigma_null * plot_range_multiplier\nx_max_null <- mu_null + sigma_null * plot_range_multiplier\nx_min_alt <- abs(mu_null) - sigma_alternative * plot_range_multiplier\nx_max_alt <- abs(mu_null) + sigma_alternative * plot_range_multiplier\n\n# Create x-axis sequence for plotting\nx_values <- seq(\n  min(x_min_null, x_min_alt),\n  max(x_max_null, x_max_alt),\n  0.01\n)\n\n# Generate theoretical distributions\ndensity_null_neg <- dnorm(x_values, mu_null, sigma_null)\ndensity_null_pos <- dnorm(\n  x_values,\n  alternative_effect_size / standard_error,\n  sigma_null\n)\ndensity_alternative <- dnorm(x_values, mu_alternative, sigma_alternative)\n\n# Create data frames for plotting\ndf_null_negative <- data.frame(x = x_values, y = density_null_neg)\ndf_null_positive <- data.frame(x = x_values, y = density_null_pos)\ndf_alternative <- data.frame(x = x_values, y = density_alternative)\n\n# =============================================================================\n# CREATE POLYGONS FOR STATISTICAL REGIONS\n# =============================================================================\n\n# Alpha region polygon (Type I error)\nalpha_polygon_data <- data.frame(\n  x = x_values,\n  y = pmin(density_null_neg, density_alternative)\n)\nalpha_polygon_data <- alpha_polygon_data[\n  alpha_polygon_data$x >= critical_value,\n]\nalpha_polygon_data <- rbind(alpha_polygon_data, c(critical_value, 0))\n\n# Beta region polygon (Type II error)\nbeta_polygon_data <- df_alternative\nbeta_polygon_data <- beta_polygon_data[beta_polygon_data$x <= critical_value, ]\nbeta_polygon_data <- rbind(beta_polygon_data, c(critical_value, 0))\n\n# Power region polygon (1 - beta)\npower_polygon_data <- df_alternative\npower_polygon_data <- power_polygon_data[\n  power_polygon_data$x >= critical_value &\n    power_polygon_data$x <= abs(critical_value),\n]\npower_polygon_data <- rbind(\n  c(critical_value, 0),\n  c(critical_value, dnorm(critical_value, mu_alternative, sigma_alternative)),\n  power_polygon_data,\n  c(\n    abs(critical_value),\n    dnorm(abs(critical_value), mu_alternative, sigma_alternative)\n  ),\n  c(abs(critical_value), 0)\n)\n\n# =============================================================================\n# COMBINE POLYGONS FOR PLOTTING\n# =============================================================================\n\n# Add polygon identifiers\nalpha_polygon_data$region_type <- \"alpha\"\nbeta_polygon_data$region_type <- \"beta\"\npower_polygon_data$region_type <- \"power\"\n\n# Create second alpha region (mirrored)\nalpha_polygon_mirrored <- alpha_polygon_data |>\n  mutate(x = -1 * x, region_type = \"alpha_mirrored\")\n\nbeta_polygon_data_mirrored <- beta_polygon_data |>\n  mutate(x = -1 * x, region_type = \"beta_mirrored\")\n\n# Combine all polygon data\nall_polygons <- rbind(\n  alpha_polygon_data,\n  alpha_polygon_mirrored,\n  beta_polygon_data,\n  beta_polygon_data_mirrored,\n  power_polygon_data\n)\n\n# Convert to factor with proper labels\nall_polygons$region_type <- factor(\n  all_polygons$region_type,\n  levels = c(\"power\", \"beta\", \"beta_mirrored\", \"alpha\", \"alpha_mirrored\"),\n  labels = c(\"power\", \"beta\", \"beta2\", \"alpha\", \"alpha2\")\n)\n\n# =============================================================================\n# DEFINE COLOR PALETTE\n# =============================================================================\n\n# Color palette for the visualization\nPALETTE <- list( # nolint: object_name_linter\n  # Hypothesis colors\n  hypothesis = c(\n    \"H0\" = \"gray60\",\n    \"HA\" = \"#437677\"\n  ),\n\n  # Statistical region colors\n  regions = c(\n    \"alpha\" = \"#0d6374\",\n    \"alpha2\" = \"#0d6374\",\n    \"beta\" = \"#be805e\",\n    \"beta2\" = \"#be805e\",\n    \"power\" = \"#529d9e\"\n  ),\n\n  # Simulation curve colors\n  simulations = c(\n    \"null\" = \"green\", # TOST null hypotheses (effect outside bounds)\n    \"alternative\" = \"#7cecee\" # TOST alternative hypothesis\n  )\n)\n\n# =============================================================================\n# CREATE POWER ANALYSIS VISUALIZATION\n# =============================================================================\n\npower_plot_eq <- ggplot(\n  all_polygons,\n  aes(x, y, fill = region_type, group = region_type)\n) +\n  # Add theoretical distribution curves\n  geom_polygon(\n    data = df_null_negative,\n    aes(\n      x, y,\n      color = \"H0\",\n      group = NULL,\n      fill = NULL\n    ),\n    linetype = \"dotted\",\n    fill = \"gray80\",\n    linewidth = 1,\n    alpha = 0.5,\n    show.legend = FALSE\n  ) +\n  geom_polygon(\n    data = df_null_positive,\n    aes(\n      x, y,\n      color = \"H0\",\n      group = NULL,\n      fill = NULL\n    ),\n    linetype = \"dotted\",\n    fill = \"gray80\",\n    alpha = 0.5,\n    linewidth = 1,\n    show.legend = FALSE\n  ) +\n  # Add filled polygons for statistical regions\n  geom_polygon(\n    show.legend = FALSE,\n    alpha = 0.8\n  ) +\n  geom_line(\n    data = df_alternative,\n    aes(\n      x = x, y = y,\n      color = \"HA\",\n      group = NULL,\n      fill = NULL\n    ),\n    linewidth = 1,\n    linetype = \"dashed\",\n    color = \"gray60\",\n    inherit.aes = FALSE\n  ) +\n\n  # Add critical value lines\n  geom_vline(\n    xintercept = critical_value,\n    linewidth = 1,\n    linetype = \"dashed\"\n  ) +\n  geom_vline(\n    xintercept = -critical_value,\n    linewidth = 1,\n    linetype = \"dashed\"\n  ) +\n\n  # Customize colors and styling\n  scale_color_manual(\n    \"Hypothesis\",\n    values = PALETTE$hypothesis\n  ) +\n  scale_fill_manual(\n    \"Statistical Region\",\n    values = PALETTE$regions\n  ) +\n  labs(\n    x = \"Test statistic (z)\",\n    y = \"Density\",\n    title = \"Statistical Power Analysis Visualization\",\n    subtitle = \"TOST Equivalence Test (normal approximation)\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\n# Display the plot\npower_plot_eq\n```\n\n::: {.cell-output-display}\n![Equivalence (TOST) conceptual plot: two one-sided critical regions and\ncentral power region within ±margin.](power-sample-size_files/figure-html/fig-equivalence-nhst-plot-1.png){#fig-equivalence-nhst-plot width=960}\n:::\n:::\n\n\n\n\nNext, we verify that simulated study results behave as expected under the equivalence setup: when the true effect lies within ±margin, most statistics fall between the two critical values; when the true effect equals a margin (the nulls), they fall near the boundaries.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Add empirical density curves from simulations\npower_plot_eq +\n  # Add empirical histograms from simulations\n  geom_histogram(\n    data = sim_null_hypothesis_pos,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"gray30\",\n    alpha = 0.2,\n    color = \"gray30\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  ) +\n  geom_histogram(\n    data = sim_null_hypothesis_neg,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"gray30\",\n    alpha = 0.2,\n    color = \"gray30\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  ) +\n  geom_histogram(\n    data = sim_alternative_hypothesis,\n    aes(\n      x = effect / standard_error,\n      y = after_stat(density),\n      group = NULL\n    ),\n    bins = 50,\n    fill = \"#27AE60\",\n    alpha = 0.2,\n    color = \"#1E8449\",\n    linewidth = 0.3,\n    inherit.aes = FALSE\n  )\n```\n\n::: {.cell-output-display}\n![Empirical histograms for equivalence simulations (null bounds and\nalternative within bounds).](power-sample-size_files/figure-html/fig-equivalence-nhst-plot-histogram-1.png){#fig-equivalence-nhst-plot-histogram width=960}\n:::\n:::\n\n\n\n\nBecause equivalence can also be assessed via confidence intervals, we visualize the CI criterion directly: success occurs when the entire CI lies strictly within ±margin. This plot makes it clear why equivalence demands more precision (n) than superiority for the same α.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nsim_alternative_hypothesis |>\n  mutate(\n    ci_significant = ci_lower > -equivalence_bound &\n      ci_upper < equivalence_bound\n  ) |>\n  sample_n(200) |>\n  plot_ci_coverage(\n    true_effect = 0,\n    null_effect = -equivalence_bound,\n    title = \"Equivalence Trial\"\n  ) +\n  geom_vline(xintercept = equivalence_bound, linewidth = 1)\n```\n\n::: {.cell-output-display}\n![Equivalence CI visualization: success when the entire 90% CI lies\nwithin ±margin.](power-sample-size_files/figure-html/fig-equivalence-ci-plot-1.png){#fig-equivalence-ci-plot width=576}\n:::\n:::\n\n\n\n\nFinally, we compare empirical power from simulation with the theoretical TOST power. They should agree up to simulation error, which validates both our code and our understanding of the test.\n\n\n\n\n::: {#tbl-equivalence-power-comparison .cell tbl-cap='Comparison of empirical power from simulation with the theoretical\nTOST power.'}\n\n```{.r .cell-code}\n# For equivalence with margin ±0.2, we use the TOSTER package\n# But we can also calculate manually\nlibrary(TOSTER)\n\n# Theoretical power calculation using TOST\ntheoretical_power <- power_t_TOST(\n  n = sample_size_per_group,\n  low_eqbound = -equivalence_bound,\n  high_eqbound = equivalence_bound,\n  alpha = alpha_level\n)\n\n# Calculate empirical power from simulation\nsim_alternative_hypothesis |>\n  summarize(\n    power_tost = mean(t_value > critical_value & t_value < abs(critical_value)),\n    power_ci = mean(\n      ci_lower > -equivalence_bound & ci_upper < equivalence_bound\n    ),\n    power_theoretical = theoretical_power$power\n  ) |>\n  pivot_longer(\n    cols = everything(),\n    names_to = \"method\",\n    values_to = \"power\"\n  ) |>\n  gt() |>\n  fmt_percent(\n    columns = \"power\",\n    decimals = 0\n  )\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"fjfqrwkaoa\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>#fjfqrwkaoa table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#fjfqrwkaoa thead, #fjfqrwkaoa tbody, #fjfqrwkaoa tfoot, #fjfqrwkaoa tr, #fjfqrwkaoa td, #fjfqrwkaoa th {\n  border-style: none;\n}\n\n#fjfqrwkaoa p {\n  margin: 0;\n  padding: 0;\n}\n\n#fjfqrwkaoa .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#fjfqrwkaoa .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#fjfqrwkaoa .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#fjfqrwkaoa .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#fjfqrwkaoa .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#fjfqrwkaoa .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#fjfqrwkaoa .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#fjfqrwkaoa .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#fjfqrwkaoa .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#fjfqrwkaoa .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#fjfqrwkaoa .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#fjfqrwkaoa .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#fjfqrwkaoa .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#fjfqrwkaoa .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#fjfqrwkaoa .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#fjfqrwkaoa .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#fjfqrwkaoa .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#fjfqrwkaoa .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#fjfqrwkaoa .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#fjfqrwkaoa .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#fjfqrwkaoa .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#fjfqrwkaoa .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#fjfqrwkaoa .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#fjfqrwkaoa .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#fjfqrwkaoa .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#fjfqrwkaoa .gt_left {\n  text-align: left;\n}\n\n#fjfqrwkaoa .gt_center {\n  text-align: center;\n}\n\n#fjfqrwkaoa .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#fjfqrwkaoa .gt_font_normal {\n  font-weight: normal;\n}\n\n#fjfqrwkaoa .gt_font_bold {\n  font-weight: bold;\n}\n\n#fjfqrwkaoa .gt_font_italic {\n  font-style: italic;\n}\n\n#fjfqrwkaoa .gt_super {\n  font-size: 65%;\n}\n\n#fjfqrwkaoa .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#fjfqrwkaoa .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#fjfqrwkaoa .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#fjfqrwkaoa .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#fjfqrwkaoa .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#fjfqrwkaoa .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#fjfqrwkaoa .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#fjfqrwkaoa .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#fjfqrwkaoa div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_col_headings\">\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"method\">method</th>\n      <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"power\">power</th>\n    </tr>\n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"method\" class=\"gt_row gt_left\">power_tost</td>\n<td headers=\"power\" class=\"gt_row gt_right\">87%</td></tr>\n    <tr><td headers=\"method\" class=\"gt_row gt_left\">power_ci</td>\n<td headers=\"power\" class=\"gt_row gt_right\">87%</td></tr>\n    <tr><td headers=\"method\" class=\"gt_row gt_left\">power_theoretical</td>\n<td headers=\"power\" class=\"gt_row gt_right\">87%</td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n\n\n\n# Summary\n\nIn this lab, you learned the essential skills for power analysis and sample size planning:\n\n- **Four components**: Effect size, sample size, significance level, and power\n- **Power calculations**: Using the `pwr.t.test()` function\n- **Visualization**: Creating power curves to understand relationships\n- **Other hypothesis types**: Superiority vs. non-inferiority vs. equivalence tests\n\n# Resources for real-world power analysis\n\nThis lab covered the fundamentals of power analysis using t-tests, but real-world studies often involve more complex designs that require specialized approaches. While the concepts you've learned here (effect size, sample size, power, and significance level) remain the same, you'll need different tools for studies with repeated measures, cluster randomization, survival endpoints, or adaptive designs.\n\nHere are two resources that catalog R packages for many power analysis scenarios you might encounter:\n\n- **[CRAN Task View: Clinical Trial Design, Monitoring, and Analysis](https://cran.r-project.org/web/views/ClinicalTrials.html#sample-size-and-power-calculations)**: An overview of R packages for power and sample size calculations specifically relevant to clinical trials and biostatistical research\n\n- **[Power and Sample Size in R](https://powerandsamplesize.org/r-packages/)**: A comprehensive website listing R packages for power analysis across different study designs and statistical methods\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}