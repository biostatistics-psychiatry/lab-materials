{
  "hash": "55c7939c4ac647ca51c1ce2fbd8713bf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification\"\n---\n\n\n\n\nIn this chapter, we will look at how we can work with classification problems in R. You will learn how to evaluate *sensitivity*, *specificity*, *positive predictive value* (PPV), and *negative predictive value* (NPV). These measures help us understand how well a model or test can identify or predict binary outcomes (e.g., disease status, treatment response). You will not work on the models themselves; that is the topic of upcoming courses and chapters.\n\n# Load packages and data\n\nWe load both the outcomes and baseline data, and join them together using the `id` variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(here)\nlibrary(knitr)\n\nd_outcomes <- read_csv(here(\"data\", \"steps_clean.csv\"))\nd_bl <- read_rds(here(\"data\", \"steps_baseline.rds\"))\n\nd <- d_outcomes |>\n  select(\n    -group,\n    -ends_with(\"_screen\")\n  ) |>\n  full_join(\n    d_bl,\n    by = \"id\"\n  )\n```\n:::\n\n\n\n\nFor the purposes of this chapter we need a binary outcome. We will use the `lsas_post` variable as our outcome of interest, and evaluate how well we can predict it using the `lsas_screen` values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_nomiss <- d |>\n  filter(!is.na(lsas_post))\n\nd_nomiss <- d_nomiss |>\n  mutate(\n    lsas_post_bin = if_else(lsas_post < 50, \"Low\", \"High\"),\n    lsas_post_bin = factor(lsas_post_bin, levels = c(\"Low\", \"High\"))\n  )\n```\n:::\n\n\n\n\n::: {.callout-warning}\n## Statistical models below\n\nIn the following code chunk, we fit a simple logistic regression model so that we have some concrete predictions to work with when evaluating the classification metrics. The theory and practice of fitting statistical models is covered in a later course, but we felt it would be silly to skip it here. We are using the `tidymodels` package, which provides a unified framework for fitting and evaluating models. There are many other packages that can be used as well, and even functions in base R.\n\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlsas_fit <- logistic_reg() |> # specify the model type\n  set_engine(\"glm\") |> # we use the standard glm engine\n  set_mode(\"classification\") |> # specify classification mode\n  fit(lsas_post_bin ~ lsas_screen + group, data = d_nomiss) # outcome ~ predictor(s)\n\nlsas_pred <- predict(lsas_fit, new_data = d_nomiss) |> # predict on the original data\n  bind_cols(d_nomiss)\n\n# create a confusion matrix: true outcome ~ predicted outcome\nlsas_conf_mat <- conf_mat(lsas_pred, truth = lsas_post_bin, estimate = .pred_class)\n\ntrue_pos <- lsas_conf_mat$table[[1]]\nfalse_neg <- lsas_conf_mat$table[[2]]\nfalse_pos <- lsas_conf_mat$table[[3]]\ntrue_neg <- lsas_conf_mat$table[[4]]\n\nlsas_conf_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction Low High\n      Low   16    7\n      High  27  119\n```\n\n\n:::\n:::\n\n\n\n\n# Evaluate classification manually\n\nNow that we have the predictions, we can calculate the metrics of interest: sensitivity, specificity, positive predictive value, and negative predictive value.\n\nFirst, let's look at the prevalence of the outcome. If the outcome is rare, this will have a big impact on the classification metrics.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprevalence <- (true_pos + false_neg) / (true_pos + false_neg + true_neg + false_pos)\n```\n:::\n\n\n\n\nThe prevalence of having LSAS <50 after treatment is 0.25.\n\n## Sensitivity\n\nSensitivity is the proportion of true positives out of all **actual** positives.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsensitivity <- true_pos / (true_pos + false_neg)\n```\n:::\n\n\n\n\nSensitivity is 0.37.\n\n## Specificity\n\nSpecificity is the proportion of true negatives out of all **actual** negatives.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspecificity <- true_neg / (true_neg + false_pos)\n```\n:::\n\n\n\n\nSpecificity is 0.94.\n\n## Positive predictive value\n\nPositive predictive value is the proportion of true positives out of all **predicted** positives. See the difference compared to sensitivity?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npos_pred_val <- true_pos / (true_pos + false_pos)\n```\n:::\n\n\n\n\nPositive predictive value is 0.7.\n\n## Negative predictive value\n\nNegative predictive value is the proportion of true negatives out of all **predicted** negatives. Again, look at the difference compared to specificity.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneg_pred_val <- true_neg / (true_neg + false_neg)\n```\n:::\n\n\n\n\nNegative predictive value is 0.82.\n\n# Evaluate classification using tidymodels\n\n...of course there are built-in functions for this! But it's good practice to do it manually when you learn these concepts.\n\n\n\n\n::: {#tbl-class-metrics .cell tbl-cap='Classification metrics'}\n\n```{.r .cell-code}\nclass_metrics <- metric_set(accuracy, sens, spec, ppv, npv)\n\nkable(class_metrics(lsas_pred, truth = lsas_post_bin, estimate = .pred_class), digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|.metric  |.estimator | .estimate|\n|:--------|:----------|---------:|\n|accuracy |binary     |      0.80|\n|sens     |binary     |      0.37|\n|spec     |binary     |      0.94|\n|ppv      |binary     |      0.70|\n|npv      |binary     |      0.82|\n\n\n:::\n:::\n\n\n\n\n::: {.callout-note}\n## The importance of base-rate of the event\n\nOne particularly difficult outcome to predict is suicide. Even in the most severe populations (e.g., people with previous suicide attempts who have been hospitalised), the 1-year risk of suicide is 5%. Therefore, most individuals classified as \"high risk\" of suicide will not die by suicide. The PPV is low in all models, it doesn't matter if you have a simple logistic regression or an ensemble machine learning model.\n\nCompare this to another common use-case of classification: predicting remission status after treatment. For many psychiatric disorders, we can expect remission rates of 40-60%. In these cases, our classification models will be more balanced, and the PPV will be higher.\n\nCan you think of other examples from your own research where the base-rate of the event is low? \n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}