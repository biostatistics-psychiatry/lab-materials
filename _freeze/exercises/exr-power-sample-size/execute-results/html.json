{
  "hash": "4f81ed2362cdde9cc020107722d4d4ed",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab: Power and Sample Size\"\nnumber-sections: true\nformat: live-html\nengine: knitr\nwebr:\n  packages:\n    - dplyr\n    - pwr\n    - ggplot2\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell edit='false'}\n```{webr}\n#| edit: false\n#| output: false\nwebr::install(\"gradethis\", quiet = TRUE)\nlibrary(gradethis)\noptions(webr.exercise.checker = function(\n  label, user_code, solution_code, check_code, envir_result, evaluate_result,\n  envir_prep, last_value, engine, stage, ...\n) {\n  if (is.null(check_code)) {\n    # No grading code, so just skip grading\n    invisible(NULL)\n  } else if (is.null(label)) {\n    list(\n      correct = FALSE,\n      type = \"warning\",\n      message = \"All exercises must have a label.\"\n    )\n  } else if (is.null(solution_code)) {\n    list(\n      correct = FALSE,\n      type = \"warning\",\n      message = htmltools::tags$div(\n        htmltools::tags$p(\"A problem occurred grading this exercise.\"),\n        htmltools::tags$p(\n          \"No solution code was found. Note that grading exercises using the \",\n          htmltools::tags$code(\"gradethis\"),\n          \"package requires a model solution to be included in the document.\"\n        )\n      )\n    )\n  } else {\n    gradethis::gradethis_exercise_checker(\n      label = label, solution_code = solution_code, user_code = user_code,\n      check_code = check_code, envir_result = envir_result,\n      evaluate_result = evaluate_result, envir_prep = envir_prep,\n      last_value = last_value, stage = stage, engine = engine)\n  }\n})\n```\n:::\n\n\n\n\n\n# Introduction to Power Analysis\n\n**Statistical power** is the probability that a statistical test will correctly reject a null hypothesis when it is false. In other words, it's the probability of detecting an effect when there truly is one.\n\nKey concepts:\n\n- **Power**: Probability of detecting a true effect (typically we aim for 0.80 or 80%)\n- **Alpha (Î±)**: Significance level (typically 0.05)\n- **Effect Size**: The magnitude of the difference we're trying to detect (Cohen's d for t-tests)\n- **Sample Size**: The number of observations in each group\n\nThese four quantities are related: if you know any three, you can calculate the fourth!\n\n# Using the `pwr` Package\n\nThe `pwr` package provides functions for power analysis. For two-group t-tests, we use `pwr.t.test()`.\n\n:::: {.panel-tabset}\n## Exercise 1\n\n**Calculate power for a planned study**\n\nSuppose you're planning a study comparing two groups with:\n- Sample size of 30 per group (`n = 30`)\n- Expected Cohen's d effect size of 0.5\n- Significance level of 0.05 (`sig.level = 0.05`)\n- Two-sided test (`alternative = \"two.sided\"`)\n\nUse `pwr.t.test()` to calculate the statistical power.\n\n\n\n\n::: {.cell exercise='ex_1'}\n```{webr}\n#| exercise: ex_1\nlibrary(pwr)\n\n# Calculate power\nresult <- pwr.t.test(\n  n = ______,\n  d = ______,\n  sig.level = ______,\n  alternative = ______\n)\n\n# Print the result\nresult\n```\n:::\n\n\n\n\n## Hints \n\n::: {.hint exercise=\"ex_1\"}\n\nThe `pwr.t.test()` function takes these arguments:\n- `n`: sample size per group\n- `d`: Cohen's d effect size\n- `sig.level`: alpha level\n- `alternative`: \"two.sided\", \"greater\", or \"less\"\n\nLeave `power` blank - that's what you're calculating!\n\n```r\npwr.t.test(\n  n = 30,\n  d = 0.5,\n  sig.level = ______,\n  alternative = \"two.sided\"\n)\n```\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_1\"}\n\n\n\n\n::: {.cell exercise='ex_1' solution='true'}\n```{webr}\n#| exercise: ex_1\n#| solution: true\nlibrary(pwr)\n\n# Calculate power\nresult <- pwr.t.test(\n  n = 30,           #<1>\n  d = 0.5,          #<2>\n  sig.level = 0.05, #<3>\n  alternative = \"two.sided\" #<4>\n)\n\n# Print the result\nresult\n```\n:::\n\n\n\n1. Sample size per group\n2. Expected Cohen's d effect size (medium effect)\n3. Significance level (alpha = 0.05)\n4. Two-sided hypothesis test\n\nWith these parameters, the power is approximately 0.47 (47%), which is below the conventional 0.80 threshold.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_1' check='true'}\n```{webr}\n#| exercise: ex_1\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n# Sample Size Calculation\n\nOften, we want to determine how many participants we need to achieve adequate power (typically 80%).\n\n:::: {.panel-tabset}\n## Exercise 2\n\n**Calculate required sample size**\n\nFor the same study design (Cohen's d = 0.5, alpha = 0.05, two-sided test), calculate the sample size needed to achieve 80% power.\n\nThis time, leave `n` blank and specify `power = 0.80`.\n\n\n\n\n::: {.cell exercise='ex_2'}\n```{webr}\n#| exercise: ex_2\nlibrary(pwr)\n\n# Calculate required sample size\nresult <- pwr.t.test(\n  d = ______,\n  sig.level = ______,\n  power = ______,\n  alternative = \"two.sided\"\n)\n\n# Print the result\nresult\n```\n:::\n\n\n\n\n## Hints \n\n::: {.hint exercise=\"ex_2\"}\n\nNow you're solving for `n` instead of `power`, so:\n- Leave `n` out completely\n- Add `power = 0.80`\n- Keep the other parameters the same\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_2\"}\n\n\n\n\n::: {.cell exercise='ex_2' solution='true'}\n```{webr}\n#| exercise: ex_2\n#| solution: true\nlibrary(pwr)\n\n# Calculate required sample size\nresult <- pwr.t.test(\n  d = 0.5,          #<1>\n  sig.level = 0.05,\n  power = 0.80,     #<2>\n  alternative = \"two.sided\"\n)\n\n# Print the result\nresult\n```\n:::\n\n\n\n1. Expected Cohen's d effect size\n2. Desired power (80%)\n\nYou need approximately 64 participants per group (128 total) to achieve 80% power for detecting a medium effect size.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_2' check='true'}\n```{webr}\n#| exercise: ex_2\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n# How Effect Size Changes Power\n\nEffect size is a critical factor in power analysis. Cohen's d describes the standardized difference between two groups:\n- **Small effect**: d = 0.2\n- **Medium effect**: d = 0.5\n- **Large effect**: d = 0.8\n\nLet's explore how effect size impacts power when sample size is held constant.\n\n:::: {.panel-tabset}\n## Exercise 3a\n\n**Compare power across different effect sizes**\n\nCalculate the power for detecting small (d = 0.2), medium (d = 0.5), and large (d = 0.8) effects with:\n- Sample size: 50 per group\n- Significance level: 0.05\n- Two-sided test\n\nStore the results in variables and compare them.\n\n\n\n\n::: {.cell exercise='ex_3a'}\n```{webr}\n#| exercise: ex_3a\nlibrary(pwr)\n\n# Calculate power for small effect\npower_small <- pwr.t.test(\n  n = 50,\n  d = ______,\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\n# Calculate power for medium effect\npower_medium <- pwr.t.test(\n  n = 50,\n  d = ______,\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\n# Calculate power for large effect\npower_large <- pwr.t.test(\n  n = 50,\n  d = ______,\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\n# Display results\ncat(\"Power for small effect (d = 0.2):\", round(power_small, 3), \"\\n\")\ncat(\"Power for medium effect (d = 0.5):\", round(power_medium, 3), \"\\n\")\ncat(\"Power for large effect (d = 0.8):\", round(power_large, 3), \"\\n\")\n```\n:::\n\n\n\n\n## Hints \n\n::: {.hint exercise=\"ex_3a\"}\n\nUse Cohen's conventional effect sizes:\n- Small: d = 0.2\n- Medium: d = 0.5\n- Large: d = 0.8\n\nEach `pwr.t.test()` call should only differ in the `d` parameter.\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_3a\"}\n\n\n\n\n::: {.cell exercise='ex_3a' solution='true'}\n```{webr}\n#| exercise: ex_3a\n#| solution: true\nlibrary(pwr)\n\n# Calculate power for small effect\npower_small <- pwr.t.test(\n  n = 50,\n  d = 0.2,  #<1>\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\n# Calculate power for medium effect\npower_medium <- pwr.t.test(\n  n = 50,\n  d = 0.5,  #<2>\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\n# Calculate power for large effect\npower_large <- pwr.t.test(\n  n = 50,\n  d = 0.8,  #<3>\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\n# Display results\ncat(\"Power for small effect (d = 0.2):\", round(power_small, 3), \"\\n\")\ncat(\"Power for medium effect (d = 0.5):\", round(power_medium, 3), \"\\n\")\ncat(\"Power for large effect (d = 0.8):\", round(power_large, 3), \"\\n\")\n```\n:::\n\n\n\n1. Small effect size (d = 0.2) - only ~29% power\n2. Medium effect size (d = 0.5) - ~70% power\n3. Large effect size (d = 0.8) - ~95% power\n\nNotice how dramatically power increases with effect size! With n=50, we have excellent power for large effects, decent power for medium effects, but poor power for small effects.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_3a' check='true'}\n```{webr}\n#| exercise: ex_3a\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n:::: {.panel-tabset}\n## Exercise 3b\n\n**Visualize effect size and power relationship**\n\nCreate a plot showing how power changes across a range of effect sizes (d = 0.1 to 1.0) for a fixed sample size of 50 per group.\n\n\n\n\n::: {.cell exercise='ex_3b'}\n```{webr}\n#| exercise: ex_3b\nlibrary(pwr)\nlibrary(ggplot2)\n\n# Create a sequence of effect sizes\neffect_sizes <- seq(0.1, 1.0, by = 0.05)\n\n# Calculate power for each effect size\npower_values <- sapply(effect_sizes, function(d) {\n  pwr.t.test(\n    n = ______,\n    d = ______,\n    sig.level = 0.05,\n    alternative = \"two.sided\"\n  )$power\n})\n\n# Create a data frame\npower_df <- data.frame(\n  effect_size = effect_sizes,\n  power = power_values\n)\n\n# Plot\nggplot(power_df, aes(x = effect_size, y = power)) +\n  geom_line(color = \"darkgreen\", linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = c(0.2, 0.5, 0.8), linetype = \"dotted\", alpha = 0.5) +\n  annotate(\"text\", x = 0.2, y = 0.05, label = \"Small\", size = 3) +\n  annotate(\"text\", x = 0.5, y = 0.05, label = \"Medium\", size = 3) +\n  annotate(\"text\", x = 0.8, y = 0.05, label = \"Large\", size = 3) +\n  labs(\n    title = \"Power vs Effect Size (n = 50 per group)\",\n    x = \"Cohen's d (Effect Size)\",\n    y = \"Statistical Power\"\n  ) +\n  theme_minimal()\n```\n:::\n\n\n\n\n## Hints \n\n::: {.hint exercise=\"ex_3b\"}\n\nIn the `sapply()` function, `d` is the iterator variable representing each effect size. The sample size should be 50.\n\n```r\npower_values <- sapply(effect_sizes, function(d) {\n  pwr.t.test(\n    n = 50,\n    d = d,  # Use the iterator variable\n    sig.level = 0.05,\n    alternative = \"two.sided\"\n  )$power\n})\n```\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_3b\"}\n\n\n\n\n::: {.cell exercise='ex_3b' solution='true'}\n```{webr}\n#| exercise: ex_3b\n#| solution: true\nlibrary(pwr)\nlibrary(ggplot2)\n\n# Create a sequence of effect sizes\neffect_sizes <- seq(0.1, 1.0, by = 0.05)\n\n# Calculate power for each effect size\npower_values <- sapply(effect_sizes, function(d) {\n  pwr.t.test(\n    n = 50,   #<1>\n    d = d,    #<2>\n    sig.level = 0.05,\n    alternative = \"two.sided\"\n  )$power\n})\n\n# Create a data frame\npower_df <- data.frame(\n  effect_size = effect_sizes,\n  power = power_values\n)\n\n# Plot\nggplot(power_df, aes(x = effect_size, y = power)) +\n  geom_line(color = \"darkgreen\", linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = c(0.2, 0.5, 0.8), linetype = \"dotted\", alpha = 0.5) +\n  annotate(\"text\", x = 0.2, y = 0.05, label = \"Small\", size = 3) +\n  annotate(\"text\", x = 0.5, y = 0.05, label = \"Medium\", size = 3) +\n  annotate(\"text\", x = 0.8, y = 0.05, label = \"Large\", size = 3) +\n  labs(\n    title = \"Power vs Effect Size (n = 50 per group)\",\n    x = \"Cohen's d (Effect Size)\",\n    y = \"Statistical Power\"\n  ) +\n  theme_minimal()\n```\n:::\n\n\n\n1. Fixed sample size of 50 per group\n2. Effect size varies from the sequence\n\nThe plot shows that with n=50, you cross the 80% power threshold (red dashed line) at approximately d = 0.57 (between medium and large effects). The vertical dotted lines show conventional effect size benchmarks.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_3b' check='true'}\n```{webr}\n#| exercise: ex_3b\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n# Power Curves\n\nA power curve shows how power changes with sample size for a given effect size.\n\n:::: {.panel-tabset}\n## Exercise 3\n\n**Create a power curve**\n\nCalculate power for sample sizes ranging from 10 to 100 (per group) for Cohen's d = 0.5, then create a plot.\n\n\n\n\n::: {.cell exercise='ex_3'}\n```{webr}\n#| exercise: ex_3\nlibrary(pwr)\nlibrary(ggplot2)\n\n# Create a sequence of sample sizes\nsample_sizes <- seq(10, 100, by = 5)\n\n# Calculate power for each sample size\npower_values <- sapply(sample_sizes, function(n) {\n  pwr.t.test(\n    n = ______,\n    d = ______,\n    sig.level = 0.05,\n    alternative = \"two.sided\"\n  )$power\n})\n\n# Create a data frame\npower_df <- data.frame(\n  n = sample_sizes,\n  power = power_values\n)\n\n# Plot\nggplot(power_df, aes(x = n, y = power)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Power Curve for Two-Sample t-test\",\n    x = \"Sample Size per Group\",\n    y = \"Statistical Power\"\n  ) +\n  theme_minimal()\n```\n:::\n\n\n\n\n## Hints \n\n::: {.hint exercise=\"ex_3\"}\n\nIn the `sapply()` function, `n` is the iterator variable from the function, so use it directly. The effect size should be 0.5.\n\n```r\npower_values <- sapply(sample_sizes, function(n) {\n  pwr.t.test(\n    n = n,\n    d = 0.5,\n    sig.level = 0.05,\n    alternative = \"two.sided\"\n  )$power\n})\n```\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_3\"}\n\n\n\n\n::: {.cell exercise='ex_3' solution='true'}\n```{webr}\n#| exercise: ex_3\n#| solution: true\nlibrary(pwr)\nlibrary(ggplot2)\n\n# Create a sequence of sample sizes\nsample_sizes <- seq(10, 100, by = 5)\n\n# Calculate power for each sample size\npower_values <- sapply(sample_sizes, function(n) {\n  pwr.t.test(\n    n = n,            #<1>\n    d = 0.5,          #<2>\n    sig.level = 0.05,\n    alternative = \"two.sided\"\n  )$power\n})\n\n# Create a data frame\npower_df <- data.frame(\n  n = sample_sizes,\n  power = power_values\n)\n\n# Plot\nggplot(power_df, aes(x = n, y = power)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Power Curve for Two-Sample t-test\",\n    x = \"Sample Size per Group\",\n    y = \"Statistical Power\"\n  ) +\n  theme_minimal()\n```\n:::\n\n\n\n1. Use the sample size from the iterator\n2. Cohen's d effect size\n\nThe red dashed line shows 80% power. Notice how power increases with sample size!\n\n:::\n\n\n\n\n::: {.cell exercise='ex_3' check='true'}\n```{webr}\n#| exercise: ex_3\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n# Simulation-Based Power Analysis\n\nAnalytical formulas (like `pwr.t.test()`) are great, but sometimes we need to verify power through simulation. This is especially useful for complex designs or when assumptions may be violated.\n\n## Understanding the Simulation Approach\n\nInstead of using formulas, we can:\n1. Simulate many datasets under specific conditions (effect size, sample size)\n2. Run a t-test on each simulated dataset\n3. Count how many tests were significant\n4. Power = proportion of significant tests\n\n:::: {.panel-tabset}\n## Exercise 4\n\n**Simulate data and run multiple t-tests**\n\nLet's simulate 1000 studies comparing two groups:\n- Group 1: mean = 0, SD = 1, n = 30\n- Group 2: mean = 0.5, SD = 1, n = 30 (Cohen's d = 0.5)\n\nFor each simulation, run a t-test and save the p-value.\n\n\n\n\n::: {.cell exercise='ex_4'}\n```{webr}\n#| exercise: ex_4\nset.seed(123)  # For reproducibility\n\n# Number of simulations\nn_sims <- 1000\nn_per_group <- 30\ntrue_effect <- 0.5  # Cohen's d\n\n# Storage for p-values\np_values <- numeric(n_sims)\n\n# Run simulations\nfor (i in 1:n_sims) {\n  # Simulate data for two groups\n  group1 <- rnorm(n_per_group, mean = 0, sd = 1)\n  group2 <- rnorm(n_per_group, mean = ______, sd = 1)\n  \n  # Run t-test and extract p-value\n  test_result <- t.test(group1, group2)\n  p_values[i] <- test_result$______\n}\n\n# Show first 10 p-values\nhead(p_values, 10)\n```\n:::\n\n\n\n\n## Hints \n\n::: {.hint exercise=\"ex_4\"}\n\nFor Cohen's d = 0.5 with SD = 1, the mean difference should be 0.5.\n\nThe p-value from a t-test object is accessed with `$p.value`.\n\n```r\ngroup2 <- rnorm(n_per_group, mean = true_effect, sd = 1)\n# ...\np_values[i] <- test_result$p.value\n```\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_4\"}\n\n\n\n\n::: {.cell exercise='ex_4' solution='true'}\n```{webr}\n#| exercise: ex_4\n#| solution: true\nset.seed(123)  # For reproducibility\n\n# Number of simulations\nn_sims <- 1000\nn_per_group <- 30\ntrue_effect <- 0.5  # Cohen's d\n\n# Storage for p-values\np_values <- numeric(n_sims)\n\n# Run simulations\nfor (i in 1:n_sims) {\n  # Simulate data for two groups\n  group1 <- rnorm(n_per_group, mean = 0, sd = 1)\n  group2 <- rnorm(n_per_group, mean = true_effect, sd = 1) #<1>\n  \n  # Run t-test and extract p-value\n  test_result <- t.test(group1, group2)\n  p_values[i] <- test_result$p.value #<2>\n}\n\n# Show first 10 p-values\nhead(p_values, 10)\n```\n:::\n\n\n\n1. Group 2 has a mean of 0.5 (effect size = 0.5 when SD = 1)\n2. Extract the p-value from the t-test result\n\n:::\n\n\n\n\n::: {.cell exercise='ex_4' check='true'}\n```{webr}\n#| exercise: ex_4\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n:::: {.panel-tabset}\n## Exercise 5\n\n**Calculate power from simulation results**\n\nNow calculate the statistical power from your simulation by determining what proportion of tests were statistically significant (p < 0.05).\n\nAlso create a histogram of the p-values to visualize the distribution.\n\n\n\n\n::: {.cell exercise='ex_5' setup='true'}\n```{webr}\n#| exercise: ex_5\n#| setup: true\nset.seed(123)\nn_sims <- 1000\nn_per_group <- 30\ntrue_effect <- 0.5\np_values <- numeric(n_sims)\nfor (i in 1:n_sims) {\n  group1 <- rnorm(n_per_group, mean = 0, sd = 1)\n  group2 <- rnorm(n_per_group, mean = true_effect, sd = 1)\n  test_result <- t.test(group1, group2)\n  p_values[i] <- test_result$p.value\n}\n```\n:::\n\n::: {.cell exercise='ex_5'}\n```{webr}\n#| exercise: ex_5\nlibrary(ggplot2)\n\n# Calculate power (proportion of p-values < 0.05)\nsimulated_power <- mean(p_values < ______)\n\ncat(\"Simulated power:\", simulated_power, \"\\n\")\n\n# Compare to analytical power\nanalytical_power <- pwr.t.test(\n  n = 30,\n  d = 0.5,\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\ncat(\"Analytical power:\", analytical_power, \"\\n\")\n\n# Visualize p-value distribution\nggplot(data.frame(p_value = p_values), aes(x = p_value)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = 0.05, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Distribution of P-values from 1000 Simulations\",\n    subtitle = paste(\"Power =\", round(simulated_power, 3)),\n    x = \"P-value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n```\n:::\n\n\n\n\n## Hints \n\n::: {.hint exercise=\"ex_5\"}\n\nTo calculate the proportion of significant tests:\n- Compare each p-value to 0.05\n- Take the mean of the TRUE/FALSE values (TRUE = 1, FALSE = 0)\n\n```r\nsimulated_power <- mean(p_values < 0.05)\n```\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_5\"}\n\n\n\n\n::: {.cell exercise='ex_5' solution='true'}\n```{webr}\n#| exercise: ex_5\n#| solution: true\nlibrary(ggplot2)\n\n# Calculate power (proportion of p-values < 0.05)\nsimulated_power <- mean(p_values < 0.05) #<1>\n\ncat(\"Simulated power:\", simulated_power, \"\\n\")\n\n# Compare to analytical power\nanalytical_power <- pwr.t.test(\n  n = 30,\n  d = 0.5,\n  sig.level = 0.05,\n  alternative = \"two.sided\"\n)$power\n\ncat(\"Analytical power:\", analytical_power, \"\\n\")\n\n# Visualize p-value distribution\nggplot(data.frame(p_value = p_values), aes(x = p_value)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = 0.05, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Distribution of P-values from 1000 Simulations\",\n    subtitle = paste(\"Power =\", round(simulated_power, 3)),\n    x = \"P-value\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n```\n:::\n\n\n\n1. Calculate proportion of p-values below 0.05\n\nThe simulated power should be very close to the analytical power (~0.47). The histogram shows that many p-values fall below 0.05 (red line), but the majority are above it, reflecting the moderate power of this design.\n\n:::\n\n\n\n\n::: {.cell exercise='ex_5' check='true'}\n```{webr}\n#| exercise: ex_5\n#| check: true\ngradethis::grade_this_code()\n```\n:::\n\n\n\n\n::::\n\n# Summary\n\nIn this lab, you learned:\n\n1. **Power analysis basics**: The relationship between power, sample size, effect size, and significance level\n2. **Using `pwr` package**: How to calculate power or required sample size analytically\n3. **Power curves**: Visualizing how power changes with sample size\n4. **Simulation-based power**: Estimating power by simulating many studies\n\n## Key Takeaways\n\n- **Higher sample size** â Higher power\n- **Larger effect size** â Higher power  \n- **Higher alpha** (e.g., 0.10 vs 0.05) â Higher power (but more false positives!)\n- **Convention**: Aim for 80% power in study planning\n- **Simulation** can verify analytical calculations and handle complex scenarios\n\n## Practice on Your Own\n\nTry varying the parameters in the exercises:\n- What happens to power with n = 50 vs n = 100?\n- How does a small effect (d = 0.2) vs large effect (d = 0.8) change required sample size?\n- What if you use a one-sided test instead of two-sided?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}