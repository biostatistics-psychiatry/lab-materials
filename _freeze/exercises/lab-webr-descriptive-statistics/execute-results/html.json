{
  "hash": "3c10fe0ab157cb6dd2727e0941b59278",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Lab webr descriptive statistics\nnumber-sections: true\nformat: live-html\nengine: knitr\nwebr:\n  packages:\n    - dplyr\n    - tableone\nresources:\n  - ../data/steps_clean.csv\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell setup='true' exercise='ex_1'}\n```{webr}\n#| setup: true\n#| exercise: ex_1\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## Exercise 1\n\nUse the functions described in the Import and clean data lab to get a quick overview of the dataset called `df_data`and give a brief summary of it.\n\n\n::: {.cell exercise='ex_1'}\n```{webr}\n#| exercise: ex_1\n\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_1\"}\nHave a look at [Import and clean data](../labs/import-clean.qmd##Check%20data%20structure)\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_1\"}\nThe full solution is:\n\n``` r\nglimpse(d_bl)\nhead(d_bl)\nncol(d_bl)\nnrow(d_bl) #<1>\n```\n:::\n:::::\n\n\n::: {.cell setup='true' exercise='ex_2'}\n```{webr}\n#| setup: true\n#| exercise: ex_2\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## Exercise 2\n\nVisualize the distribution of the PHQ-9 scale and the PID-5 scale and provide the mean, median and mode.\n\n\n::: {.cell exercise='ex_2'}\n```{webr}\n#| exercise: ex_2\n#Creating a function to get the mode\nget_mode <- function(x) {\n  ux <- unique(x)\n  ux[which.max(tabulate(match(x, ux)))]\n}\n\n___(df_data$pid_5_screen)\n\n___(____phq9_screen)\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_2\"}\nHave a look at [Import and clean data](../labs/descriptive-statistics.qmd###%20Numeric%20data)\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_2\"}\nThe full solution is:\n\n``` r\n#PHQ-9\nhist(df_data$phq9_screen)\nmean(df_data$phq9_screen)\nmedian(df_data$phq9_screen)\nget_mode(df_data$phq9_screen)\n\n#PID-5\nhist(df_data$pid_5_screen)\nmean(df_data$pid_5_screen)\nmedian(df_data$pid_5_screen)\nget_mode(df_data$pid_5_screen) #<2>\n```\n:::\n:::::\n\n::: panel-tabset\n## Exercise 3\n\nHow does the centrality measures differ and why?\n\n## Hints\n\nThink about mean, median, and mode and how they are influenced by the shape of the distribution or outliers.\n\n## Solution\n\nThe mean, median, and mode differ because they describe different aspects of the data:\n\n-   The **mean** is sensitive to extreme values and gives the arithmetic average.\n-   The **median** is the middle value, unaffected by outliers, and reflects the central point in skewed distributions.\n-   The **mode** identifies the most frequent value and is useful for categorical or discrete variables.\n\nThey differ because of how they respond to skewness and outliers in the data.\n:::\n\n::: panel-tabset\n## Exercise 4\n\nReason on the pros and cons of the different centrality measures for these scales.\n\n## Hints\n\nConsider which measure gives the most typical picture and when one measure might mislead.\n\n## Solution\n\n-   **Mean**\n    -   *Pros:* Uses all data values, good for symmetric distributions.\\\n    -   *Cons:* Highly affected by skewness and outliers.\n-   **Median**\n    -   *Pros:* Robust to outliers, better for skewed distributions.\\\n    -   *Cons:* Ignores the magnitude of extreme values, less informative for symmetric data.\n-   **Mode**\n    -   *Pros:* Useful for categorical data and identifying the most common response.\\\n    -   *Cons:* Can be unstable if multiple modes exist or if the distribution is flat.\n\nFor these psychological scales, the **median** is often more informative if the distributions are skewed, while the **mean** is more common when data is approximately normal. The **mode** can be informative but is less often used for continuous scales.\n:::\n\n\n::: {.cell}\n\n:::\n\n\n\n\n::: {.cell setup='true' exercise='ex_5'}\n```{webr}\n#| setup: true\n#| exercise: ex_5\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## Exercise 5\n\nCalculate some spread measures for the LSAS-SR scale, what do they tell you and which ones do you think are most useful to describe the spread of the values. Motive you answer briefly!\n\n\n::: {.cell exercise='ex_5'}\n```{webr}\n#| exercise: ex_5\n\n___(df_data$lsas_screen)\n___(df_data$lsas_screen)\n___(df_data$lsas_screen)\n___(df_data$lsas_screen)\n___(df_data$lsas_screen)\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_5\"}\nHave a look at [Descriptive statistics](../labs/descriptive-statistics.qmd###%20Numeric%20data)\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_5\"}\nThe full solution is:\n\n``` r\nsd(df_data$lsas_screen)\nvar(df_data$lsas_screen)\nrange(df_data$lsas_screen)\nIQR(df_data$lsas_screen)\nhist(df_data$lsas_screen) #<5>\n```\n\nwhere the historgram helps you determine which of the spread measures is most useful\n:::\n:::::\n\n\n::: {.cell setup='true' exercise='ex_6'}\n```{webr}\n#| setup: true\n#| exercise: ex_6\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## Exercise 6\n\nCalculate the counts, proportions and percentages for the simulated income categories and visualize the distribution\n\n\n::: {.cell exercise='ex_6'}\n```{webr}\n#| exercise: ex_6\n# Simulate an income variable\nn <- nrow(df_data)\nincome_levels <- c(\"Low\", \"Medium\", \"High\")\nincome_probs <- c(0.2, 0.6, 0.2) # Adjust probabilities as needed\ndf_data$income <- sample(income_levels, size = n, replace = TRUE, prob = income_probs)\n\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_6\"}\n\nTo calculate the proportion, you will need to compare the counts with the total number of rows in the dataset `nrow()`.\n\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_6\"}\nThe full solution is:\n\n``` r\n# counts\ntable(df_data$income)\n\n# proportions\ntable(df_data$income)/nrow(df_data)\n\n# percentages\ntable(df_data$income)/nrow(df_data)*100 \n#<6>\n```\n\nwhere the histogram helps you determine which of the spread measures is most useful\n:::\n:::::\n\n\n::: {.cell setup='true' exercise='ex_7'}\n```{webr}\n#| setup: true\n#| exercise: ex_7\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## Exercise 7\n\nVisualize the joint distribution of GAD-7 and PHQ-9 as numeric variables and describe what you see. Which plot is the most useful for this purpose?\n\n\n::: {.cell exercise='ex_7'}\n```{webr}\n#| exercise: ex_7\n\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_7\"}\nOne useful way to visualize joint distributions of numeric variables is to create a scatter plot. See [Descriptive statistics](../labs/descriptive-statistics.qmd#numeric-by-numeric-distributions)\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_7\"}\nThe full solution is:\n\n``` r\nplot(df_data$gad_screen, df_data$phq9_screen)\n#<7>\n```\n:::\n:::::\n\n\n::: {.cell setup='true' exercise='ex_8'}\n```{webr}\n#| setup: true\n#| exercise: ex_8\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## Exercise 8\n\nVisualize the distribution of of LSAS scores by income level and describe what you see. Which plot is the most useful for this purpose?\n\n\n::: {.cell exercise='ex_8'}\n```{webr}\n#| exercise: ex_8\n# first simulate the income variable again\nn <- nrow(df_data)\nincome_levels <- c(\"Low\", \"Medium\", \"High\")\nincome_probs <- c(0.2, 0.6, 0.2) # Adjust probabilities as needed\ndf_data$income <- sample(income_levels, size = n, replace = TRUE, prob = income_probs)\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_8\"}\nOne useful way to visualize joint distributions of numeric variables and categoric variables is to create a grouped boxplot. See [Descriptive statistics](../labs/descriptive-statistics.qmd#numeric-data)\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_8\"}\nThe full solution is:\n\n``` r\nboxplot(df_data$lsas_screen ~df_data$income,\n        ylab = \"LSAS-SR\",\n        xlab = \"Income\")\n#<8>\n```\n:::\n:::::\n\n\n::: {.cell setup='true' exercise='ex_9'}\n```{webr}\n#| setup: true\n#| exercise: ex_9\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## Exercise 9\n\nCreate a table using the tableone package to show descriptive statistics stratified by high vs low depression levels. Briefly interpret what you see.\n\n\n::: {.cell exercise='ex_9'}\n```{webr}\n#| exercise: ex_9\nlibrary(tableone)\n# create a variable for high vs low depression\ndf_data$phq_cat <- ifelse(df_data$phq9_screen >= 10, \"High depression\", \"Low depression\")\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_9\"}\nSee [Descriptive statistics](../labs/descriptive-statistics.qmd#numeric-data)\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_9\"}\nThe full solution is:\n\n``` r\n# define the variables you want\nvars <- c(\n  \"lsas_screen\",\n  \"gad_screen\",\n  \"phq9_screen\",\n  \"bbq_screen\",\n  \"scs_screen\",\n  \"dmrsodf_screen\",\n  \"ders_screen\",\n  \"pid_5_screen\"\n)\n\nCreateTableOne(vars= vars, data = df_data, strata = \"phq_cat\", test = FALSE)\n\n#<9>\n```\n:::\n:::::\n\n\n::: {.cell setup='true' exercise='ex_10'}\n```{webr}\n#| setup: true\n#| exercise: ex_10\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## BONUS Exercise 10\n\nCalculate the **population** variance and standard deviation of LSAS. How and why do these differ from the ones given by the functions `sd()` and `var()`?\n\n\n::: {.cell exercise='ex_10'}\n```{webr}\n#| exercise: ex_10\n\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_10\"}\nSee [Descriptive statistics](../labs/descriptive-statistics.qmd#numeric-data)\n\nYou can use the following code to create functions for the population variance and SD:\n\n``` r\n#creating a function to calculate the population variance \npop_var <- function(x){\n  1/length(x)*sum((x-mean(x))^2)\n}\n\n# and the population sd\npop_sd <- function(x){\n  sqrt(1/length(x)*sum((x-mean(x))^2))\n}\n```\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_10\"}\nThe full solution is:\n\n``` r\n#creating a function to calculate the population variance \npop_var <- function(x){\n  1/length(x)*sum((x-mean(x))^2)\n}\n\n# and the population sd\npop_sd <- function(x){\n  sqrt(1/length(x)*sum((x-mean(x))^2))\n}\n\n# variance\npop_var(df_data$lsas_screen) # population\nvar(df_data$lsas_screen) #sample\n\n# SD\npop_sd(df_data$lsas_screen) # population \nsd(df_data$lsas_screen) # sample\n\n```\n:::\n:::::\n\n\n\n\n::: {.cell setup='true' exercise='ex_11'}\n```{webr}\n#| setup: true\n#| exercise: ex_11\n# Load the CSV data\nlibrary(dplyr)\ndf_data <- read.csv(\"data/steps_clean.csv\")\n```\n:::\n\n\n::::: panel-tabset\n## BONUS Exercise 11\n\nUse only the first ten participants and compare the population and the sample variance and standard deviation of LSAS-SR. What do you find, and how do the results compare to those from the previous exercise?\n\n\n::: {.cell exercise='ex_11'}\n```{webr}\n#| exercise: ex_11\n#creating a function to calculate the population variance \npop_var <- function(x){\n  1/length(x)*sum((x-mean(x))^2)\n}\n\n# and the population sd\npop_sd <- function(x){\n  sqrt(1/length(x)*sum((x-mean(x))^2))\n}\n\n```\n:::\n\n\n## Hints\n\n::: {.hint exercise=\"ex_11\"}\n\nYou can use the following code to get the first 10 rows of your data\n\n``` r\ndf_data_10 <- df_data[1:10,]\n```\n:::\n\n## Solution\n\n::: {.solution exercise=\"ex_11\"}\nThe full solution is:\n\n``` r\n#creating a function to calculate the population variance \npop_var <- function(x){\n  1/length(x)*sum((x-mean(x))^2)\n}\n\n# and the population sd\npop_sd <- function(x){\n  sqrt(1/length(x)*sum((x-mean(x))^2))\n}\n\n#create dataset with only the first 10 participants\ndf_data_10 <- df_data[1:10,]\n\n# variance\npop_var(df_data_10$lsas_screen)\nvar(df_data_10$lsas_screen)\n\n# SD\npop_sd(df_data_10$lsas_screen)\nsd(df_data_10$lsas_screen)\n\n\n```\n:::\n:::::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}