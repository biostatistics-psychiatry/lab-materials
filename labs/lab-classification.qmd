---
title: "Lab: Classification"
number-sections: true
format: live-html
engine: knitr
webr:
  packages:
    - dplyr
    - tidymodels
    - here
resources:
  - ../data/steps_clean.csv
---

{{< include ../_extensions/r-wasm/live/_knitr.qmd >}}
{{< include ../_extensions/r-wasm/live/_gradethis.qmd >}}

In this lab, we'll explore classification metrics using the PHQ-9 scale. We'll practice calculating sensitivity, specificity, positive predictive value, and negative predictive value. The companion chapter [Classification](../chapters/classification.qmd) explains the concepts in more detail, so use it as a reference if you get stuck.

::: {.callout-tip}
While you can complete all the exercises in your browser, we recommend also practicing in RStudio. Using an editor like RStudio will help you build real-world skills for writing, running, and saving your R code.
:::

# Load and prepare the data

First, let's load the data and create the binary outcome variable we'll use for classification.

::: {#exr-phq9-binary}
## Create a binary variable of PHQ-9 at post-treatment
:::

:::: {.panel-tabset}
## Problem

For the exercises in this chapter, you will work with PHQ-9 instead of LSAS. Create a binary outcome variable `phq9_post_bin` that is "Low" if `phq9_post` is less than 10, and "High" otherwise.

```{webr}
#| label: ex-1
#| exercise: ex_1
#| envir: class_env

library(dplyr)
library(readr)
library(here)

# load data
df_data <- read_csv(here("data", "steps_clean.csv"))

# create binary outcome variable
df_data <- df_data |>
  mutate(
    phq9_post_bin = _____(phq9_post <__, "Low", "High"),
    phq9_post_bin = factor(phq9_post_bin, levels = c("Low", "High"))
  )

# Check the distribution
table(df_data$phq9_post_bin)
```

## Hints

::: {.hint exercise="ex_1"}

The cutoff for PHQ-9 is 10. Use `if_else()` to create the binary variable.

:::

## Solution

::: {.solution exercise="ex_1"}

```{webr}
#| label: ex-1-solution
#| exercise: ex_1
#| solution: true

library(dplyr)
library(readr)
library(here)

# load data
df_data <- read_csv(here("data", "steps_clean.csv"))

# create binary outcome variable
df_data <- df_data |>
  mutate(
    phq9_post_bin = if_else(phq9_post <10, "Low", "High"),
    phq9_post_bin = factor(phq9_post_bin, levels = c("Low", "High"))
  )

# Check the distribution
table(df_data$phq9_post_bin)
```
1. PHQ-9 cutoff of 10 separates low from high depression symptoms

The binary variable is now ready for classification analysis.

:::

```{webr}
#| label: ex-1-gradethis
#| exercise: ex_1
#| check: true
gradethis::grade_this_code()
```

::::

# Fit a logistic regression model

Now let's fit a logistic regression model to predict the binary outcome. We will use a few functions from the `tidymodels` package to achieve this, but regression models can be fit with other packages as well. Note also that regression models are not the focus of this course, so we will not go into the details of fitting them here.

::: {#exr-phq9-logistic}
## Fit a logistic regression model (PHQ-9)
:::

:::: {.panel-tabset}
## Problem

Fit a logistic regression model to predict `phq9_post_bin` using `phq9_screen` and `group`. Create a confusion matrix for the predictions.

```{webr}
#| label: ex-2
#| exercise: ex_2
#| envir: class_env

# load the tidymodels package
library(tidymodels)

# fit logistic regression model
phq9_fit <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification") |>
  fit(___ ~ ___ + ___, data = df_data)

# Make predictions
phq9_pred <- predict(phq9_fit, new_data = df_data) |>
  bind_cols(df_data)

# Create confusion matrix
phq9_conf_mat <- conf_mat(phq9_pred, truth = phq9_post_bin, estimate = .pred_class)

# Display confusion matrix
phq9_conf_mat
```

## Hints

::: {.hint exercise="ex_2"}

The `tidymodels` workflow is:  
1. Specify the model type: `logistic_reg()`  
2. Set the engine: `set_engine("glm")`  
3. Set the mode: `set_mode("classification")`  
4. Fit the model: `fit(formula, data)`  
5. Make predictions: `predict(model, new_data)`  
6. Create confusion matrix: `conf_mat(data, truth, estimate)`  

The missing step here is the formula. The typical formula is `outcome ~ predictors`.

:::

## Solution

::: {.solution exercise="ex_2"}

```{webr}
#| label: ex-2-solution
#| exercise: ex_2
#| solution: true
library(tidymodels)

# Fit logistic regression model
phq9_fit <- logistic_reg() |> #<1>
  set_engine("glm") |> #<2>
  set_mode("classification") |> #<3>
  fit(phq9_post_bin ~ phq9_screen + group, data = df_data) #<4>

# Make predictions
phq9_pred <- predict(phq9_fit, new_data = df_data) |> #<5>
  bind_cols(df_data)

# Create confusion matrix
phq9_conf_mat <- conf_mat(phq9_pred, truth = phq9_post_bin, estimate = .pred_class) #<6>

# Display confusion matrix
phq9_conf_mat
```
1. Specify logistic regression model
2. Use GLM engine
3. Set to classification mode
4. Fit model with predictors
5. Make predictions on the data
6. Create confusion matrix

The confusion matrix shows how well our model predicts the binary outcome.

:::

```{webr}
#| label: ex-2-gradethis
#| exercise: ex_2
#| check: true
gradethis::grade_this_code()
```

::::

# Calculate classification metrics manually

Now let's calculate the key classification metrics manually by looking at the confusion matrix. We will look at *sensitivity*, *specificity*, *positive predictive value*, and *negative predictive value*. All you need are the four numbers from the confusion matrix!

If you need to refresh your memory about which numbers are needed for which metric, look in the companion chapter [Classification](../chapters/classification.qmd#evaluate-classification-manually).

::: {#exr-sensitivity}
## Calculate sensitivity
:::

:::: {.panel-tabset}
## Problem

We begin by extracting all values from the confusion matrix. Then we will use the relevant values for each metric.

Calculate the sensitivity (True Positive Rate) for the PHQ-9 predictions. Extract the values from the confusion matrix first.

```{webr}
#| label: ex-3
#| exercise: ex_3
#| envir: class_env

# Extract values from confusion matrix
true_pos <- phq9_conf_mat$table[1, 1]  # True positives
false_pos <- phq9_conf_mat$table[2, 1] # False positives
false_neg <- phq9_conf_mat$table[1, 2] # False negatives
true_neg <- phq9_conf_mat$table[2, 2]  # True negatives

# Calculate sensitivity (True Positive Rate)
sensitivity <- ____ / (____ + ____)

# Display result
sensitivity
```

## Hints

::: {.hint exercise="ex_3"}

To calculate sensitivity you need the *true positives* and *false negatives*.

:::

## Solution

::: {.solution exercise="ex_3"}

```{webr}
#| label: ex-3-solution
#| exercise: ex_3
#| solution: true

# Extract values from confusion matrix
true_pos <- phq9_conf_mat$table[1, 1]  # True positives
false_pos <- phq9_conf_mat$table[2, 1] # False positives
false_neg <- phq9_conf_mat$table[1, 2] # False negatives
true_neg <- phq9_conf_mat$table[2, 2]  # True negatives

# Calculate sensitivity (True Positive Rate)
sensitivity <- true_pos / (true_pos + false_neg) #<1>

# Display result
sensitivity
```
1. Sensitivity: proportion of actual positives correctly identified

Sensitivity tells us how well our model identifies true positive cases.

:::

```{webr}
#| label: ex-3-gradethis
#| exercise: ex_3
#| check: true
gradethis::grade_this_code()
```

::::

::: {#exr-specificity}
## Calculate specificity
:::

:::: {.panel-tabset}
## Problem

Calculate the specificity (True Negative Rate) for the PHQ-9 predictions.

```{webr}
#| label: ex-4
#| exercise: ex_4
#| envir: class_env

# Calculate specificity (True Negative Rate)
specificity <- ____ / (____ + ____)

# Display result
specificity
```

## Hints

::: {.hint exercise="ex_4"}

To calculate specificity you need the *true negatives* and *false positives*.

:::

## Solution

::: {.solution exercise="ex_4"}

```{webr}
#| label: ex-4-solution
#| exercise: ex_4
#| solution: true

# Calculate specificity (True Negative Rate)
specificity <- true_neg / (true_neg + false_pos) #<1>

# Display result
specificity
```
1. Specificity: proportion of actual negatives correctly identified

Specificity tells us how well our model identifies true negative cases.

:::

```{webr}
#| label: ex-4-gradethis
#| exercise: ex_4
#| check: true
gradethis::grade_this_code()
```

::::

::: {#exr-ppv}
## Calculate positive predictive value
:::

:::: {.panel-tabset}
## Problem

Calculate the positive predictive value (PPV) for the PHQ-9 predictions.

```{webr}
#| label: ex-5
#| exercise: ex_5
#| envir: class_env

# Calculate positive predictive value (Precision)
ppv <- ____ / (____ + ____)

# Display result
ppv
```

## Hints

::: {.hint exercise="ex_5"}

To calculate PPV you need the *true positives* and *false positives*.

:::

## Solution

::: {.solution exercise="ex_5"}

```{webr}
#| label: ex-5-solution
#| exercise: ex_5
#| solution: true

# Calculate positive predictive value (Precision)
ppv <- true_pos / (true_pos + false_pos) #<1>

# Display result
ppv
```
1. PPV: proportion of predicted positives that are actually positive

PPV tells us how reliable our positive predictions are.

:::

```{webr}
#| label: ex-5-gradethis
#| exercise: ex_5
#| check: true
gradethis::grade_this_code()
```

::::

::: {#exr-npv}
## Calculate negative predictive value
:::

:::: {.panel-tabset}
## Problem

Calculate the negative predictive value (NPV) for the PHQ-9 predictions.

```{webr}
#| label: ex-6
#| exercise: ex_6
#| envir: class_env

# Calculate negative predictive value
npv <- ____ / (____ + ____)

# Display result
npv
```

## Hints

::: {.hint exercise="ex_6"}

To calculate NPV you need the *true negatives* and *false negatives*.

:::

## Solution

::: {.solution exercise="ex_6"}

```{webr}
#| label: ex-6-solution
#| exercise: ex_6
#| solution: true

# Calculate negative predictive value
npv <- true_neg / (true_neg + false_neg) #<1>

# Display result
npv
```
1. NPV: proportion of predicted negatives that are actually negative

:::

```{webr}
#| label: ex-6-gradethis
#| exercise: ex_6
#| check: true
gradethis::grade_this_code()
```

::::

# Summary

In this lab, you learned:

1. **Binary outcome creation**: How to create binary outcomes from continuous variables using meaningful cutoffs
2. **Logistic regression**: How to fit classification models using `tidymodels`
3. **Confusion matrices**: How to create and interpret confusion matrices
4. **Classification metrics**: How to calculate sensitivity, specificity, PPV, and NPV using the confusion matrix
