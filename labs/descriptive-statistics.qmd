---
title: "Descriptive statistics"
---

In this lab we will practice calculating and interpreting descriptive statistics using the STePS dataset. We start by loading the cleaned dataset saved in the [Import and clean data](import-clean.qmd) lab.

```{r}
#| label: load-packages
#| message: false
library(here)
library(tidyverse)

d <- read_csv(here("data", "steps_clean.csv"))
```

We'll start of doing some basic descriptive statistics of the baseline variables. We start by checking the variable names to identify the baseline variables.

```{r}
#| label: colnames
colnames(d)
```

For easy handling, we'll create a data frame containing only the baseline variables, ID, Group and all variables ending with the suffix "\_screen".

```{r}
#| label: select-baseline
d_bl <- d |>
  select(
    id,
    group,
    lsas_screen,
    gad_screen,
    phq9_screen,
    bbq_screen,
    scs_screen,
    dmrsodf_screen,
    ders_screen,
    pid_5_screen
  )
```

Or more efficiently (if variables are correctly named) we can use the `ends_with()` function from the *tidyselect* package:

```{r}
#| label: ends-with-screen
d_bl <- d |>
  select(
    id,
    group,
    ends_with("_screen")
  )
```

::: {#exr-dataset-overview .callout-caution collapse="true"}
## Get an overview of the dataset

**2.1** Use the functions described in the [Import and clean data](import-clean.qmd) lab to get a quick overview of the dataset and give a brief summary of it.
:::

# Numeric data

We'll start exploring descriptive statistics by examining baseline levels of the LSAS-questionnaire, which measures symptoms of social anxiety and was used as the outcome variable in the study.

## Visual presentations

The simplest, and often most informative way, to get an overview of a variable is to produce a visual representation of its distribution. For uni-variable numeric variables, two common visualizations are histograms `hist()` and boxplots `boxplot()`.

### Histogram

A **histogram** is a graphical representation used to visualize the **distribution of a numeric variable**. It shows how data are **spread across intervals (bins)** and helps identify patterns such as **central tendency, variability, skewness**, and **outliers**.

| Feature            | What It Tells You                  |
|--------------------|------------------------------------|
| **Height of bars** | Number of observations in each bin |
| **Width of bars**  | Range of values grouped together   |
| **Shape**          | Symmetry, skewness, modality       |
| **Outliers**       | Bars isolated from the main group  |

```{r}
#| label: hist-lsas-screen
hist(d_bl$lsas_screen)
```

#### Boxplot

A **boxplot** (also called a **box-and-whisker plot**) is a compact, visual summary of the **distribution, central tendency, and variability** of a dataset.

| Component             | Description                              |
|-----------------------|------------------------------------------|
| **Minimum**           | Smallest value (excluding outliers)      |
| **Q1 (1st Quartile)** | 25th percentile (lower hinge of the box) |
| **Median (Q2)**       | 50th percentile (line inside the box)    |
| **Q3 (3rd Quartile)** | 75th percentile (upper hinge of the box) |
| **Maximum**           | Largest value (excluding outliers)       |

```{r}
#| label: boxplot-lsas-screen
boxplot(d_bl$lsas_screen)
```

## Centrality measures

Sometimes we need more comprehensive summaries of our data. For this purpose, it is common to use **centrality measures**. Centrality measures are statistical summaries that describe the **center or typical value** of a dataset. They help summarize where most values lie and include:

-   **Mean**: The average of all values (sum of all values divided by the number of values).

-   **Median**: The middle value when data is ordered (or the average of the two middle numbers if the length of the vector is an even number).

-   **Mode**: The most frequently occurring value.

These measures provide insight into the **distribution’s central tendency**, helping you understand the "typical" case in your data.

::: callout-note
## Means, medians and outliers

The mean is sensitive to outliers, while the median is not. The values \[1, 2, 3, 100\] has a mean of 26.5. and a median of 2.5. None of them are "wrong", but the usefulness of each measure depends on what you want your centrality measures to tell. However, with highly skewed variables (e.g. income), the median is usually viewed as more informative.
:::

Now let's use R to calculate some centrality measures for LSAS at baseline.

### Getting the mean using `mean()`

```{r}
#| label: mean-lsas-screen
mean(d_bl$lsas_screen)
```

### Getting the median using `median()`

```{r}
#| label: median-lsas-screen
median(d_bl$lsas_screen)
```

### Getting the mode

Unlike `mean()` and `median()`, base R does not include a built-in `mode()` function for computing the statistical mode (i.e. the most frequent value). However, you can create one.

```{r}
#| label: mode-function
get_mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
get_mode(d_bl$lsas_screen)
```

Much of this information could also be easily found using the `summary()` function.

```{r}
#| label: summary-lsas-screen
summary(d_bl$lsas_screen)

```

### Visualizing centrality measures

You could also plot the mean, median and mode over the histogram, to get a better sense of the data.

```{r}
#| label: hist-lsas-annotated
hist(d_bl$lsas_screen,
  main = "Histogram with Mean, Median, and Mode",
  xlab = "Values", probability = TRUE
)

# Add lines for mean, median, and mode
abline(v = mean(d_bl$lsas_screen), col = "blue", lwd = 2, lty = 2) # Mean
abline(v = median(d_bl$lsas_screen), col = "red", lwd = 2, lty = 2) # Median
abline(v = get_mode(d_bl$lsas_screen), col = "darkgreen", lwd = 2, lty = 2) # Mode

```

::: {#exr-visualize-centrality .callout-caution collapse="true"}
## Visualize centrality measures

**2.2** Visualize the distribution of the PHQ-9 scale and the PID-5 scale and provide the mean, median and mode.

**2.3** How does the centrality measures differ and why?

**2.4** Reason on the pros and cons of the different centrality measures for these scales.
:::

## Spread measures

Centrality measures give information on the most typical value in the distribution, but no info on the spread of values. For example, the two distributions below have the same mean value (0), but different spread (standard deviation 0.2 vs. 1).

```{r}
#| label: hist-lsas-spread
#| echo: false
par(mfrow = c(1, 2))
curve(dnorm(x, mean = 0, sd = 0.2),
  from = -4, to = 4,
  col = "black", lwd = 2,
  main = "",
  xlab = "", ylab = ""
)
abline(v = 0, col = "red", lwd = 2)
curve(dnorm(x, mean = 0, sd = 1),
  from = -4, to = 4,
  col = "black", lwd = 2,
  main = "",
  xlab = "", ylab = ""
)
abline(v = 0, col = "red", lwd = 2)
```

To get a summary of the spread of the data, we use different **spread measures.** Spread measures describe how much the data **varies or is dispersed** around a central value like the mean or median. They help you understand whether the values are tightly clustered or widely scattered. Commonly used spread measures include:

-   **Range** `range()`: The difference between the maximum and minimum values. Simple but sensitive to outliers.

    $$\text{Range} = \max(X) - \min(X)$$

-   **Interquartile Range (IQR)** `IQR()`: The range of the middle 50% of data (Q3 − Q1). More robust against extreme values.

    $$\mathrm{IQR} = Q_3 - Q_1$$

-   **Variance** `var()`: The average of the squared deviations from the mean. It gives more weight to larger deviations.

    Population variance

    $$
    \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2
    $$

    Sample variance

    $$
    s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
    $$

-   **Standard Deviation (SD)** `sd()`: The square root of variance. It measures average distance from the mean and is widely used in statistics.

    Population SD

    $$
    \sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2}
    $$

    Sample SD

    $$ s = \sqrt{\frac{1}{n - 1} \sum_{i=1}^{n} (x_i - \bar{x})^2}$$

    ::: callout-note
    ## Population vs samples

    The population variance ($\sigma^2$) and standard deviation ($\sigma$) are used when you have data for the **entire population** — that is, every single value of interest is included.

    The sample variance ($s^2$) and standard deviation ($s$) are used when you're working with a **subset (sample)** of a larger population. It includes a correction (called **Bessel’s correction**) to account for the fact that samples tend to underestimate variability. The **sample SD** divides by $n -1$ to compensate for the fact that we use $\bar{x}$, an estimate of the true population mean ($\mu$). This correction makes the sample variance an **unbiased estimator** of the population variance.

    The functions `sd`() and `var()` gives the sample standard deviation and variance.
    :::

::: {#exr-calculate-spread .callout-caution collapse="true"}
## Calculate spread measures

**2.5** Calculate spread measures for the LSAS-SR scale, what do they tell you and which ones do you think are most useful to describe the spread of the values. Motivate your answer briefly!

**Overcourse:**

**2.6** Calculate the *population variance and standard deviation*. How do these differ from the ones given by the functions `sd()` and `var()`.

**2.7** Use only the first ten participants and compare the population and the sample variance and standard deviation of LSAS-SR. What do you find, and how do the results compare to those from exercise 2.6.
:::

# Categorical data

Now we'll have a look at categorical data. First, we need to simulate some categorical data (since this is not included in the STePS dataset).

```{r}
#| label: simulate-categorical
# Simulating a gender variable
n <- nrow(d_bl)
d_bl$gender <- rbinom(n, 1, 0.7)
d_bl$gender <- ifelse(d_bl$gender == 1, "Woman", "Man")

# Simulate an education variable
education_levels <- c("Primary", "Secondary", "University")
education_probs <- c(0.4, 0.4, 0.2) # Adjust probabilities as needed

d_bl$education <- sample(education_levels, size = n, replace = TRUE, prob = education_probs)

# Simulate an income variable
income_levels <- c("Low", "Medium", "High")
income_probs <- c(0.2, 0.6, 0.2) # Adjust probabilities as needed

d_bl$income <- sample(income_levels, size = n, replace = TRUE, prob = income_probs)
```

The `table()` function provides a quick overview of the number of values in each category.

```{r}
#| label: table-function
table(d_bl$gender)
table(d_bl$education)
```

Dived by the number of rows, it gives proportion in each category.

```{r}
#| label: table-function-prop
table(d_bl$gender) / nrow(d_bl)
table(d_bl$education) / nrow(d_bl)
```

::: callout-note
## Spread of categorical data

We do not need spread measures for categorical data, as all information there is about the distribution in provided in the counts or proportions. We can however calculate the variance of a proportion
:::

Sample variance of a proportion

$$
\mathrm{Var}(\hat{p}) = \frac{\hat{p}(1-\hat{p}))}{n}
$$

## Visual presentations

As for numeric variables, visualizations can help get a better sense for the distribution of the data. Two common ways are **barcharts** and **piecharts**

### Barcharts

The height of each bar gives the number of occurrences of each category. When you use the base function `plot()` on a factor level variable, it gives you a barchart.

```{r}
#| label: plot-education
plot(as.factor(d_bl$education))
```

### Piecharts

A piechart shows the distribution of a categorical variable as a pie, with the size of each piece representing the proportion of each level of the categorical variable.

```{r}
#| label: pie-education
pie(table(d_bl$education))
```

::: {#exr-visualize-categorical .callout-caution collapse="true"}
## Visualize categorical data

**2.8** Calculate the counts, proportions and percentages for the simulated income categories and visualize the distribution.
:::

# Bi-variable distributions

It can also be useful to investigate bi-variable distributions - i.e. the joint distribution of two variables - to get a sense of how variables in your data is related to each other.

## Numeric by numeric distributions

For two numeric variables, the most common visualization is the **scatter plot.** It shows the distribution of each datapoint with one variable on the x-axis and the other on the y-axis. Using the `plot()` function with two numeric variables will give you a scatter plot.

```{r}
#| label: scatter-plot
plot(d_bl$lsas_screen, d_bl$gad_screen)
```

## Numeric by categorical distributions

The joint distribution of a numeric and a categorical variable can be visualized as a stratified boxplots. For this let's look at the distribution of LSAS scores for people with high generalized anxiety (GAD-7 ≥ 10 or more) vs low generalized anxiety (GAD-7 \< 10).

```{r}  
#| label: boxplot-lsas-gad
# create a variable indicating if GAD-7 is 10 or more
d_bl$gad_cat <- ifelse(d_bl$gad_screen >= 10, "High anxiety", "Low anxiety")

boxplot(lsas_screen ~ gad_cat, data = d_bl)
```

## Categorical by categorical distributions

All the information of the joint distribution of two categorical variables can be seen using a **cross-table**. For this let's look at the distribution of high vs low depression (PHQ-9 ≥ 10 vs PHQ-9 \<10 or less) against high vs low generalized anxiety.

```{r}
#| label: crosstable
d_bl$phq_cat <- ifelse(d_bl$phq9_screen >= 10, "High depression", "Low depression")

table(d_bl$gad_cat, d_bl$phq_cat)
```

Although all information about the distribution is available in the crosstable, you may still want to visualize this distribution. One way is to use a **mosaic plot**, which you can get by providing cross-table to the `plot()` function.

```{r}
#| label: mosaic-plot
plot(table(d_bl$gad_cat, d_bl$phq_cat), main = "Depression and anxiety")
```

## Descriptive statistics using the 'tableone' package

A convenient way to get descriptive statistics for a range of variables is to use the *tableone* package and the `CreateTableOne()` function. First let get some descriptives for the overall sample

```{r}
#| label: tableone-package
# install.packages("tableone")
library(tableone)

# define the variables you want
vars <- c(
  "lsas_screen",
  "gad_screen",
  "phq9_screen",
  "bbq_screen",
  "scs_screen",
  "dmrsodf_screen",
  "ders_screen",
  "pid_5_screen",
  "gender",
  "education",
  "income"
)

CreateTableOne(vars = vars, data = d_bl)
```

We can also do this stratified by a categorical variable using the strata argument. Let's have it by treatment group.

```{r}
#| label: tableone-package-strata
CreateTableOne(vars = vars, data = d_bl, strata = "group", test = FALSE)
```

::: {#exr-visualize-bivariate .callout-caution collapse="true"}
## Visualize bi-variable distributions

**2.9** Visualize the joint distribution of GAD-7 and PHQ-9 as numeric variables and describe what you see

**2.10** Visualize the distribution of of LSAS scores by income level and describe what you see

**2.11** Create a variable for high vs. low DERS scores and investigate the joint distribution of this variable and phq_cat (that we created in an earlier example)

**2.12** Create a table using the tableone package to show descriptives statistics stratified by high vs low depression levels. Briefly interpret what you see.
:::

# Save output

```{r}
#| label: save-output

write_rds(d_bl, here("data", "steps_baseline.rds"))
```
