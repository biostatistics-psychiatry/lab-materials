---
title: "Sampling from a population and testing mean and proportions"
format: html
editor: visual
execute: 
  freeze: false
  warning: false
---

```{r}
#| label: load-packages

library(here)
library(tidyverse)
d_bl <- read_rds(here("data", "steps_baseline.rds"))
```

## Sampling from a population

In yesterdays lab, we restricted ourselves to describing the sample that we have. Often, however, we are not only interested about our sample, but want to make inferences about the **population** from which our sample came.

### Central Limit Theorem (CLT):

No matter what the population distribution looks like, if you take many random samples and calculate their means, those samples means will form a **normal distribution** as the sample size grows large.

-   Sample means become normally distributed.

-   Happens even if the original population is **not normal**.

-   Works better as **sample size increases** (n â‰¥ 30 is a common rule).

Here is a simulation to visualize what the relation between the distribution of a variable in the population and the sampling distribution of the means from that same population.

```{r}
#| echo: false

# Parameters
population <- rnorm(10000, mean = 50, sd = 10) # simulate a population
sample_size <- 30
num_samples <- 1000
sample_means <- numeric(num_samples)

par(mfrow = c(2, 1))
hist(population, xlim = c(15, 85), main = "Normal population distrubution")

# Draw multiple samples and compute their means
for (i in 1:num_samples) {
  sample_i <- sample(population, sample_size, replace = TRUE)
  sample_means[i] <- mean(sample_i)
}

# Plot the sampling distribution of the mean
hist(sample_means,
  breaks = 30, probability = TRUE,
  main = paste("Sampling Distribution of the Mean, \n 1000 samples of n=30"),
  xlab = "Sample Mean", col = "skyblue", border = "white", xlim = c(15, 85)
)

# Add a normal curve for comparison
curve(dnorm(x, mean = mean(sample_means), sd = sd(sample_means)),
  col = "red", lwd = 2, add = TRUE
)

```

```{r}
#| echo: false

# Parameters
population <- runif(10000, min = 15, max = 85) # simulate a population
sample_size <- 30
num_samples <- 1000
sample_means <- numeric(num_samples)

par(mfrow = c(2, 1))
hist(population, xlim = c(15, 85), main = "Uniform population distrubution")

# Draw multiple samples and compute their means
for (i in 1:num_samples) {
  sample_i <- sample(population, sample_size, replace = TRUE)
  sample_means[i] <- mean(sample_i)
}

# Plot the sampling distribution of the mean
hist(sample_means,
  breaks = 30, probability = TRUE,
  main = paste("Sampling Distribution of the Mean, \n 1000 samples of n=30"),
  xlab = "Sample Mean", col = "skyblue", border = "white", xlim = c(15, 85)
)

# Add a normal curve for comparison
curve(dnorm(x, mean = mean(sample_means), sd = sd(sample_means)),
  col = "red", lwd = 2, add = TRUE
)


```

```{r}
#| echo: false

# Parameters
population <- c(rnorm(10000 / 2, mean = 65, sd = 5), rnorm(10000 / 2, mean = 35, sd = 5)) # simulate a population
sample_size <- 30
num_samples <- 1000
sample_means <- numeric(num_samples)

par(mfrow = c(2, 1))
hist(population, xlim = c(15, 85), breaks = 30, main = "Bi-modal population distribution")

# Draw multiple samples and compute their means
for (i in 1:num_samples) {
  sample_i <- sample(population, sample_size, replace = TRUE)
  sample_means[i] <- mean(sample_i)
}

# Plot the sampling distribution of the mean
hist(sample_means,
  breaks = 30, probability = TRUE,
  main = paste("Sampling Distribution of the Mean, n 1000 samples of n=30"),
  xlab = "Sample Mean", col = "skyblue", border = "white", xlim = c(15, 85)
)

# Add a normal curve for comparison
curve(dnorm(x, mean = mean(sample_means), sd = sd(sample_means)),
  col = "red", lwd = 2, add = TRUE
)

```

### **Law of Large Numbers (LLN):**

As you collect more and more observations, the **sample mean** (or proportion) will get closer and closer to the **true population mean**.

-   More data = more accuracy.

-   Guarantees that with enough data, **random variation** averages out.

Imagine that we had the patience to flip a coin 1000 times. For each toss, we calculated the number of heads we've gotten so far and divided by the total tosses, to get the proportion that landed heads. Even if we expect the coin to be fair, we wouldn't expect it to be 50-50 heads and tails in the beginning. In fact, at the first toss that would be impossible, As we continued, however, we would see the proportion of heads gradually stabilize around 0.5.

```{r}
#| echo: false
# Simulate coin tosses (0 = Tails, 1 = Heads)
num_tosses <- 1000
tosses <- rbinom(num_tosses, 1, 0.5)

# Calculate cumulative proportion of heads
toss_data <- data.frame(
  TossNumber = 1:num_tosses,
  ProportionHeads = cumsum(tosses) / (1:num_tosses)
)

# Plot cumulative proportion
ggplot(toss_data, aes(x = TossNumber, y = ProportionHeads)) +
  geom_line(color = "steelblue", size = 1) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  labs(
    title = "Law of Large Numbers: Coin Toss Simulation",
    x = "Number of Tosses",
    y = "Proportion of Heads"
  ) +
  theme_minimal()
```

Together the central limit theorem and the law of large numbers tells us that when sampling from a population 1) the sampling distribution approaches a normal distribution as the number of samples increases, AND 2) the value of the sample mean $\bar{x}$, the estimate, will approach the true population mean $\mu$ as the sample size increase.

### Sampling in research

This is all fine and well, but you may find yourself wondering "How does this help my PhD?". You won't be able to sample repeatedly from the population, if you are lucky you'll have ONE sample to work with. But there is an upside! You can use you sample values to estimate *the standard deviation of the sampling distribution*, also known as the **standard error (SE).**

#### Standard error of a mean

The formula for the standard error of a mean, if we knew the population variance $\sigma^2$, is:

$$
SE = {\sqrt{\sigma^2 / n}}
$$

However, we rarely know the population standard deviation. Luckily we can use the sample variance $s^2$ to estimate it:

$$
SE = {\sqrt{s^2 / n}}
$$

#### Standard error of a proportion

For a proportion, the standard error for the population is calculated by the formula:

$$
\mathrm{SE}(p) = \sqrt{\frac{p(1 - p)}{n}}
$$

##### Excerises

**3.1** Estimate the standard error of LSAS_Screen in the STePS study using the formula above, and describe in words what the number means

**3.2** Calculate the standard error for the proportion of men in the STePS study, and describe the meaning of this number in words

**3.3** Describe what would happen to these standard errors if the sample size had been 1000 participants and explain why?

## Probability

The law of large numbers brings us to the concept of **probability.** In frequentist statistics, probability is defined as the long run frequency of an event as the number of events approach infinity, $\infty$. For instance the proportion of heads in an infinite number of coin tosses will approach 0.5.

### P-values

Probability - defined as the long-run frequency of an event occurring - is very much used in research to get an idea of **how probable our observed data is, given some hypothesis of interest.** In probability notation $P(Data|Hypothesis)$.

For instance we might have an hypothesis the the mean of LSAS-SR is 82 in the population. We can determine how probable our data would be under this hypothesis, by investigating how often we would expect to get our **observed sample mean** if this (null)hypothesis was true.

For this we would need the sampling distribution of mean LSAS-SR scores in samples of 181 people (the size, $n$, of our sample), *if the true population mean was 82.* One way to get this would be to simulate say 10 000 samples of LSAS-SR scores, from a population with a true mean of 82. To simulate this, we also need to know the spread (standard deviation) of the true population. We don't know this, but let's assume it is the same as in our sample.

Below, the function `rnorm()` is used to take a random sample of 181 values from a normal distribution with a mean of 82 and a standard deviation 16.5 (same as in out sample). We use a for loop to repeat this sampling 10 000 times and save the mean values of each sample in the vector called means.

```{r}
n_samples <- 1e4 # the number of samples
smp_size <- 181 # the size of our samples
means <- rep(NA, n_samples) # an empty vector to contain our mean values

for (i in 1:n_samples) {
  x <- rnorm(smp_size, mean = 82, sd = sd(d_bl$lsas_screen))
  means[i] <- mean(x)
}

hist(means, main = "Simulated sampling distribution of LSAS means")
```

\
We can use this simulated sampling distribution to see how probable our *observed LSAS-SR mean* is if the (null)hypothesis that the true mean is 82 would be correct. First let plot the sampling distribution again, and show the observed LSAS-SR mean as a vertical line.

```{r}
hist(means, main = "Simulated sampling distribution of LSAS means")
abline(v = mean(d_bl$lsas_screen), col = "red", lwd = 2, lty = 2) # vertical line showing the observed LSAS-SR mean
```

We can also quantify the probability by calculating, ***the proportion of times that a sample mean would be equal to or greater that our observed mean, IF the true population mean was 82*****.** This quantity is the very (in)famous **p-value.**

```{r}
mean(means >= mean(d_bl$lsas_screen)) # proportion of simulated means that are larger than our observed mean
```

If we find this simulation exercise a bit tedious, we could also use theoretical distributions for the sample means to calculate our p-value. The **t-distribution** can be used to estimate the spread to the sample means *when the population variance is unknown and the sample variance is used to approximate it*. It is very similar to the normal distribution, but has heavier tails that accounts for the uncertainty produced by using the sample variance instead of the true population variance when estimating the standard error of the sampling distribution. However, when the sample size increase, the t-distribution will come closer and closer to a normal distribution (also known as a z-distribution when standardized to have mean=0 and sd=1).

```{r}
# Set up the plot range
x_range <- seq(-4, 4, length = 500)

# Plot standard normal distribution
plot(x_range, dnorm(x_range),
  type = "l", lwd = 2, col = "black",
  ylab = "Density", xlab = "x", main = "t-Distribution vs Normal Distribution"
)

# Add t-distributions with different degrees of freedom
lines(x_range, dt(x_range, df = 1), col = "red", lwd = 2, lty = 2)
lines(x_range, dt(x_range, df = 5), col = "blue", lwd = 2, lty = 3)
lines(x_range, dt(x_range, df = 15), col = "darkgreen", lwd = 2, lty = 4)
lines(x_range, dt(x_range, df = 30), col = "purple", lwd = 2, lty = 5)

# Add a legend
legend("topright",
  legend = c("Normal (Z)", "t (df=1)", "t (df=5)", "t (df=15)", "t (df=30)"),
  col = c("black", "red", "blue", "darkgreen", "purple"),
  lwd = 2, lty = 1:5, bty = "n"
)

```

The probability of getting a a sample mean that is greater or equal to our observed mean can be calculated by transforming our observed mean to a **t-value** and compare it to the **t-distribution**.

The t-value of our mean $\bar{x}$ under the null hypothesis that the population mean is $\mu$, is given by the formula:

$$
t = \frac{\bar{x} - \mu}{SE}
$$

Replacing these greek letters with our actual values $\bar{x} = 84.75$, $\mu = 82$, and $SE=1.22$, we get:

```{r}
se <- sd(d_bl$lsas_screen) / sqrt(nrow(d_bl))
x_bar <- mean(d_bl$lsas_screen)
t_value <- (x_bar - 82) / se
t_value
```

Now let's see the probability of getting a value larger or equal to this - **our one-sided p-value!** For this we use the `pt()` function, that provides the cumulative probability up until a given t-value. 1 minus this cumulative probability gives the probability of values equal or above the given t-value.

```{r}
1 - pt(t_value, df = 180)
```

::: callout-note
## z-values and t-values

When the sample size increases, the t-distribution approaches the z-distribution and these estimates become very similar. As a general rule of thumb, it is fine to use z-values rather than t-values for sample sizes larger than 200.
:::

We could also get a very similar p-value from the z-distribution (although we would assume we have the population variance for out calculation of the standard error). If this was a proportion, a z-test would be the one to use, since we are not using any estimates when calculating the standard error.

```{r}
1 - pnorm(t_value)
```

Very similar to our simulated p-value above! More conveniently, of course, we could get this p-value using the `t.test()` function.

```{r}
t.test(d_bl$lsas_screen, mu = 82, alternative = "greater")
```

::: callout-note
## One-sided and two-sided p-values

What we have calculated, bow in three different ways, is the **one-sided** p-value. This is because we only looked at the probability to *get data equal to or greater* than our observed data, given that the null-hypothesis was true.

If we wanted to see the probability of getting data equal or greater than our observed data OR equal or less than out observed data under the null-hypothesis, we would want a **two-sided** p-value. Since the sampling distribution is symmetrical, we could get this by multiplying of one-sided p-value by 2.
:::

```{r}
#| echo: false
# Define the test statistic
z_stat_one <- 1.645
z_stat_two <- 1.96

# Generate normal distribution
x <- seq(-4, 4, length = 1000)
y <- dnorm(x)
plot_df <- data.frame(x = x, y = y)

# ---- One-sided plot ----
p_one_sided <- ggplot(plot_df, aes(x = x, y = y)) +
  geom_line(color = "black") +
  geom_area(data = subset(plot_df, x >= z_stat_one), aes(y = y), fill = "blue", alpha = 0.5) +
  geom_vline(xintercept = z_stat_one, color = "blue", linetype = "dashed") +
  labs(
    title = "One-sided p-value (Right Tail)",
    x = "Z", y = "Density"
  ) +
  theme_minimal()

# ---- Two-sided plot ----
p_two_sided <- ggplot(plot_df, aes(x = x, y = y)) +
  geom_line(color = "black") +
  geom_area(data = subset(plot_df, x >= z_stat_two), aes(y = y), fill = "red", alpha = 0.5) +
  geom_area(data = subset(plot_df, x <= -z_stat_two), aes(y = y), fill = "red", alpha = 0.5) +
  geom_vline(xintercept = c(-z_stat, z_stat_two), color = "red", linetype = "dashed") +
  labs(
    title = "Two-sided p-value (Both Tails)",
    x = "Z", y = "Density"
  ) +
  theme_minimal()

# ---- Plot results ----
print(p_one_sided)
print(p_two_sided)

```

Code for the two-sided p-value

```{r}
# manual code
se <- sd(d_bl$lsas_screen) / sqrt(nrow(d_bl))
x_bar <- mean(d_bl$lsas_screen)
t_value <- (x_bar - 82) / se
(1 - pt(t_value, df = 180)) * 2

# or using the t-test function
t.test(d_bl$lsas_screen, mu = 82, alternative = "two.sided")
```

##### Exercises

**3.4** Calculate the two-sided p-value for the null hypothesis that the mean PHQ-9 value in the underlying population is 9, and describe in words what this number mean.

**3.5** Calculate the p-value for getting our observed proportion of men, $\hat{p}$, if the the true population proportion, $p$, is 40% or more using a z-test.

HINT: use the standard error of the proportion: $$
\mathrm{SE}(p) = \sqrt{\frac{p(1 - p)}{n}}
$$

and combine with the formula for the z-scores

$$
z= \frac{p - \hat{p}}{SE}
$$

**3.6** Modify the simulation code for the sampling distribution above to determine what would happen to the p-value if the sample size was 10, 100 or 1000

### Confidence intervals

Using the standard error, we can also calculate the confidence interval, defined as **an interval that, if computed on a repeated set of samples, would contain the true population statistic 95% of the times.**

When the sample standard deviation is used, we get the confidence intervals by taking the observed mean and adding or subtracting the *t-value* of the desired percentiles of the sampling distribution (indicated by the asterix) times the *standard error.*

$$ \text{CI} = \bar{x} \pm t^*\ \frac{s}{\sqrt{n}}$$

If we knew the *stardard error of the population,* we could substitute the sample variance $s$ for the population variance $\sigma$, and use ***z**-values* instead of *t-values.* For a 95% confidence intervals, the z-value is 1.96.

$$
\text{CI} = \bar{x} \pm z^*\frac{\sigma}{\sqrt{n}}
$$

For a proportion, the confidence intervals becomes:

$$
\hat{p} \pm z^* \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}}
$$

::: callout-caution
## Confidence intervals for proportions

This confidence intervals for proportion, known as Wald confidence intervals, is easy to compute. However, since it uses the sample proportion to estimate the population proportion, they can be erratic, especially when $\hat{p}$ approach 0 or 1. We therefore recommend using more advanced confidence intervals, calculated by statistical software, for instance using the function `prop.test()`.
:::

Now let's use these formulas to calculate the confidence interval of the mean of LSAS-SR

```{r}
t_value <- qt(1- 0.025, 180) # the t-value for a 95% confidence interval with 180 degrees of freedom

se <- sd(d_bl$lsas_screen)/sqrt(nrow(d_bl)) # standard error of LSAS-SR

ucl <- mean(d_bl$lsas_screen) + t_value * se # the upper confidence limit
lcl <- mean(d_bl$lsas_screen) - t_value * se # the lower confidence limit

print(c(lcl, ucl))
```

We can also use the `t.test()` function to get this interval

```{r}
t.test(d_bl$lsas_screen, conf.level = 0.95)  # For a 95% CI
```

Let's also see what happens if we use z-values (for 95% confidence intervals, the z-value is approx 1.96)

```{r}
se <- sd(d_bl$lsas_screen)/sqrt(nrow(d_bl)) # standard error of LSAS-SR

ucl <- mean(d_bl$lsas_screen) + 1.96 * se
lcl <- mean(d_bl$lsas_screen) - 1.96 * se

print(c(lcl, ucl))
```

##### Exercises

**3.7.** Explain why the confidence intervals calculated using z-scores are narrower that the ones using t-scores.

**3.8** Calculate the 95% confidence interval for PHQ-9, and describe in words what these numbers mean.

**3.9** Calculate the 95% Wald confidence interval for the proportion of men in the dataset using the formula above and interpret its meaning.

**3.10** Compare this to what you would obtain using the function `prop.test()` in R.

**3.11** Reason about the the meaning and interpretation of the confidence intervals you have calculated in the context of how the actual STePs study was performed. The study can be found at: <https://www.nature.com/articles/s44184-024-00063-0>
