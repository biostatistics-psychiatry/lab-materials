---
title: "Sampling from a population"
---

So far, we have restricted ourselves to describing the sample that we have. Often, however, we are not only interested about our sample, but want to make inferences about the **population** from which our sample came.

# Load packages and data

```{r}
#| label: load-packages
#| message: false

library(here)
library(tidyverse)
df_data <- read_csv(here("data", "steps_clean.csv"))
```

# Central Limit Theorem (CLT):

No matter what the population distribution looks like, if you take many random samples and calculate their means, those samples means will form a **normal distribution** as the sample size grows large.

-   Sample means become normally distributed.
-   Happens even if the original population is **not normal**.
-   Works better as **sample size increases** (n â‰¥ 30 is a common rule).

Here is a simulation to visualize what the relation between the distribution of a variable in the population and the sampling distribution of the means from that same population.

```{r}
#| label: clt-normal
#| echo: false

# Parameters
population <- rnorm(10000, mean = 50, sd = 10) # simulate a population
sample_size <- 30
num_samples <- 1000
sample_means <- numeric(num_samples)

par(mfrow = c(2, 1))
hist(population, xlim = c(15, 85), main = "Normal population distrubution")

# Draw multiple samples and compute their means
for (i in 1:num_samples) {
  sample_i <- sample(population, sample_size, replace = TRUE)
  sample_means[i] <- mean(sample_i)
}

# Plot the sampling distribution of the mean
hist(sample_means,
  breaks = 30, probability = TRUE,
  main = paste("Sampling Distribution of the Mean, \n 1000 samples of n=30"),
  xlab = "Sample Mean", col = "skyblue", border = "white", xlim = c(15, 85)
)

# Add a normal curve for comparison
curve(dnorm(x, mean = mean(sample_means), sd = sd(sample_means)),
  col = "red", lwd = 2, add = TRUE
)

```

```{r}
#| label: clt-uniform
#| echo: false

# Parameters
population <- runif(10000, min = 15, max = 85) # simulate a population
sample_size <- 30
num_samples <- 1000
sample_means <- numeric(num_samples)

par(mfrow = c(2, 1))
hist(population, xlim = c(15, 85), main = "Uniform population distrubution")

# Draw multiple samples and compute their means
for (i in 1:num_samples) {
  sample_i <- sample(population, sample_size, replace = TRUE)
  sample_means[i] <- mean(sample_i)
}

# Plot the sampling distribution of the mean
hist(sample_means,
  breaks = 30, probability = TRUE,
  main = paste("Sampling Distribution of the Mean, \n 1000 samples of n=30"),
  xlab = "Sample Mean", col = "skyblue", border = "white", xlim = c(15, 85)
)

# Add a normal curve for comparison
curve(dnorm(x, mean = mean(sample_means), sd = sd(sample_means)),
  col = "red", lwd = 2, add = TRUE
)

```

```{r}
#| label: clt-bimodal
#| echo: false

# Parameters
population <- c(rnorm(10000 / 2, mean = 65, sd = 5), rnorm(10000 / 2, mean = 35, sd = 5)) # simulate a population
sample_size <- 30
num_samples <- 1000
sample_means <- numeric(num_samples)

par(mfrow = c(2, 1))
hist(population, xlim = c(15, 85), breaks = 30, main = "Bi-modal population distribution")

# Draw multiple samples and compute their means
for (i in 1:num_samples) {
  sample_i <- sample(population, sample_size, replace = TRUE)
  sample_means[i] <- mean(sample_i)
}

# Plot the sampling distribution of the mean
hist(sample_means,
  breaks = 30, probability = TRUE,
  main = paste("Sampling Distribution of the Mean, n 1000 samples of n=30"),
  xlab = "Sample Mean", col = "skyblue", border = "white", xlim = c(15, 85)
)

# Add a normal curve for comparison
curve(dnorm(x, mean = mean(sample_means), sd = sd(sample_means)),
  col = "red", lwd = 2, add = TRUE
)

```

# Law of Large Numbers (LLN):

As you collect more and more observations, the **sample mean** (or proportion) will get closer and closer to the **true population mean**.

-   More data = more accuracy.
-   Guarantees that with enough data, **random variation** averages out.

Imagine that we had the patience to flip a coin 1000 times. For each toss, we calculated the number of heads we've gotten so far and divided by the total tosses, to get the proportion that landed heads. Even if we expect the coin to be fair, we wouldn't expect it to be 50-50 heads and tails in the beginning. In fact, at the first toss that would be impossible, As we continued, however, we would see the proportion of heads gradually stabilize around 0.5.

```{r}
#| label: lln-coin
#| echo: false
# Simulate coin tosses (0 = Tails, 1 = Heads)
num_tosses <- 1000
tosses <- rbinom(num_tosses, 1, 0.5)

# Calculate cumulative proportion of heads
toss_data <- data.frame(
  TossNumber = 1:num_tosses,
  ProportionHeads = cumsum(tosses) / (1:num_tosses)
)

# Plot cumulative proportion
ggplot(toss_data, aes(x = TossNumber, y = ProportionHeads)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "Law of Large Numbers: Coin Toss Simulation",
    x = "Number of Tosses",
    y = "Proportion of Heads"
  ) +
  theme_minimal()
```

Together the central limit theorem and the law of large numbers tells us that when sampling from a population 1) the sampling distribution approaches a normal distribution as the number of samples increases, AND 2) the value of the sample mean $\bar{x}$, the estimate, will approach the true population mean $\mu$ as the sample size increase.

# Sampling in research

This is all fine and well, but you may find yourself wondering "How does this help my PhD?". You won't be able to sample repeatedly from the population, if you are lucky you'll have ONE sample to work with. But there is an upside! You can use you sample values to estimate *the standard deviation of the sampling distribution*, also known as the **standard error (SE).**

## Standard error of a mean

The formula for the standard error of a mean, if we knew the population variance $\sigma^2$, is:

$$
SE = {\sqrt{\sigma^2 / n}}
$$

However, we rarely know the population standard deviation. Luckily we can use the sample variance $s^2$ to estimate it:

$$
SE = {\sqrt{s^2 / n}}
$$

Base R has no built-in functions for standard errors, but you can easily calculate them from formulas. To calculate the standard error of the LSAS scale at screening, we can use the following code:

```{r}
sd(df_data$lsas_screen)/sqrt(length(df_data$lsas_screen))

# or we can create a function
stderr <- function(x) sd(x) / sqrt(length(x))
stderr(df_data$lsas_screen)
```

## Standard error of a proportion

For a proportion, the standard error for the population is calculated by the formula:

$$
\mathrm{SE}(p) = \sqrt{\frac{p(1 - p)}{n}}
$$

We can get this in R using the following code:

```{r}
# Simulating a gender variable 
n <- nrow(df_data) 
df_data$gender <- rbinom(n, 1, 0.7)
df_data$gender <- ifelse(df_data$gender == 1, "Woman", "Man")

p <- mean(df_data$gender=="Man") # using the mean function to get the probability 
n <- nrow(df_data) # the number of observations (since we have no NA values) 
se <- sqrt(p * (1 - p) / n) # standard error 
se
```
